


Methode dans reflex pour le cache
class PinnokioCacheManager:
    """Gestionnaire de cache Redis pour les donn√©es externes."""

    def __init__(self):
        self.redis_client: Optional[redis.Redis] = None
        self._connection_config = None

    async def _get_redis_client(self) -> redis.Redis:
        """R√©cup√®re le client Redis (m√™me configuration que les listeners)."""
        if self.redis_client is None:
            self._connection_config = self._load_redis_config()

            self.redis_client = redis.Redis(
                host=self._connection_config.get("host"),
                port=self._connection_config.get("port", 6379),
                password=self._connection_config.get("password"),
                ssl=self._connection_config.get("tls", False),
                db=self._connection_config.get("db", 0),
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
        return self.redis_client

    def _load_redis_config(self) -> Dict:
        """Charge la configuration Redis depuis les variables d'environnement."""
        return {
            "host": os.getenv("LISTENERS_REDIS_HOST", "localhost"),
            "port": int(os.getenv("LISTENERS_REDIS_PORT", "6379")),
            "password": os.getenv("LISTENERS_REDIS_PASSWORD"),
            "tls": os.getenv("LISTENERS_REDIS_TLS", "false").lower() == "true",
            "db": int(os.getenv("LISTENERS_REDIS_DB", "0")),
        }

    def _build_cache_key(self, user_id: str, company_id: str, data_type: str, sub_type: str = None) -> str:
        """Construit une cl√© de cache standardis√©e."""
        key = f"cache:{user_id}:{company_id}:{data_type}"
        if sub_type:
            key += f":{sub_type}"
        return key

    async def get_cached_data(
        self,
        user_id: str,
        company_id: str,
        data_type: str,
        sub_type: str = None,
        fallback_fn: Optional[Callable] = None,
        ttl_seconds: int = 3600
        ) -> Optional[Dict]:
        """R√©cup√®re des donn√©es du cache avec fallback optionnel vers la source."""
        cache_key = self._build_cache_key(user_id, company_id, data_type, sub_type)
        print(f"üîç [CACHE] Tentative de r√©cup√©ration: {cache_key}")
        
        try:
            redis_client = await self._get_redis_client()
            print(f"üîó [CACHE] Connexion Redis √©tablie")

            # Tentative de r√©cup√©ration depuis le cache
            cached_data = await redis_client.get(cache_key)
            if cached_data:
                data = json.loads(cached_data)
                cache_info = data.get("cached_at", "unknown")
                data_size = len(data.get("data", {})) if isinstance(data.get("data"), dict) else "N/A"
                print(f"‚úÖ [CACHE] HIT: {cache_key} | Cached: {cache_info} | Size: {data_size}")
                return data

            # Cache miss - utiliser le fallback si fourni
            print(f"‚ùå [CACHE] MISS: {cache_key}")
            if fallback_fn and callable(fallback_fn):
                print(f"üîÑ [CACHE] Appel du fallback pour: {cache_key}")
                fresh_data = await self._call_fallback_safely(fallback_fn)
                if fresh_data:
                    print(f"üíæ [CACHE] Mise en cache des donn√©es fra√Æches: {cache_key}")
                    await self.set_cached_data(user_id, company_id, data_type, sub_type, fresh_data, ttl_seconds)
                    # Retourner les donn√©es avec la m√™me structure que le cache
                    return {
                        "data": fresh_data,
                        "cached_at": datetime.now().isoformat(),
                        "source": "fallback"
                    }
                else:
                    print(f"‚ö†Ô∏è [CACHE] Fallback n'a retourn√© aucune donn√©e: {cache_key}")

            return None
        except Exception as e:
            print(f"‚ùå [CACHE] Erreur lors de la r√©cup√©ration: {cache_key} | Error: {e}")
            # En cas d'erreur de cache, appeler le fallback si disponible
            if fallback_fn and callable(fallback_fn):
                print(f"üîÑ [CACHE] Tentative de fallback apr√®s erreur: {cache_key}")
                fresh_data = await self._call_fallback_safely(fallback_fn)
                if fresh_data:
                    # Retourner les donn√©es avec la m√™me structure que le cache
                    return {
                        "data": fresh_data,
                        "cached_at": datetime.now().isoformat(),
                        "source": "fallback_after_error"
                    }
                return fresh_data
            return None

    async def set_cached_data(
        self,
        user_id: str,
        company_id: str,
        data_type: str,
        sub_type: str = None,
        data: Dict = None,
        ttl_seconds: int = 3600
        ) -> bool:
        """Stocke des donn√©es dans le cache."""
        cache_key = self._build_cache_key(user_id, company_id, data_type, sub_type)
        print(f"üíæ [CACHE] Tentative de stockage: {cache_key} | TTL: {ttl_seconds}s")
        
        try:
            if not data:
                print(f"‚ö†Ô∏è [CACHE] Donn√©es vides pour: {cache_key}")
                return False

            redis_client = await self._get_redis_client()

            # Calculer la taille des donn√©es
            data_size = len(str(data)) if data else 0
            print(f"üìä [CACHE] Taille des donn√©es: {data_size} caract√®res")

            # Ajouter des m√©tadonn√©es de cache
            cached_payload = {
                "data": data,
                "cached_at": datetime.now().isoformat(),
                "ttl_seconds": ttl_seconds,
                "source": f"{data_type}.{sub_type}" if sub_type else data_type
            }

            # Stocker avec TTL
            await redis_client.setex(
                cache_key,
                ttl_seconds,
                json.dumps(cached_payload)
            )

            # Mettre √† jour les m√©tadonn√©es de refresh
            await self._update_refresh_metadata(user_id, company_id, data_type, sub_type)

            print(f"‚úÖ [CACHE] Stockage r√©ussi: {cache_key} | TTL: {ttl_seconds}s | Taille: {data_size}")
            return True
        except Exception as e:
            print(f"‚ùå [CACHE] Erreur de stockage: {cache_key} | Error: {e}")
            return False

    async def invalidate_company_cache(self, user_id: str, company_id: str) -> bool:
        """Invalide tout le cache d'une soci√©t√© pour un utilisateur."""
        pattern = f"cache:{user_id}:{company_id}:*"
        print(f"üóëÔ∏è [CACHE] Invalidation demand√©e: {pattern}")
        
        try:
            redis_client = await self._get_redis_client()

            # Rechercher toutes les cl√©s correspondant au pattern
            keys = await redis_client.keys(pattern)
            print(f"üîç [CACHE] Cl√©s trouv√©es pour invalidation: {len(keys)}")
            
            if keys:
                for key in keys:
                    print(f"üóëÔ∏è [CACHE] Suppression de la cl√©: {key}")
                
                await redis_client.delete(*keys)
                print(f"‚úÖ [CACHE] Invalidation r√©ussie: {len(keys)} cl√©s supprim√©es pour user={user_id}, company={company_id}")
            else:
                print(f"‚ÑπÔ∏è [CACHE] Aucune cl√© √† invalider pour: {pattern}")

            return True
        except Exception as e:
            print(f"‚ùå [CACHE] Erreur d'invalidation: {pattern} | Error: {e}")
            return False

    async def get_cache_stats(self, user_id: str, company_id: str) -> Dict:
        """Retourne les statistiques du cache pour une soci√©t√©."""
        try:
            redis_client = await self._get_redis_client()
            pattern = f"cache:{user_id}:{company_id}:*"
            keys = await redis_client.keys(pattern)

            stats = {
                "total_keys": len(keys),
                "data_types": {},
                "total_size_bytes": 0,
                "oldest_entry": None,
                "newest_entry": None
            }

            for key in keys:
                try:
                    data = await redis_client.get(key)
                    if data:
                        parsed = json.loads(data)
                        data_type = key.split(":")[-2] if len(key.split(":")) > 3 else "unknown"

                        if data_type not in stats["data_types"]:
                            stats["data_types"][data_type] = 0
                        stats["data_types"][data_type] += 1

                        stats["total_size_bytes"] += len(data)

                        cached_at = parsed.get("cached_at")
                        if cached_at:
                            if not stats["oldest_entry"] or cached_at < stats["oldest_entry"]:
                                stats["oldest_entry"] = cached_at
                            if not stats["newest_entry"] or cached_at > stats["newest_entry"]:
                                stats["newest_entry"] = cached_at
                except Exception:
                    continue

            return stats
        except Exception as e:
            print(f"‚ö†Ô∏è Cache stats error: {e}")
            return {"error": str(e)}

    async def _call_fallback_safely(self, fallback_fn: Callable) -> Any:
        """Appelle la fonction de fallback de mani√®re s√©curis√©e."""
        try:
            if asyncio.iscoroutinefunction(fallback_fn):
                return await fallback_fn()
            else:
                return fallback_fn()
        except Exception as e:
            print(f"‚ö†Ô∏è Fallback function error: {e}")
            return None

    async def _update_refresh_metadata(self, user_id: str, company_id: str, data_type: str, sub_type: str = None):
        """Met √† jour les m√©tadonn√©es de rafra√Æchissement."""
        try:
            meta_key = self._build_cache_key(user_id, company_id, "meta", "last_refresh")

            redis_client = await self._get_redis_client()
            current_meta = await redis_client.get(meta_key)

            meta_data = json.loads(current_meta) if current_meta else {}
            source = f"{data_type}.{sub_type}" if sub_type else data_type
            meta_data[source] = datetime.now().isoformat()

            await redis_client.setex(meta_key, 86400, json.dumps(meta_data))  # TTL 24h pour les m√©tadonn√©es
        except Exception as e:
            print(f"‚ö†Ô∏è Metadata update error: {e}")


***************
FORMAT DE CACHING Des transasaction bancaire extraction depuis le cache redis
# V√©rifier le cache Redis
                cached_data = await cache_manager.get_cached_data(
                    user_id=self.firebase_user_id,
                    company_id=self.companies_search_id,
                    data_type="bank",
                    sub_type="transactions"
                )

Format des donn√©es quand extraite (Redis ou depui la source dans Reflex)
@rx.event(background=True)
    async def load_bank_transactions_from_cache(self, cached_data: dict):
        """Charge les transactions depuis les donn√©es du cache."""
        async with self:
            try:
                print("üìã [BANK] Chargement depuis le cache...")
                
                # Charger les transactions to_reconcile
                to_reconcile_data = cached_data.get("to_reconcile", [])
                self.items_to_reconcile = [
                    TransactionItem.from_dict(tx_data) 
                    for tx_data in to_reconcile_data
                ]
                
                # Charger les transactions pending
                pending_data = cached_data.get("pending", [])
                self.pending_items = [
                    TransactionItem.from_dict(tx_data) 
                    for tx_data in pending_data
                ]
                
                # Charger les transactions in_process
                in_process_data = cached_data.get("in_process", [])
                self.in_process_items = [
                    TransactionItem.from_dict(tx_data) 
                    for tx_data in in_process_data
                ]
                
                # Charger les lots en cours (BatchItem)
                in_process_batches_data = cached_data.get("in_process_batches", [])
                self._in_process_batches = [
                    BatchItem(
                        batch_id=batch_data.get("batch_id", ""),
                        bank_account=batch_data.get("bank_account", ""),
                        transaction_count=int(batch_data.get("transaction_count", 0) or 0),
                        status=batch_data.get("status", ""),
                        timestamp=str(batch_data.get("timestamp", "")),
                    )
                    for batch_data in in_process_batches_data
                ]
                
                # Charger les comptes bancaires
                self.available_bank_accounts = cached_data.get("bank_accounts", [])
                self.selected_bank_account = cached_data.get("selected_bank_account", "")
                
                # Calculer le solde total
                self._calculate_total_balance()
                
                print(f"‚úÖ [BANK] Cache charg√©: {len(self.items_to_reconcile)} to_reconcile, "
                      f"{len(self.pending_items)} pending, {len(self.in_process_items)} in_process, "
                      f"{len(self._in_process_batches)} batches")
                
            except Exception as e:
                print(f"‚ùå [BANK] Erreur chargement depuis cache: {e}")
                raise e
***************************************************************************
Si pas de donn√©e en cache , extraire depuis la source qui l'erp

Prendre depuis les m√©tadonn√©e qui sont consitut√© dans le brain , les valeur de 
odoo_url = erp_data.get("odoo_url")
            odoo_db_name = erp_data.get("odoo_db") 
            odoo_username = erp_data.get("odoo_username")
            odoo_company_name = erp_data.get("odoo_company_name")
            
            # R√©cup√©rer la cl√© API depuis le gestionnaire de secrets
            secret_manager_name = erp_data.get("secret_manager")
Les prendre depuis reconstruct_full_company profile

Mettre une condition l'erp est odoo.
Ensuite cr√©er l'instance ERP_PINNOKIO, avec comme parametre l'erp_type, pour l'instant int√©grer unqiueemnt odoo, et si odoo
int√©grer cr√©er une instance pour l'utilisateur_company avec la connection compartimenter , 
cr√©er la m√©thode get_bank_statement_move_line_not_rec
si odoo, voici la m√©thode pour faire appel si odoo
def get_odoo_bank_statement_move_line_not_rec(self, journal_id=None, reconciled=None):
        """
        R√©cup√®re les mouvements des relev√©s bancaires depuis Odoo pour un mod√®le sp√©cifi√©, en retournant les d√©tails
        sp√©cifiques de chaque mouvement de mani√®re regroup√©e. Les filtres sur `journal_id` et `reconciled` sont optionnels.

        Args:
            journal_id (int, optional): L'identifiant du journal √† filtrer. R√©cup√®re tous les mouvements si None.
            reconciled (bool, optional): Filtrer les mouvements qui sont r√©concili√©s ou non. R√©cup√®re tous les mouvements si None.

        Returns:
            list: Une liste de dictionnaires, chaque dictionnaire contenant les d√©tails regroup√©s d'un mouvement de relev√© bancaire.
            pd.DataFrame: Un DataFrame contenant les m√™mes donn√©es pour une manipulation ult√©rieure.
        """
        # D√©finition des crit√®res de recherche de base (uniquement filtr√© par l'entreprise)
        domain = []
        filters = [['company_id.name', '=', self.company_name]]
        search_criteria = domain + filters

        # Liste des champs sp√©cifiques √† r√©cup√©rer pour chaque mouvement de relev√© bancaire
        fields_to_retrieve = [
            'move_id', 'journal_id', 'payment_ids', 'partner_id', 'account_number', 'partner_name',
            'transaction_type', 'payment_ref', 'currency_id', 'amount', 'running_balance',
            'amount_currency', 'amount_residual', 'is_reconciled', 'statement_complete',
            'statement_valid', 'display_name', 'name', 'ref', 'date', 'state', 'move_type',
            'company_id'
        ]

        # Ex√©cution de la requ√™te vers Odoo pour r√©cup√©rer les informations sans filtres suppl√©mentaires
        bank_statement_moves = self.execute_kw('account.bank.statement.line', 'search_read', [search_criteria], {'fields': fields_to_retrieve})

        # Conversion en DataFrame pour une manipulation facile
        df = pd.DataFrame(bank_statement_moves)

        # Application des filtres optionnels
        if journal_id is not None:
            df = df[df['journal_id'].apply(lambda x: x[0] == journal_id)]  # Filtre sur journal_id

        if reconciled is not None:
            if 'is_reconciled' in df.columns:
                df = df[df['is_reconciled'] == reconciled]  # Filtre sur reconciled
            else:
                # Colonne absente: ignorer le filtre et informer (soci√©t√© sans module bancaire configur√©)
                print("‚ÑπÔ∏è [ERP] Colonne 'is_reconciled' absente; filtre 'reconciled' ignor√© (banking module non configur√©).")

        # Conversion du DataFrame filtr√© en liste de dictionnaires
        df=self.expand_list_columns(df)
        filtered_data = df.to_dict('records')

        return filtered_data, df

**************
Au final qu'on soit par cache ou par appel a Odoo on obtiens les trnsactions
Ce processus peut prendre un peu de temps donc le mettre asynchrone, une fois les donn√©es charg√©e si provenant de la source, charge le cache redis exactement sous le meme format attendu que si c√©tait effecut√© depuis reflex.

a present nous avons un dictionnaire avec toutes les transaction bancaires non r√©concilier par compte en compte, l'objectif il faudrait que cela soit dans un dictionnaire pour que l'agent
soit capable soit de trier sur la base des donn√©es (a regareder entre dataframe ou dictionnaire)

Voici la continuit√© du code cot√© Reflex apres la r√©cup√©ration afin que tu puisse comprendre les colonnes et champs important du cl√© du dictionnaire pour aigullier l'agent.

#print(f"üîç DONN√âES BRUTES ERP - Premier √©l√©ment journal_id: {transactions_data[0] if transactions_data else 'Aucune donn√©e'}")    
                # Valider le format: une liste (√©ventuellement vide) est acceptable
                if not isinstance(transactions_data, list):
                    print(f"‚ùå Format de r√©sultat inattendu: {type(transactions_data)}")
                    yield rx.toast.error(
                        title="Format error",
                        description=f"Unexpected result type from ERP: {type(transactions_data)}",
                        duration=5000
                    )

                    self._reset_transaction_data()
                    return
                
                '''# Filtrer les transactions non r√©concili√©es
                filtered_transactions = [
                    tx for tx in transactions_data 
                    if isinstance(tx, dict) and tx.get('is_reconciled') == False
                ]'''
                
                

                # 2. R√©cup√©rer les IDs des transactions en cours de traitement
                in_process_transaction_ids = await self._get_in_process_transaction_ids()
                print(f"üîÑ {len(in_process_transaction_ids)} transactions en cours identifi√©es")
                
                pending_transaction_ids = await self._get_pending_transaction_ids_optimized()
                print(f"üìã {len(pending_transaction_ids)} transactions pending identifi√©es")
                # 3. Filtrer les transactions (non r√©concili√©es ET non en cours)
                
                filtered_transactions = []
                excluded_count = 0
                
                for tx in transactions_data:
                    if not isinstance(tx, dict) or tx.get('is_reconciled', False):
                        continue
                        
                    # R√©cup√©rer l'ID de la transaction (selon votre structure de donn√©es)
                    tx_id = str(tx.get('move_id', '') or tx.get('transaction_id', ''))
                    
                    # Exclure si en cours de traitement
                    if tx_id and tx_id in in_process_transaction_ids:
                        excluded_count += 1
                        print(f"‚è≠Ô∏è Transaction {tx_id} exclue (en cours de traitement)")
                        continue

                    if tx_id and tx_id in pending_transaction_ids:
                        excluded_count += 1
                        print(f"‚è≠Ô∏è Transaction {tx_id} exclue (dans pending)")
                        continue
                        
                    filtered_transactions.append(tx)
                
                print(f"üìä {len(filtered_transactions)} transactions disponibles pour r√©conciliation")
                print(f"üö´ {excluded_count} transactions exclues (en cours de traitement)")
                    
                
                
                
                
                
                # Extraction des comptes bancaires uniques
                bank_accounts = self._extract_bank_accounts(filtered_transactions)
                
                if not bank_accounts:
                    print("‚ö†Ô∏è Aucun compte bancaire trouv√© - utilisation d'un compte par d√©faut")
                    yield rx.toast.info(
                        title="No bank account detected",
                        description="Using a default account to continue.",
                        duration=3000
                    )
                    bank_accounts = ["Default"]
                
                print(f"üè¶ {len(bank_accounts)} comptes bancaires identifi√©s: {bank_accounts}")
                
                # Mise √† jour des comptes bancaires disponibles
                #self.available_bank_accounts = bank_accounts
                
                self._merge_bank_accounts(bank_accounts)

                # S√©lectionner automatiquement le premier compte bancaire
                if not self.selected_bank_account or self.selected_bank_account not in bank_accounts:
                    self.selected_bank_account = bank_accounts[0]
                    print(f"üéØ Compte bancaire s√©lectionn√© automatiquement: {self.selected_bank_account}")
                    
                    # R√©initialiser les s√©lections lors du changement de compte
                    self.selected_items = []
                    print("üîÑ S√©lections r√©initialis√©es")
                
                # Conversion en objets TransactionItem
                self.items_to_reconcile = self._convert_transactions_to_items(filtered_transactions)
                
                # Calculer le solde total pour le compte s√©lectionn√©
                self._calculate_total_balance()
                
                # Notification de succ√®s
                yield rx.toast.success(
                    title="Loading complete",
                    description=f"{len(self.items_to_reconcile)} transactions loaded, account '{self.selected_bank_account}' selected",
                    duration=3000
                )
                
                # Mettre en cache si demand√©
                if getattr(self, '_should_cache_after_load', False):
                    print(f"üíæ [BANK] Mise en cache automatique apr√®s fetch_transactions")
                    yield BankTransactionState.cache_bank_data_now
                
            except Exception as e:
                print(f"‚ùå Erreur lors de l'appel √† l'API ERP: {e}")
                import traceback
                print(f"‚ùå Traceback complet: {traceback.format_exc()}")
                
                # Toast sp√©cifique pour absence de module bancaire
                if "is_reconciled" in str(e):
                    yield rx.toast.info(
                        title="No Banking Module",
                        description="This company has no banking transactions configured or available.",
                        duration=4000
                    )
                else:
                    yield rx.toast.error(
                        title="ERP Connection Error",
                        description=f"Unable to retrieve transactions: {str(e)}",
                        duration=5000
                    )
                self.error = f"Erreur lors de la r√©cup√©ration des transactions: {str(e)}"
                self._reset_transaction_data()
                
        except Exception as e:
            print(f"‚ùå Erreur globale dans fetch_transactions: {e}")
            import traceback
            print(f"‚ùå Traceback complet: {traceback.format_exc()}")
            self.error = f"Erreur inattendue: {str(e)}"
            yield rx.toast.error(
                title="Unexpected error",
                description=self.error,
                duration=5000
            )

            self._reset_transaction_data()
        finally:
            self.is_loading = False
            # Lib√©rer le verrou fetch_bank_inflight si il √©tait activ√©
            if getattr(self, 'fetch_bank_inflight', False):
                print("üîì [BANK] D√©sactivation du verrou fetch_bank_inflight (fin fetch_transactions)")
                self.fetch_bank_inflight = False
************************
Pour les documents de Router.
Nous dispons d√©j√† d'une instance Google drive dans notre microservice, 
deux source de donn√©e google drive avec le parametre input_drive_id et firebase avec mandate_path et une regle de filtrage
@rx.event()
    async def fetch_drive_documents(self):
        """Fetch documents from Google Drive."""
        
        try:
            
            
            drive_service = DriveClientService(user_id=self.firebase_user_id,mode='prod')
            data = await drive_service.list_files_in_doc_to_do(self.input_drive_id)
            #print(f"impress de dara de drive:{data}")
            
            # Cas d'erreur du service
            if isinstance(data, dict) and "erreur" in data:
                print(f"Erreur re√ßue du service: {data['erreur']}")
                # D√©tection re-consent si invalid_grant
                try:
                    err_txt = str(data.get("erreur", "")).lower()
                    if "invalid_grant" in err_txt:
                        # Signaler √† l'appelant qu'un re-consent est requis
                        raise Exception("OAUTH_REAUTH_REQUIRED: invalid_grant")
                except Exception:
                    pass
                return  # Retourne None en cas d'erreur
            
            # Cas de dossier vide
            elif data == []:
                print("Aucun fichier trouv√© dans le dossier")
                return {  # Retourne un dictionnaire avec des listes vides mais valides
                    "to_process": [],
                    "in_process": []
                }
                
                

            # 2. Initialiser une liste de GdriveDocumentItem avec les donn√©es Drive
           # Initialiser les listes pour diff√©rentes cat√©gories de documents
            drive_documents_to_process = []  # Pour les documents √† traiter
            drive_documents_in_process = []  # Pour les documents en cours
            all_drive_documents = []
            for doc in data:
                drive_doc = GdriveDocumentItem(
                    id=doc.get('id', ''),
                    name=doc.get('name', ''),
                    created_time=datetime.strptime(doc.get('createdTime', ''), 
                                                "%Y-%m-%dT%H:%M:%S.%fZ").strftime("%d/%m/%Y %H:%M"),
                    status="to_process",  # Statut par d√©faut
                    client=self.client_name,
                    router_drive_view_link=doc.get('webViewLink')
                )
                all_drive_documents.append(drive_doc)
        
            # 3. R√©cup√©rer les statuts des notifications Firebase pour mettre √† jour les statuts
            firebase_service = FireBaseManagement()
            
            # Pour chaque document Drive, v√©rifier s'il existe une notification correspondante
            for drive_doc in all_drive_documents:
                # V√©rifier dans Firebase si ce document a une notification
                notification = firebase_service.check_job_status(
                    user_id=self.firebase_user_id,
                    file_id=drive_doc.id
                )
                
                # Si une notification existe et qu'elle correspond √† la fonction Router
                if notification and notification.get('function_name') == 'Router':
                    firebase_status = notification.get('status')
                    
                    # Mettre √† jour le statut du document
                    if firebase_status == 'running':
                        drive_doc.status = 'on_process'
                        drive_documents_in_process.append(drive_doc)  # Ajouter aux documents en cours
                    elif firebase_status == 'in queue':
                        drive_doc.status = 'in_queue'
                        drive_documents_in_process.append(drive_doc)  # Ajouter aux documents en cours
                    elif firebase_status == 'stopping':
                        drive_doc.status = 'stopping'
                        drive_documents_in_process.append(drive_doc)  # Ajouter aux documents en cours
                    else:
                        # Tous les autres statuts restent dans la liste principale
                        if firebase_status == 'error':
                            drive_doc.status = 'error'
                        elif firebase_status == 'pending':
                            drive_doc.status = 'pending'
                        elif firebase_status in ['completed', 'success']:
                            drive_doc.status = 'routed'
                        # et on l'ajoute √† la liste des documents √† traiter
                        drive_documents_to_process.append(drive_doc)
                else:
                    # Aucune notification, document √† traiter
                    drive_documents_to_process.append(drive_doc)  # Statut inconnu, on garde dans la liste principale
            
                    
                print(f"Document {drive_doc.name} (ID: {drive_doc.id}): statut mis √† jour de 'to_process' √† '{drive_doc.status}'")
            
            # 4. Mettre √† jour la liste des documents non trait√©s
        
            #self.unprocessed_documents = drive_documents_to_process
            #self.items_in_process = drive_documents_in_process
            
        
            print(f"Chargement r√©ussi: {len(drive_documents_to_process)} documents √† traiter et {len(drive_documents_in_process)} documents en cours")
            return {
            "to_process": drive_documents_to_process,
            "in_process": drive_documents_in_process
            }
        except Exception as e:
            err_str = str(e)
            print(f"Erreur lors du chargement des documents Drive ou de la synchronisation des statuts: {err_str}")
            # Propager un signal clair √† l'appelant si re-consent requis
            if "invalid_grant" in err_str.lower() or "OAUTH_REAUTH_REQUIRED" in err_str:
                raise Exception("OAUTH_REAUTH_REQUIRED: invalid_grant")
            return None


****************
Meme principe si non redis extreaire les donn√©e de la source , et ensutie mettre √† jour dans Redis


Pour les factures fournisseurs

async def _fetch_ap_from_firebase(self) -> Dict:
        """R√©cup√®re les documents APbookeeper depuis Firebase (m√©thode existante adapt√©e)."""
        try:
            print("üîÑ [AP] R√©cup√©ration des documents depuis Firebase...")
            
            # Utiliser la logique existante de fetch_documents mais retourner les donn√©es structur√©es
            firebase_c = FireBaseManagement()
            departement = 'APbookeeper'
            
            # Helper function pour cr√©er DocumentItem (r√©utilis√©e de fetch_documents)
            def create_document_item(doc):
                def _format_timestamp(value):
                    if not value:
                        return "N/A"
                    try:
                        if hasattr(value, "strftime"):
                            return value.strftime("%d/%m/%Y %H:%M")
                        if isinstance(value, str):
                            s = value.strip()
                            from datetime import datetime, timezone
                            try:
                                if s.endswith("Z"):
                                    return datetime.fromisoformat(s.replace("Z", "+00:00")).strftime("%d/%m/%Y %H:%M")
                                return datetime.fromisoformat(s).strftime("%d/%m/%Y %H:%M")
                            except Exception:
                                return s
                        if isinstance(value, (int, float)):
                            from datetime import datetime, timezone
                            return datetime.fromtimestamp(value, tz=timezone.utc).strftime("%d/%m/%Y %H:%M")
                        seconds = getattr(value, "seconds", None)
                        if isinstance(seconds, (int, float)):
                            from datetime import datetime, timezone
                            return datetime.fromtimestamp(seconds, tz=timezone.utc).strftime("%d/%m/%Y %H:%M")
                    except Exception:
                        pass
                    return "N/A"

                ts = _format_timestamp(doc['data'].get('timestamp'))

                return DocumentItem(
                    client=doc['data'].get('client', ''),
                    file_name=doc['data'].get('file_name', ''),
                    status=doc['data'].get('status', 'to_process'),
                    timestamp=ts,
                    source=doc['data'].get('source', ''),
                    uri_drive_link=doc['data'].get('uri_drive_link', ''),
                    job_id=doc['data'].get('job_id', ''),
                    drive_file_id=doc['data'].get('drive_file_id', ""),
                    pinnokio_func=departement
                )
            
            # R√©cup√©rer tous les documents
            all_docs = {}
            
            # TO_DO documents
            todo_docs = firebase_c.fetch_journal_entries_by_mandat_id(
                self.firebase_user_id,
                self.base_collection_id,
                source='documents/accounting/invoices/doc_to_do',
                departement=departement
            )
            
            items_to_do = [create_document_item(doc) for doc in todo_docs]
            items_in_process = []
            final_items_to_do = []
            
            # Traiter les statuts avec notifications Firebase
            for item in items_to_do:
                notification = firebase_c.check_job_status(
                    user_id=self.firebase_user_id,
                    job_id=item.job_id
                )
                
                if notification and notification.get('function_name') == 'APbookeeper':
                    firebase_status = notification.get('status')
                    
                    if firebase_status == 'running':
                        item.status = 'on_process'
                        items_in_process.append(item)
                    elif firebase_status == 'in queue':
                        item.status = 'in_queue'
                        items_in_process.append(item)
                    elif firebase_status == 'stopping':
                        item.status = 'stopping'
                        items_in_process.append(item)
                    elif firebase_status == 'pending':
                        # Sera g√©r√© dans la section PENDING
                        pass
                    else:
                        if firebase_status in ['error','stopped']:
                            item.status = firebase_status
                        elif firebase_status in ['completed', 'success','close']:
                            item.status = 'completed'
                        final_items_to_do.append(item)
                else:
                    final_items_to_do.append(item)
            
            # PENDING documents
            pending_docs = firebase_c.fetch_pending_journal_entries_by_mandat_id(
                self.firebase_user_id,
                self.base_collection_id,
                source='documents/accounting/invoices/doc_to_do',
                departement=departement
            )
            
            items_pending = []
            for doc in pending_docs:
                doc_item = create_document_item(doc)
                notification = firebase_c.check_job_status(
                    user_id=self.firebase_user_id,
                    job_id=doc_item.job_id
                )
                
                if notification and notification.get('function_name') == 'APbookeeper':
                    firebase_status = notification.get('status')
                    if firebase_status == 'pending':
                        doc_item.status = 'pending'
                        items_pending.append(doc_item)
            
            # PROCESSED documents
            booked_docs = firebase_c.fetch_journal_entries_by_mandat_id(
                self.firebase_user_id,
                self.base_collection_id,
                source='documents/invoices/doc_booked',
                departement=departement
            )
            
            items_booked = [create_document_item(doc) for doc in booked_docs]
            for item in items_booked:
                item.status = 'completed'
            
            # Convertir en dictionnaires pour le cache
            cache_data = {
                "to_do": [item.to_dict() for item in final_items_to_do],
                "in_process": [item.to_dict() for item in items_in_process],
                "pending": [item.to_dict() for item in items_pending],
                "processed": [item.to_dict() for item in items_booked]
            }
            
            print(f"üìä [AP] Donn√©es r√©cup√©r√©es - to_do: {len(final_items_to_do)}, "
                  f"in_process: {len(items_in_process)}, pending: {len(items_pending)}, "
                  f"processed: {len(items_booked)}")
            
            return cache_data
            
        except Exception as e:
            print(f"‚ùå [AP] Erreur r√©cup√©ration Firebase: {e}")
            raise e



********************************
Ceci va permettre √† de√©finir les valeurs a initailiser pour le SPT des job, et de ceci fournir les m√©ttric de base a int√©grer au system de prompt de cette agent. 
Dans le context de l'agent il va peut donner des indications precises sur la nature de ces documents, les informations comme le file_id qui peut transmettre ect‚Ä¶


Acces aux notification, autre outil , client/{uid}/notifications, faire un Stream sur la collection , 
avec acces aux champs status, file_name, job_id, function 
ou appel √† une notifiaction pr√©cise en ajouter client/{uid}/notifications/job_id


Ceci sont les informations pour permettre √† l'agent, JobManager de pouvoir fournir les informaitons n√©caissaire sur les jobs a traiter et 