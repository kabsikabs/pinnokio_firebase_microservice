"""
Gestionnaire LLM centralis√© utilisant Firebase Realtime Database.
G√®re les sessions LLM et l'int√©gration avec BaseAIAgent.
"""

import asyncio
import json
import uuid
import time
import logging
import threading
from typing import Dict, Optional, Any
from datetime import datetime, timezone
from ..llm.klk_agents import BaseAIAgent, ModelProvider, ModelSize
from .llm_context import LLMContext
from .rtdb_message_formatter import RTDBMessageFormatter

logger = logging.getLogger("llm_service.manager")


class StreamingController:
    """Contr√¥leur pour g√©rer les arr√™ts de streaming via WebSocket."""
    
    def __init__(self):
        self.active_streams: Dict[str, Dict[str, Any]] = {}  # {session_key: {thread_key: task_info}}
        self._lock = threading.Lock()
    
    async def register_stream(self, session_key: str, thread_key: str, task: asyncio.Task):
        """Enregistre un stream actif."""
        with self._lock:
            if session_key not in self.active_streams:
                self.active_streams[session_key] = {}
            
            self.active_streams[session_key][thread_key] = {
                "task": task,
                "started_at": datetime.now(timezone.utc),
                "status": "streaming"
            }
            logger.info(f"Stream enregistr√©: {session_key}:{thread_key}")
    
    async def stop_stream(self, session_key: str, thread_key: str) -> bool:
        """Arr√™te un stream sp√©cifique."""
        with self._lock:
            if session_key not in self.active_streams:
                return False
            
            if thread_key not in self.active_streams[session_key]:
                return False
            
            stream_info = self.active_streams[session_key][thread_key]
            
            # Arr√™ter la t√¢che
            if not stream_info["task"].done():
                stream_info["task"].cancel()
                logger.info(f"Stream arr√™t√©: {session_key}:{thread_key}")
            
            # Marquer comme interrompu
            stream_info["status"] = "interrupted"
            stream_info["interrupted_at"] = datetime.now(timezone.utc)
            
            return True
    
    async def stop_all_streams(self, session_key: str) -> int:
        """Arr√™te tous les streams d'une session."""
        with self._lock:
            if session_key not in self.active_streams:
                return 0
            
            stopped_count = 0
            for thread_key, stream_info in self.active_streams[session_key].items():
                if not stream_info["task"].done():
                    stream_info["task"].cancel()
                    stream_info["status"] = "interrupted"
                    stream_info["interrupted_at"] = datetime.now(timezone.utc)
                    stopped_count += 1
            
            logger.info(f"Tous les streams arr√™t√©s pour {session_key}: {stopped_count}")
            return stopped_count
    
    async def unregister_stream(self, session_key: str, thread_key: str):
        """D√©senregistre un stream termin√©."""
        with self._lock:
            if session_key in self.active_streams and thread_key in self.active_streams[session_key]:
                del self.active_streams[session_key][thread_key]
                if not self.active_streams[session_key]:
                    del self.active_streams[session_key]
                logger.info(f"Stream d√©senregistr√©: {session_key}:{thread_key}")
    
    async def get_active_streams(self, session_key: str) -> Dict[str, Any]:
        """Retourne les streams actifs d'une session."""
        with self._lock:
            return self.active_streams.get(session_key, {}).copy()


class LLMSession:
    """Session LLM isol√©e pour un utilisateur/soci√©t√©.
    
    G√®re l'agent BaseAIAgent et l'historique des conversations pour tous les threads
    de cet utilisateur dans cette soci√©t√©.
    """
    
    def __init__(self, session_key: str, context: LLMContext):
        self.session_key = session_key  # user_id:collection_name
        self.context = context
        self.agent: Optional[BaseAIAgent] = None
        
        # Lock pour cette session sp√©cifique (pas de conflit entre utilisateurs)
        self._lock = threading.Lock()
        
        # Historique par thread de conversation
        self.conversations: Dict[str, list] = {}
        
        # T√¢ches actives par thread
        self.active_tasks: Dict[str, list] = {}
        
        # √âtat par thread
        self.thread_states: Dict[str, str] = {}
        
        # M√©triques
        self.created_at = datetime.now(timezone.utc)
        self.last_activity: Dict[str, datetime] = {}
        self.response_times: Dict[str, list] = {}
    
    async def initialize_agent(self):
        """Initialise l'agent BaseAIAgent avec le contexte."""
        try:
            logger.info(f"Initialisation BaseAIAgent pour session {self.session_key}")
            
            # Initialiser BaseAIAgent avec les param√®tres du contexte
            self.agent = BaseAIAgent(
                collection_name=self.context.collection_name,
                dms_system=self.context.dms_system,
                dms_mode=self.context.dms_mode,
                firebase_user_id=self.context.user_id
            )
            
            # Enregistrer les providers par d√©faut
            # Note: √Ä adapter selon vos besoins - ici on initialise Anthropic par d√©faut
            try:
                from ..llm.klk_agents import NEW_Anthropic_Agent
                anthropic_instance = NEW_Anthropic_Agent()
                self.agent.register_provider(ModelProvider.ANTHROPIC, anthropic_instance)
                logger.info("Provider Anthropic enregistr√©")
            except Exception as e:
                logger.warning(f"Impossible d'enregistrer Anthropic: {e}")
            
            # D√©finir les valeurs par d√©faut
            self.agent.default_provider = ModelProvider.ANTHROPIC
            self.agent.default_model_size = ModelSize.MEDIUM
            
            logger.info(f"Agent LLM initialis√© pour session {self.session_key}")
            
        except Exception as e:
            logger.error(f"Erreur initialisation agent: {e}", exc_info=True)
            raise
    
    def update_context(self, **kwargs):
        """Met √† jour le contexte dynamiquement."""
        for key, value in kwargs.items():
            if hasattr(self.context, key):
                setattr(self.context, key, value)
        
        # Si DMS change, r√©initialiser l'agent
        if 'dms_system' in kwargs or 'dms_mode' in kwargs:
            if self.agent:
                self.agent._initialize_dms(
                    self.context.dms_mode,
                    self.context.dms_system,
                    self.context.user_id
                )
    
    def add_user_message(self, thread_key: str, message: str):
        """Ajoute un message utilisateur √† l'historique d'un thread."""
        if thread_key not in self.conversations:
            self.conversations[thread_key] = []
        
        self.conversations[thread_key].append({
            "role": "user",
            "content": message,
            "timestamp": datetime.now(timezone.utc).isoformat()
        })
        
        self.last_activity[thread_key] = datetime.now(timezone.utc)
    
    async def process_message_streaming(
        self,
        thread_key: str,
        message: str,
        system_prompt: str = None
    ):
        """Traite un message et yield les chunks de r√©ponse.
        
        Yields:
            dict: {"content": str, "index": int, "is_final": bool, "tool_calls": list}
        """
        try:
            self.thread_states[thread_key] = "processing"
            start_time = datetime.now(timezone.utc)
            
            # Mettre √† jour le prompt syst√®me si fourni
            if system_prompt and self.agent:
                logger.info(f"Mise √† jour system_prompt: {system_prompt[:100]}...")
                self.agent.update_system_prompt(system_prompt)
            
            if not self.agent:
                raise Exception("Agent non initialis√©")
            
            # Utiliser le vrai streaming depuis l'agent Anthropic
            logger.info(f"Utilisation du vrai streaming Anthropic...")
            
            # V√©rifier que l'agent existe
            if not self.agent:
                logger.error("Agent non initialis√© !")
                raise Exception("Agent non initialis√©")
            
            logger.info(f"Agent trouv√©: {type(self.agent)}")
            
            # V√©rifier que la m√©thode existe
            if not hasattr(self.agent, 'process_text_streaming'):
                logger.error("M√©thode process_text_streaming non trouv√©e !")
                raise Exception("M√©thode process_text_streaming non trouv√©e")
            
            logger.info("M√©thode process_text_streaming trouv√©e")
            
            # Ajouter le message utilisateur √† l'historique
            logger.info("Ajout du message utilisateur √† l'historique...")
            self.agent.add_user_message(message)
            logger.info("Message utilisateur ajout√©")
            
            # Utiliser le streaming via BaseAIAgent
            logger.info("D√©but du streaming via BaseAIAgent...")
            async for chunk in self.agent.process_text_streaming(
                content=message,
                max_tokens=1024
            ):
                chunk_content = chunk.get("content", "")
                is_final = chunk.get("is_final", False)
                
                logger.info(f"Chunk streaming re√ßu: '{chunk_content[:50]}...' (final: {is_final})")
                
                yield {
                    "content": chunk_content,
                    "index": 0,  # Pas d'index pour le vrai streaming
                    "is_final": is_final,
                    "tool_calls": None
                }
                
                # Si c'est le chunk final, on peut sortir
                if is_final:
                    logger.info(f"Streaming termin√©")
                    break
            
            # Ajouter r√©ponse √† l'historique (le contenu est d√©j√† dans l'agent)
            # L'agent a d√©j√† ajout√© la r√©ponse compl√®te via add_ai_message
            logger.info(f"R√©ponse ajout√©e √† l'historique de l'agent")
            
            # M√©triques
            duration_ms = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
            if thread_key not in self.response_times:
                self.response_times[thread_key] = []
            self.response_times[thread_key].append(duration_ms)
            
            self.thread_states[thread_key] = "idle"
            
        except Exception as e:
            self.thread_states[thread_key] = "error"
            logger.error(f"Erreur process_message_streaming: {e}", exc_info=True)
            raise
    
    def _extract_response_text(self, response) -> str:
        """Extrait le texte de la r√©ponse de BaseAIAgent."""
        response_text = ""
        if isinstance(response, dict):
            if 'text_output' in response:
                text_output = response.get('text_output', {})
                if isinstance(text_output, dict):
                    content = text_output.get('content', {})
                    if isinstance(content, dict):
                        response_text = content.get('answer_text', '')
                    else:
                        response_text = str(content)
                else:
                    response_text = str(text_output)
            else:
                response_text = str(response)
        else:
            response_text = str(response)
        
        return response_text
    
    def get_token_stats(self, thread_key: str) -> dict:
        """Retourne les stats de tokens depuis BaseAIAgent."""
        if not self.agent:
            return {"prompt": 0, "completion": 0, "total": 0}
        
        try:
            usage = self.agent.get_token_usage_by_provider()
            
            # Agr√©ger les stats de tous les providers
            total_input = sum(p.get('total_input_tokens', 0) for p in usage.values())
            total_output = sum(p.get('total_output_tokens', 0) for p in usage.values())
            
            return {
                "prompt": total_input,
                "completion": total_output,
                "total": total_input + total_output
            }
        except Exception:
            return {"prompt": 0, "completion": 0, "total": 0}
    
    def get_last_response_duration_ms(self, thread_key: str) -> int:
        """Retourne la dur√©e de la derni√®re r√©ponse en ms."""
        if thread_key in self.response_times and self.response_times[thread_key]:
            return int(self.response_times[thread_key][-1])
        return 0


class LLMManager:
    """Gestionnaire LLM utilisant Firebase Realtime Database."""
    
    def __init__(self):
        self.sessions: Dict[str, LLMSession] = {}
        self._lock = threading.Lock()
        self.rtdb_formatter = RTDBMessageFormatter()
        self.streaming_controller = StreamingController()
    
    def _get_rtdb_ref(self, path: str):
        """Obtient une r√©f√©rence Firebase RTDB."""
        from ..listeners_manager import _get_rtdb_ref
        return _get_rtdb_ref(path)
    
    async def initialize_session(
        self,
        user_id: str,
        collection_name: str,
        dms_system: str = "google_drive",
        dms_mode: str = "prod",
        chat_mode: str = "general_chat"
    ) -> dict:
        """Initialise une session LLM pour un utilisateur/soci√©t√©."""
        try:
            logger.info(f"=== D√âBUT initialize_session ===")
            logger.info(f"Param√®tres: user_id={user_id}, collection_name={collection_name}")
            logger.info(f"Chat mode: {chat_mode}")
            
            with self._lock:
                base_session_key = f"{user_id}:{collection_name}"
                
                logger.info(f"Initialisation session LLM: {base_session_key}")
                
                # V√©rifier si session existe d√©j√†
                if base_session_key in self.sessions:
                    session = self.sessions[base_session_key]
                    # Mettre √† jour le contexte si n√©cessaire
                    if (session.context.dms_system != dms_system or 
                        session.context.chat_mode != chat_mode):
                        session.update_context(
                            dms_system=dms_system,
                            dms_mode=dms_mode,
                            chat_mode=chat_mode
                        )
                    
                    logger.info(f"Session existante r√©utilis√©e: {base_session_key}")
                    return {
                        "success": True,
                        "session_id": base_session_key,
                        "status": "existing",
                        "message": "Session LLM r√©utilis√©e"
                    }
                
                # Cr√©er nouvelle session
                context = LLMContext(
                    user_id=user_id,
                    collection_name=collection_name,
                    dms_system=dms_system,
                    dms_mode=dms_mode,
                    chat_mode=chat_mode
                )
                
                session = LLMSession(
                    session_key=base_session_key,
                    context=context
                )
                
                # Initialiser l'agent
                logger.info(f"Initialisation de l'agent...")
                await session.initialize_agent()
                logger.info(f"Agent initialis√© avec succ√®s")
                
                # Stocker en cache
                logger.info(f"Stockage de la session en cache...")
                self.sessions[base_session_key] = session
                logger.info(f"Session stock√©e en cache")
                
                logger.info(f"Nouvelle session cr√©√©e: {base_session_key}")
                logger.info(f"=== FIN initialize_session ===")
                return {
                    "success": True,
                    "session_id": base_session_key,
                    "status": "created",
                    "message": "Session LLM initialis√©e avec succ√®s"
                }
                
        except Exception as e:
            logger.error(f"Erreur initialisation session LLM: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "√âchec de l'initialisation LLM"
            }
    
    async def send_message(
        self,
        user_id: str,
        collection_name: str,
        thread_key: str,
        message: str,
        chat_mode: str = "general_chat",
        system_prompt: str = None,
        selected_tool: str = None
    ) -> dict:
        """Envoie un message √† l'agent LLM et √©crit la r√©ponse dans Firebase RTDB."""
        try:
            base_session_key = f"{user_id}:{collection_name}"
            
            logger.info(f"=== D√âBUT send_message ===")
            logger.info(f"Envoi message pour session: {base_session_key}, thread: {thread_key}")
            logger.info(f"Param√®tres re√ßus: user_id={user_id}, collection_name={collection_name}")
            logger.info(f"Message: {message[:100]}...")
            logger.info(f"System prompt: {system_prompt}")
            logger.info(f"Selected tool: {selected_tool}")
            
            # R√©cup√©rer ou cr√©er la session (lock MINIMAL pour acc√®s au dict global)
            logger.info(f"V√©rification de l'existence de la session: {base_session_key}")
            with self._lock:
                # V√©rifier rapidement si la session existe
                session_exists = base_session_key in self.sessions
                if session_exists:
                    session = self.sessions[base_session_key]
                    logger.info(f"Session existante trouv√©e")
            
            # Si session n'existe pas, l'initialiser (hors du lock global)
            if not session_exists:
                logger.info(f"Session non trouv√©e, initialisation...")
                init_result = await self.initialize_session(
                    user_id, collection_name, chat_mode=chat_mode
                )
                if not init_result.get("success"):
                    logger.error(f"√âchec de l'initialisation: {init_result}")
                    return init_result
                logger.info(f"Session initialis√©e avec succ√®s")
                
                # R√©cup√©rer la session nouvellement cr√©√©e
                with self._lock:
                    session = self.sessions[base_session_key]
            
            logger.info(f"Session r√©cup√©r√©e: {type(session)}")
            
            # G√©n√©rer IDs pour les messages
            logger.info(f"G√©n√©ration des IDs pour les messages...")
            user_message_id = str(uuid.uuid4())
            assistant_message_id = str(uuid.uuid4())
            logger.info(f"User message ID: {user_message_id}")
            logger.info(f"Assistant message ID: {assistant_message_id}")
            
            # √âcrire le message utilisateur dans Firebase RTDB
            user_msg_path = f"{collection_name}/chats/{thread_key}/messages/{user_message_id}"
            user_msg_ref = self._get_rtdb_ref(user_msg_path)
            user_msg_ref.set({
                "role": "user",
                "content": message,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "user_id": user_id,
                "read": False
            })
            logger.info(f"Message utilisateur √©crit dans RTDB: {user_msg_path}")
            
            # Cr√©er un message assistant "vide" (pour le streaming)
            # IMPORTANT: Capturer le timestamp pour le r√©utiliser √† la fin
            assistant_timestamp = datetime.now(timezone.utc).isoformat()
            assistant_msg_path = f"{collection_name}/chats/{thread_key}/messages/{assistant_message_id}"
            assistant_msg_ref = self._get_rtdb_ref(assistant_msg_path)
            assistant_msg_ref.set({
                "role": "assistant",
                "content": "",
                "timestamp": assistant_timestamp,
                "status": "streaming",
                "streaming_progress": 0.0,
                "read": False
            })
            logger.info(f"Message assistant cr√©√© dans RTDB: {assistant_msg_path}")
            
            # Lancer le traitement en arri√®re-plan
            logger.info(f"=== LANCEMENT DU TRAITEMENT EN ARRI√àRE-PLAN ===")
            logger.info(f"Lancement du traitement en arri√®re-plan pour thread: {thread_key}")
            logger.info(f"Session: {session}")
            logger.info(f"Assistant message ID: {assistant_message_id}")
            logger.info(f"Message: {message[:100]}...")
            logger.info(f"System prompt: {system_prompt}")
            logger.info(f"Selected tool: {selected_tool}")
            
            logger.info(f"Cr√©ation de la t√¢che asyncio...")
            task = asyncio.create_task(
                self._process_message_with_ws_streaming(
                    session=session,
                    user_id=user_id,
                    collection_name=collection_name,
                    thread_key=thread_key,
                    assistant_message_id=assistant_message_id,
                    assistant_timestamp=assistant_timestamp,
                    message=message,
                    system_prompt=system_prompt,
                    selected_tool=selected_tool
                )
            )
            
            # Enregistrer le stream pour contr√¥le d'arr√™t
            await self.streaming_controller.register_stream(
                session_key=base_session_key,
                thread_key=thread_key,
                task=task
            )
            
            logger.info(f"T√¢che de traitement lanc√©e pour thread: {thread_key}")
            logger.info(f"=== FIN send_message ===")
            
            # Format de canal WebSocket identique √† RTDB pour faciliter la transition
            ws_channel = f"chat:{user_id}:{collection_name}:{thread_key}"
            
            return {
                "success": True,
                "user_message_id": user_message_id,
                "assistant_message_id": assistant_message_id,
                "ws_channel": ws_channel,
                "message": "Message envoy√©, r√©ponse en cours de streaming via WebSocket"
            }
            
        except Exception as e:
            logger.error(f"Erreur envoi message LLM: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e)
            }
    
    async def update_context(
        self,
        user_id: str,
        collection_name: str,
        system_prompt: str = None
    ) -> dict:
        """
        Met √† jour le contexte de soci√©t√© pour le LLM.
        Appelle update_system_prompt de BaseAIAgent.
        
        Args:
            user_id: ID de l'utilisateur
            collection_name: ID de la soci√©t√©
            system_prompt: Prompt syst√®me personnalis√© (optionnel)
        """
        try:
            base_session_key = f"{user_id}:{collection_name}"
            
            logger.info(f"Mise √† jour contexte pour session: {base_session_key}")
            
            # R√©cup√©rer la session existante (lock MINIMAL)
            with self._lock:
                if base_session_key not in self.sessions:
                    return {
                        "success": False,
                        "error": "Session non trouv√©e",
                        "message": "Session LLM non initialis√©e"
                    }
                session = self.sessions[base_session_key]
            
            # Mettre √† jour le prompt syst√®me via BaseAIAgent
            if session.agent:
                # Utiliser le system_prompt fourni ou cr√©er un prompt par d√©faut
                if system_prompt:
                    new_system_prompt = system_prompt
                    logger.info(f"System prompt fourni: {system_prompt[:100]}...")
                else:
                    # Cr√©er un nouveau prompt syst√®me bas√© sur le contexte
                    new_system_prompt = f"""
                    Contexte mis √† jour pour l'utilisateur {user_id} dans la soci√©t√© {collection_name}.
                    Vous √™tes maintenant configur√© pour cette soci√©t√© sp√©cifique.
                    """
                    logger.info(f"System prompt par d√©faut cr√©√© pour {collection_name}")
                
                # Appeler update_system_prompt de BaseAIAgent
                session.agent.update_system_prompt(new_system_prompt)
                
                logger.info(f"Contexte mis √† jour pour session: {base_session_key}")
                
                return {
                    "success": True,
                    "message": "Contexte mis √† jour avec succ√®s",
                    "session_id": base_session_key
                }
            else:
                return {
                    "success": False,
                    "error": "Agent non initialis√©",
                    "message": "Agent LLM non disponible"
                }
                
        except Exception as e:
            logger.error(f"Erreur update_context: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "√âchec de la mise √† jour du contexte"
            }
    
    async def load_chat_history(
        self,
        user_id: str,
        collection_name: str,
        thread_key: str,
        history: list
    ) -> dict:
        """
        Charge l'historique d'un chat dans la session LLM.
        
        Cette m√©thode doit √™tre appel√©e chaque fois qu'on change de chat
        pour que le microservice ait le contexte complet de la conversation.
        
        Args:
            user_id: ID Firebase de l'utilisateur
            collection_name: ID de la soci√©t√© (space_code)
            thread_key: Cl√© du thread de chat
            history: Historique du chat au format [{"role": "user", "content": "..."}, ...]
            
        Returns:
            dict: {"success": bool, "message": str, "loaded_messages": int}
        """
        try:
            base_session_key = f"{user_id}:{collection_name}"
            
            logger.info(f"üìö Chargement historique pour session: {base_session_key}, thread: {thread_key}")
            logger.info(f"Nombre de messages √† charger: {len(history)}")
            
            # R√©cup√©rer la session existante (lock MINIMAL)
            with self._lock:
                if base_session_key not in self.sessions:
                    return {
                        "success": False,
                        "error": "Session non trouv√©e",
                        "message": "Session LLM non initialis√©e. Appelez initialize_session d'abord.",
                        "loaded_messages": 0
                    }
                session = self.sessions[base_session_key]
            
            # Charger l'historique dans l'agent BaseAIAgent
            if session.agent:
                # Utiliser la m√©thode load_chat_history de BaseAIAgent
                session.agent.load_chat_history(
                    provider=ModelProvider.ANTHROPIC,  # Provider par d√©faut
                    history=history
                )
                
                # Mettre √† jour l'historique local de la session
                session.conversations[thread_key] = history.copy()
                
                # Mettre √† jour la derni√®re activit√©
                session.last_activity[thread_key] = datetime.now(timezone.utc)
                
                logger.info(f"‚úÖ Historique charg√©: {len(history)} messages pour thread {thread_key}")
                
                return {
                    "success": True,
                    "message": f"Historique charg√© avec succ√®s: {len(history)} messages",
                    "loaded_messages": len(history),
                    "session_id": base_session_key,
                    "thread_key": thread_key
                }
            else:
                return {
                    "success": False,
                    "error": "Agent non initialis√©",
                    "message": "Agent LLM non disponible",
                    "loaded_messages": 0
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement historique: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": f"√âchec du chargement de l'historique: {str(e)}",
                "loaded_messages": 0
            }
    
    async def flush_chat_history(
        self,
        user_id: str,
        collection_name: str,
        provider: str = "ANTHROPIC"
    ) -> dict:
        """
        Vide l'historique de chat de l'agent BaseAIAgent.
        
        Cette m√©thode r√©initialise compl√®tement l'historique des conversations
        pour permettre de d√©marrer une nouvelle conversation sans contexte.
        
        Args:
            user_id: ID Firebase de l'utilisateur
            collection_name: ID de la soci√©t√© (space_code)
            provider: Provider LLM (par d√©faut "ANTHROPIC")
            
        Returns:
            dict: {"success": bool, "message": str}
        """
        try:
            base_session_key = f"{user_id}:{collection_name}"
            
            logger.info(f"üóëÔ∏è Flush historique pour session: {base_session_key}")
            
            # R√©cup√©rer la session existante (lock MINIMAL)
            with self._lock:
                if base_session_key not in self.sessions:
                    return {
                        "success": False,
                        "error": "Session non trouv√©e",
                        "message": "Session LLM non initialis√©e. Appelez initialize_session d'abord."
                    }
                session = self.sessions[base_session_key]
            
            # Vider l'historique dans l'agent BaseAIAgent
            if session.agent:
                # Convertir le provider string en enum
                try:
                    provider_enum = ModelProvider[provider.upper()]
                except KeyError:
                    provider_enum = ModelProvider.ANTHROPIC
                
                # Appeler flush_chat_history de BaseAIAgent
                session.agent.flush_chat_history(provider=provider_enum)
                
                # Vider √©galement l'historique local de la session
                session.conversations.clear()
                session.last_activity.clear()
                
                logger.info(f"‚úÖ Historique vid√© pour session {base_session_key}")
                
                return {
                    "success": True,
                    "message": f"Historique de chat vid√© avec succ√®s pour le provider {provider}",
                    "session_id": base_session_key
                }
            else:
                return {
                    "success": False,
                    "error": "Agent non initialis√©",
                    "message": "Agent LLM non disponible"
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur flush historique: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": f"√âchec du flush de l'historique: {str(e)}"
            }
    
    async def stop_streaming(
        self,
        user_id: str,
        collection_name: str,
        thread_key: str = None
    ) -> dict:
        """
        Arr√™te le streaming via WebSocket pour un thread sp√©cifique ou tous les threads.
        
        Args:
            user_id: ID de l'utilisateur
            collection_name: ID de la soci√©t√©
            thread_key: Thread sp√©cifique (optionnel, arr√™te tous si omis)
        """
        try:
            base_session_key = f"{user_id}:{collection_name}"
            
            logger.info(f"Arr√™t streaming pour session: {base_session_key}")
            
            if thread_key:
                # Arr√™ter un thread sp√©cifique
                success = await self.streaming_controller.stop_stream(base_session_key, thread_key)
                if success:
                    logger.info(f"Stream arr√™t√© pour thread: {thread_key}")
                    return {
                        "success": True,
                        "message": f"Stream arr√™t√© pour thread {thread_key}",
                        "thread_key": thread_key
                    }
                else:
                    return {
                        "success": False,
                        "error": "Thread non trouv√© ou d√©j√† arr√™t√©",
                        "message": f"Thread {thread_key} non trouv√©"
                    }
            else:
                # Arr√™ter tous les threads de la session
                stopped_count = await self.streaming_controller.stop_all_streams(base_session_key)
                
                logger.info(f"Tous les streams arr√™t√©s: {stopped_count}")
                return {
                    "success": True,
                    "message": f"Tous les streams arr√™t√©s ({stopped_count} threads)",
                    "stopped_count": stopped_count
                }
                
        except Exception as e:
            logger.error(f"Erreur stop_streaming: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "√âchec de l'arr√™t du streaming"
            }
    
    async def _process_message_with_ws_streaming(
        self,
        session: LLMSession,
        user_id: str,
        collection_name: str,
        thread_key: str,
        assistant_message_id: str,
        assistant_timestamp: str,
        message: str,
        system_prompt: str = None,
        selected_tool: str = None
    ):
        """Traite le message et stream la r√©ponse via WebSocket + 1 √©criture RTDB finale."""
        
        base_session_key = f"{user_id}:{collection_name}"
        ws_channel = f"chat:{user_id}:{collection_name}:{thread_key}"
        accumulated_content = ""
        
        try:
            logger.info(f"Traitement message avec streaming WebSocket pour thread: {thread_key}")
            logger.info(f"Canal WebSocket: {ws_channel}")
            
            # Importer hub WebSocket
            from ..ws_hub import hub
            
            # G√©rer selected_tool si fourni
            final_system_prompt = system_prompt
            logger.info(f"System prompt re√ßu: {system_prompt}")
            logger.info(f"Selected tool: {selected_tool}")
            if selected_tool:
                tool_prompt = f"Utilise l'outil s√©lectionn√© : {selected_tool}"
                if final_system_prompt:
                    final_system_prompt = f"{final_system_prompt}\n\n{tool_prompt}"
                else:
                    final_system_prompt = tool_prompt
                logger.info(f"Outil s√©lectionn√© : {selected_tool}")
            
            logger.info(f"Final system prompt: {final_system_prompt}")
            
            # 1Ô∏è‚É£ Notifier le d√©but du streaming via WebSocket
            await hub.broadcast(user_id, {
                "type": "llm_stream_start",
                "channel": ws_channel,
                "payload": {
                    "message_id": assistant_message_id,
                    "thread_key": thread_key,
                    "space_code": collection_name,
                    "timestamp": assistant_timestamp
                }
            })
            
            # 2Ô∏è‚É£ Stream depuis l'agent LLM
            logger.info(f"D√©but du streaming depuis l'agent LLM...")
            chunk_count = 0
            try:
                async for chunk in session.process_message_streaming(
                    thread_key, 
                    message,
                    system_prompt=final_system_prompt
                ):
                    chunk_count += 1
                    chunk_content = chunk.get("content", "")
                    is_final = chunk.get("is_final", False)
                    
                    accumulated_content += chunk_content
                    
                    logger.info(f"Chunk #{chunk_count} re√ßu: '{chunk_content[:50]}...' (final: {is_final})")
                    
                    # üöÄ BROADCAST IMM√âDIAT via WebSocket (aucun buffer)
                    await hub.broadcast(user_id, {
                        "type": "llm_stream_chunk",
                        "channel": ws_channel,
                        "payload": {
                            "message_id": assistant_message_id,
                            "thread_key": thread_key,
                            "space_code": collection_name,
                            "chunk": chunk_content,
                            "accumulated": accumulated_content,
                            "is_final": is_final
                        }
                    })
                    
                    # Si c'est le chunk final, on peut sortir de la boucle
                    if is_final:
                        logger.info(f"Chunk final re√ßu, fin du streaming")
                        break
                        
            except asyncio.CancelledError:
                logger.info(f"Streaming interrompu par l'utilisateur")
                # Notifier l'interruption via WebSocket
                await hub.broadcast(user_id, {
                    "type": "llm_stream_interrupted",
                    "channel": ws_channel,
                    "payload": {
                        "message_id": assistant_message_id,
                        "thread_key": thread_key,
                        "space_code": collection_name,
                        "accumulated": accumulated_content
                    }
                })
                raise
            except Exception as streaming_error:
                logger.error(f"Erreur pendant le streaming: {streaming_error}")
                # Notifier l'erreur via WebSocket
                await hub.broadcast(user_id, {
                    "type": "llm_stream_error",
                    "channel": ws_channel,
                    "payload": {
                        "message_id": assistant_message_id,
                        "thread_key": thread_key,
                        "space_code": collection_name,
                        "error": str(streaming_error)
                    }
                })
                raise
            
            logger.info(f"Streaming termin√©. Total chunks: {chunk_count}")
            logger.info(f"Contenu final accumul√©: '{accumulated_content[:200]}...'")
            
            # 3Ô∏è‚É£ UNE SEULE √âCRITURE RTDB FINALE (persistence)
            assistant_msg_path = f"{collection_name}/chats/{thread_key}/messages/{assistant_message_id}"
            assistant_msg_ref = self._get_rtdb_ref(assistant_msg_path)
            
            final_message_data = self.rtdb_formatter.format_ai_message(
                content=accumulated_content,
                user_id=user_id,
                message_id=assistant_message_id,
                timestamp=assistant_timestamp,
                metadata={
                    "tokens_used": session.get_token_stats(thread_key),
                    "duration_ms": session.get_last_response_duration_ms(thread_key),
                    "model": "claude-3-7-sonnet-20250219",
                    "status": "complete",
                    "streaming_progress": 1.0,
                    "completed_at": datetime.now(timezone.utc).isoformat()
                }
            )
            
            logger.info(f"√âcriture finale dans RTDB: {assistant_msg_path}")
            assistant_msg_ref.set(final_message_data)
            
            # 4Ô∏è‚É£ Notifier la fin du streaming via WebSocket
            await hub.broadcast(user_id, {
                "type": "llm_stream_complete",
                "channel": ws_channel,
                "payload": {
                    "message_id": assistant_message_id,
                    "thread_key": thread_key,
                    "space_code": collection_name,
                    "full_content": accumulated_content,
                    "metadata": final_message_data.get("metadata", {})
                }
            })
            
            # D√©senregistrer le stream termin√©
            await self.streaming_controller.unregister_stream(base_session_key, thread_key)
            
            logger.info(f"Message assistant compl√©t√© (WebSocket + RTDB)")
            
        except asyncio.CancelledError:
            logger.info(f"T√¢che de streaming annul√©e")
            # D√©senregistrer le stream
            await self.streaming_controller.unregister_stream(base_session_key, thread_key)
            raise
        except Exception as e:
            logger.error(f"Erreur streaming WebSocket: {e}", exc_info=True)
            
            # Marquer comme erreur dans Firebase RTDB
            try:
                assistant_msg_path = f"{collection_name}/chats/{thread_key}/messages/{assistant_message_id}"
                assistant_msg_ref = self._get_rtdb_ref(assistant_msg_path)
                assistant_msg_ref.update({
                    "status": "error",
                    "error": str(e),
                    "error_at": datetime.now(timezone.utc).isoformat()
                })
            except Exception as update_error:
                logger.error(f"Impossible de mettre √† jour l'erreur dans RTDB: {update_error}")
            
            # Notifier l'erreur via WebSocket
            try:
                from ..ws_hub import hub
                await hub.broadcast(user_id, {
                    "type": "llm_stream_error",
                    "channel": ws_channel,
                    "payload": {
                        "message_id": assistant_message_id,
                        "thread_key": thread_key,
                        "space_code": collection_name,
                        "error": str(e)
                    }
                })
            except Exception:
                pass
            
            # D√©senregistrer le stream
            await self.streaming_controller.unregister_stream(base_session_key, thread_key)


# Singleton pour le gestionnaire LLM
_llm_manager: Optional[LLMManager] = None

def get_llm_manager() -> LLMManager:
    """R√©cup√®re l'instance singleton du LLM Manager."""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager


