"""
Gestionnaire de cache Redis unifiÃ© pour tous les modules Pinnokio.

Ce module implÃ©mente un cache asynchrone gÃ©nÃ©rique pour optimiser les performances
des requÃªtes vers les diffÃ©rentes sources de donnÃ©es (PostgreSQL, Firebase, Google Drive).

Architecture:
    - Cache-first: Tentative de lecture depuis Redis avant la source
    - Write-through: Mise Ã  jour du cache aprÃ¨s Ã©criture source
    - Invalidation sÃ©lective: Suppression ciblÃ©e aprÃ¨s modifications

Structure des clÃ©s Redis:
    - cache:{user_id}:{company_id}:{data_type}:{sub_type}

Exemples:
    - cache:{user_id}:{company_id}:hr:employees
    - cache:{user_id}:{company_id}:expenses:details
    - cache:{user_id}:{company_id}:drive:documents
    - cache:{user_id}:{company_id}:apbookeeper:documents
    - cache:{user_id}:{company_id}:bank:transactions
    - cache:{user_id}:{company_id}:mandate:snapshot

TTLs configurables par module via RedisTTL.
"""

import json
import logging
from datetime import datetime
from typing import Optional, Dict, Any
import redis.asyncio as redis
import os

logger = logging.getLogger("cache.unified")


class UnifiedCacheManager:
    """
    Gestionnaire de cache Redis asynchrone unifiÃ© pour tous les modules.

    Utilise redis.asyncio pour la cohÃ©rence avec l'architecture async/await.
    Suit la structure de clÃ©s existante du projet.
    """

    def __init__(self, log_prefix: str = "CACHE"):
        """
        Initialise le cache manager.

        Args:
            log_prefix: PrÃ©fixe pour les logs (ex: "HR_CACHE", "FIREBASE_CACHE")
        """
        self.redis_client: Optional[redis.Redis] = None
        self._connection_config = None
        self.log_prefix = log_prefix

    async def _get_redis_client(self) -> redis.Redis:
        """
        RÃ©cupÃ¨re le client Redis async (mÃªme configuration que les listeners).
        """
        if self.redis_client is None:
            self._connection_config = self._load_redis_config()

            self.redis_client = redis.Redis(
                host=self._connection_config.get("host"),
                port=self._connection_config.get("port", 6379),
                password=self._connection_config.get("password"),
                ssl=self._connection_config.get("tls", False),
                db=self._connection_config.get("db", 0),
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )
            logger.info(f"âœ… [{self.log_prefix}] Client Redis async initialisÃ©")

        return self.redis_client

    def _load_redis_config(self) -> Dict:
        """
        Charge la configuration Redis depuis les variables d'environnement.
        Utilise la mÃªme configuration que listeners_manager.
        """
        use_local = os.getenv("USE_LOCAL_REDIS", "false").lower() == "true"

        if use_local:
            return {
                "host": "127.0.0.1",
                "port": 6379,
                "password": None,
                "tls": False,
                "db": int(os.getenv("LISTENERS_REDIS_DB", "0")),
            }
        else:
            return {
                "host": os.getenv("LISTENERS_REDIS_HOST", "localhost"),
                "port": int(os.getenv("LISTENERS_REDIS_PORT", "6379")),
                "password": os.getenv("LISTENERS_REDIS_PASSWORD"),
                "tls": os.getenv("LISTENERS_REDIS_TLS", "false").lower() == "true",
                "db": int(os.getenv("LISTENERS_REDIS_DB", "0")),
            }

    def _build_cache_key(
        self,
        user_id: str,
        company_id: str,
        data_type: str,
        sub_type: str = None
    ) -> str:
        """
        Construit une clÃ© de cache standardisÃ©e conforme Ã  l'existant.

        Format: cache:{user_id}:{company_id}:{data_type}[:sub_type]

        Exemples:
            - cache:uid123:comp456:hr:employees
            - cache:uid123:comp456:expenses:details
            - cache:uid123:comp456:drive:documents
        """
        key = f"cache:{user_id}:{company_id}:{data_type}"
        if sub_type:
            key += f":{sub_type}"
        return key

    async def get_cached_data(
        self,
        user_id: str,
        company_id: str,
        data_type: str,
        sub_type: str = None,
        ttl_seconds: int = 3600
    ) -> Optional[Dict]:
        """
        RÃ©cupÃ¨re des donnÃ©es du cache Redis.

        Args:
            user_id: Firebase UID de l'utilisateur
            company_id: UUID ou ID de la sociÃ©tÃ©
            data_type: Type de donnÃ©es (ex: "hr", "expenses", "drive")
            sub_type: Sous-type (ex: "employees", "details", "documents")
            ttl_seconds: TTL suggÃ©rÃ© (non utilisÃ© en lecture, info seulement)

        Returns:
            Dict avec structure {"data": ..., "cached_at": ..., "source": "cache"}
            ou None si non trouvÃ©
        """
        cache_key = self._build_cache_key(user_id, company_id, data_type, sub_type)
        logger.info(f"ðŸ” [{self.log_prefix}] Tentative de rÃ©cupÃ©ration: {cache_key}")

        try:
            redis_client = await self._get_redis_client()

            # Tentative de rÃ©cupÃ©ration depuis le cache
            cached_data = await redis_client.get(cache_key)

            if cached_data:
                data = json.loads(cached_data)
                cache_info = data.get("cached_at", "unknown")
                data_content = data.get("data", {})

                # Validation: vÃ©rifier que les donnÃ©es ne sont pas vides
                if isinstance(data_content, list):
                    total_items = len(data_content)
                    logger.info(
                        f"âœ… [{self.log_prefix}] HIT: {cache_key} | "
                        f"Cached: {cache_info} | Items: {total_items}"
                    )

                    # Rejeter les listes vides et forcer le fallback
                    if total_items == 0:
                        logger.warning(
                            f"âš ï¸ [{self.log_prefix}] DonnÃ©es VIDES dÃ©tectÃ©es: {cache_key}"
                        )
                        await redis_client.delete(cache_key)
                        return None

                    return data
                elif isinstance(data_content, dict):
                    data_size = len(data_content)
                    logger.info(
                        f"âœ… [{self.log_prefix}] HIT: {cache_key} | "
                        f"Cached: {cache_info} | Keys: {data_size}"
                    )
                    return data
                else:
                    logger.info(f"âœ… [{self.log_prefix}] HIT: {cache_key} | Cached: {cache_info}")
                    return data

            # Cache miss
            logger.info(f"âŒ [{self.log_prefix}] MISS: {cache_key}")
            return None

        except Exception as e:
            logger.error(f"âŒ [{self.log_prefix}] Erreur lors de la rÃ©cupÃ©ration: {cache_key} | Error: {e}")
            # En cas d'erreur Redis, retourner None pour continuer avec la source
            return None

    async def set_cached_data(
        self,
        user_id: str,
        company_id: str,
        data_type: str,
        sub_type: str = None,
        data: Any = None,
        ttl_seconds: int = 3600
    ) -> bool:
        """
        Stocke des donnÃ©es dans le cache Redis.

        Args:
            user_id: Firebase UID de l'utilisateur
            company_id: UUID ou ID de la sociÃ©tÃ©
            data_type: Type de donnÃ©es (ex: "hr", "expenses", "drive")
            sub_type: Sous-type (ex: "employees", "details", "documents")
            data: DonnÃ©es Ã  mettre en cache
            ttl_seconds: DurÃ©e de vie du cache en secondes

        Returns:
            True si succÃ¨s, False sinon
        """
        cache_key = self._build_cache_key(user_id, company_id, data_type, sub_type)
        logger.info(f"ðŸ’¾ [{self.log_prefix}] Tentative de stockage: {cache_key} | TTL: {ttl_seconds}s")

        try:
            if data is None:
                logger.warning(f"âš ï¸ [{self.log_prefix}] DonnÃ©es None pour: {cache_key}")
                return False

            redis_client = await self._get_redis_client()

            # Calculer la taille des donnÃ©es
            data_size = len(str(data)) if data else 0
            logger.debug(f"ðŸ“Š [{self.log_prefix}] Taille des donnÃ©es: {data_size} caractÃ¨res")

            # Ajouter des mÃ©tadonnÃ©es de cache
            cached_payload = {
                "data": data,
                "cached_at": datetime.now().isoformat(),
                "ttl_seconds": ttl_seconds,
                "source": f"{data_type}.{sub_type}" if sub_type else data_type
            }

            # Stocker avec TTL
            await redis_client.setex(
                cache_key,
                ttl_seconds,
                json.dumps(cached_payload)
            )

            logger.info(
                f"âœ… [{self.log_prefix}] Stockage rÃ©ussi: {cache_key} | "
                f"TTL: {ttl_seconds}s | Taille: {data_size}"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ [{self.log_prefix}] Erreur de stockage: {cache_key} | Error: {e}")
            return False

    async def invalidate_cache(
        self,
        user_id: str,
        company_id: str,
        data_type: str,
        sub_type: str = None
    ) -> bool:
        """
        Invalide une entrÃ©e de cache spÃ©cifique.

        UtilisÃ© aprÃ¨s les opÃ©rations CRUD pour forcer le rechargement.

        Args:
            user_id: Firebase UID de l'utilisateur
            company_id: UUID ou ID de la sociÃ©tÃ©
            data_type: Type de donnÃ©es (ex: "hr", "expenses", "drive")
            sub_type: Sous-type (ex: "employees", "details", "documents")

        Returns:
            True si succÃ¨s, False sinon
        """
        cache_key = self._build_cache_key(user_id, company_id, data_type, sub_type)
        logger.info(f"ðŸ—‘ï¸ [{self.log_prefix}] Invalidation demandÃ©e: {cache_key}")

        try:
            redis_client = await self._get_redis_client()
            deleted = await redis_client.delete(cache_key)
            logger.info(f"âœ… [{self.log_prefix}] ClÃ© supprimÃ©e: {cache_key} | Deleted={deleted}")
            return True

        except Exception as e:
            logger.error(f"âŒ [{self.log_prefix}] Erreur d'invalidation: {cache_key} | Error: {e}")
            return False

    async def invalidate_module_cache(
        self,
        user_id: str,
        company_id: str,
        data_type: str
    ) -> bool:
        """
        Invalide tout le cache d'un module pour une sociÃ©tÃ© et un utilisateur.

        Utilise SCAN pour Ã©viter de bloquer Redis avec KEYS.

        Args:
            user_id: Firebase UID de l'utilisateur
            company_id: UUID ou ID de la sociÃ©tÃ©
            data_type: Type de donnÃ©es (ex: "hr", "expenses", "drive")

        Returns:
            True si succÃ¨s, False sinon
        """
        pattern = f"cache:{user_id}:{company_id}:{data_type}:*"
        logger.info(f"ðŸ—‘ï¸ [{self.log_prefix}] Invalidation module complÃ¨te: {pattern}")

        try:
            redis_client = await self._get_redis_client()

            # SCAN au lieu de KEYS - ne bloque pas Redis
            cursor = 0
            keys_to_delete = []

            while True:
                # Scanner par lots de 100 clÃ©s Ã  la fois
                cursor, batch = await redis_client.scan(
                    cursor=cursor,
                    match=pattern,
                    count=100
                )

                keys_to_delete.extend(batch)

                # Si cursor revient Ã  0, on a tout scannÃ©
                if cursor == 0:
                    break

            logger.info(f"ðŸ” [{self.log_prefix}] ClÃ©s trouvÃ©es pour invalidation: {len(keys_to_delete)}")

            if keys_to_delete:
                # Supprimer par lots de 1000 max
                batch_size = 1000
                total_deleted = 0
                for i in range(0, len(keys_to_delete), batch_size):
                    batch = keys_to_delete[i:i+batch_size]
                    await redis_client.delete(*batch)
                    total_deleted += len(batch)
                    logger.debug(
                        f"ðŸ—‘ï¸ [{self.log_prefix}] SupprimÃ© lot {i//batch_size + 1}: {len(batch)} clÃ©s"
                    )

                logger.info(
                    f"âœ… [{self.log_prefix}] Invalidation rÃ©ussie: {total_deleted} clÃ©s supprimÃ©es "
                    f"pour user={user_id}, company={company_id}, module={data_type}"
                )
            else:
                logger.info(f"â„¹ï¸ [{self.log_prefix}] Aucune clÃ© Ã  invalider pour: {pattern}")

            return True

        except Exception as e:
            logger.error(f"âŒ [{self.log_prefix}] Erreur d'invalidation: {pattern} | Error: {e}")
            return False

    async def get_cache_stats(
        self,
        user_id: str,
        company_id: str,
        data_type: str = None
    ) -> Dict:
        """
        Retourne les statistiques du cache pour une sociÃ©tÃ©.

        Utile pour le monitoring et le debugging.

        Args:
            user_id: Firebase UID de l'utilisateur
            company_id: UUID ou ID de la sociÃ©tÃ©
            data_type: Type de donnÃ©es optionnel (ex: "hr", "expenses")

        Returns:
            Dict avec statistiques (total_keys, data_types, etc.)
        """
        try:
            redis_client = await self._get_redis_client()

            if data_type:
                pattern = f"cache:{user_id}:{company_id}:{data_type}:*"
            else:
                pattern = f"cache:{user_id}:{company_id}:*"

            # SCAN pour trouver toutes les clÃ©s
            cursor = 0
            keys = []

            while True:
                cursor, batch = await redis_client.scan(
                    cursor=cursor,
                    match=pattern,
                    count=100
                )
                keys.extend(batch)
                if cursor == 0:
                    break

            stats = {
                "total_keys": len(keys),
                "data_types": {},
                "total_size_bytes": 0,
                "oldest_entry": None,
                "newest_entry": None
            }

            for key in keys:
                try:
                    data = await redis_client.get(key)
                    if data:
                        parsed = json.loads(data)

                        # Extraire le type de donnÃ©es depuis la clÃ©
                        # Format: cache:user:company:TYPE:SUBTYPE
                        key_parts = key.split(":")
                        key_data_type = key_parts[3] if len(key_parts) > 3 else "unknown"

                        if key_data_type not in stats["data_types"]:
                            stats["data_types"][key_data_type] = 0
                        stats["data_types"][key_data_type] += 1

                        stats["total_size_bytes"] += len(data)

                        cached_at = parsed.get("cached_at")
                        if cached_at:
                            if not stats["oldest_entry"] or cached_at < stats["oldest_entry"]:
                                stats["oldest_entry"] = cached_at
                            if not stats["newest_entry"] or cached_at > stats["newest_entry"]:
                                stats["newest_entry"] = cached_at

                except Exception:
                    continue

            logger.info(f"ðŸ“Š [{self.log_prefix}] Stats: {stats['total_keys']} clÃ©s, {stats['total_size_bytes']} bytes")
            return stats

        except Exception as e:
            logger.error(f"âš ï¸ [{self.log_prefix}] Stats error: {e}")
            return {"error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INSTANCES SPÃ‰CIALISÃ‰ES (pour chaque module)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_firebase_cache_manager: Optional[UnifiedCacheManager] = None
_drive_cache_manager: Optional[UnifiedCacheManager] = None


def get_firebase_cache_manager() -> UnifiedCacheManager:
    """
    Retourne l'instance singleton du cache manager Firebase.

    Usage:
        from app.cache.unified_cache_manager import get_firebase_cache_manager

        cache = get_firebase_cache_manager()
        cached = await cache.get_cached_data(user_id, company_id, "expenses", "details")
    """
    global _firebase_cache_manager
    if _firebase_cache_manager is None:
        _firebase_cache_manager = UnifiedCacheManager(log_prefix="FIREBASE_CACHE")
    return _firebase_cache_manager


def get_drive_cache_manager() -> UnifiedCacheManager:
    """
    Retourne l'instance singleton du cache manager Drive.

    Usage:
        from app.cache.unified_cache_manager import get_drive_cache_manager

        cache = get_drive_cache_manager()
        cached = await cache.get_cached_data(user_id, company_id, "drive", "documents")
    """
    global _drive_cache_manager
    if _drive_cache_manager is None:
        _drive_cache_manager = UnifiedCacheManager(log_prefix="DRIVE_CACHE")
    return _drive_cache_manager
