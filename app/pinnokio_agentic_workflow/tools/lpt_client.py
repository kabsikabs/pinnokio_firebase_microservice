"""
Client LPT (Long Process Tooling) pour l'agent Pinnokio.
G√®re les appels HTTP vers les agents externes (APBookkeeper, Router, Banker).
"""

import logging
import uuid
import aiohttp
import asyncio
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone

logger = logging.getLogger("pinnokio.lpt_client")


class LPTClient:
    """
    Client pour g√©rer les outils LPT (Long Process Tooling).
    
    Responsabilit√©s :
    1. Fournir des d√©finitions d'outils SIMPLIFI√âES pour l'agent (seulement IDs + instructions)
    2. Construire automatiquement les payloads complets avec les valeurs de contexte
    3. Envoyer les requ√™tes HTTP vers les agents externes
    4. Cr√©er les notifications Firebase
    5. Sauvegarder les t√¢ches dans Firebase pour suivi UI
    """
    
    def __init__(self):
        # Configuration environnement LOCAL vs PROD
        self.environment = os.getenv('PINNOKIO_ENVIRONMENT', 'LOCAL').upper()
        self.source = os.getenv('PINNOKIO_SOURCE', 'aws')

        # URLs par environnement
        if self.environment == 'LOCAL':
            self.base_url = 'http://127.0.0.1'
            self.router_url = f"{self.base_url}:8080/event-trigger"
            self.apbookeeper_url = f"{self.base_url}:8081/apbookeeper-event-trigger"
            self.banker_url = f"{self.base_url}:8082/banker-event-trigger"
            self.onboarding_url = f"{self.base_url}:8080/onboarding_manager_agent"
        else:  # PROD
            self.base_url = os.getenv('PINNOKIO_AWS_URL', 'http://klk-load-balancer-http-https-435479360.us-east-1.elb.amazonaws.com')
            # En PROD, tous les services utilisent le m√™me ALB AWS
            self.router_url = f"{self.base_url}/event-trigger"
            self.apbookeeper_url = f"{self.base_url}/apbookeeper-event-trigger"
            self.banker_url = f"{self.base_url}/banker-event-trigger"
            self.onboarding_url = f"{self.base_url}/onboarding_manager_agent"

        logger.info(f"LPTClient initialis√© (env={self.environment}, source={self.source})")
        logger.info(
            f"URLs - Router: {self.router_url}, APBookkeeper: {self.apbookeeper_url}, Banker: {self.banker_url}, "
            f"Onboarding: {self.onboarding_url}"
        )
    
    def check_balance_before_lpt(
        self, 
        user_id: str = None,
        mandate_path: str = None,
        estimated_cost: float = 1.0,
        lpt_tool_name: str = "LPT"
    ) -> Dict[str, Any]:
        """
        V√©rifie si l'utilisateur a un solde suffisant avant d'ex√©cuter un outil LPT.
        
        Args:
            user_id: ID de l'utilisateur
            mandate_path: Chemin du mandat dans Firebase (prioritaire sur user_id)
            estimated_cost: Co√ªt estim√© de l'op√©ration en $ (par d√©faut 1.0)
            lpt_tool_name: Nom de l'outil LPT pour les logs (APBookkeeper, Router, Banker, etc.)
            
        Returns:
            dict: 
                - "sufficient": True si le solde est suffisant, False sinon
                - "current_balance": Solde actuel du compte
                - "required_balance": Solde requis (estimated_cost * 1.2)
                - "message": Message √† retourner √† l'agent si insuffisant
        """
        try:
            from ...firebase_providers import FirebaseManagement
            firebase_management = FirebaseManagement()
            
            # üîç R√©cup√©rer les informations de solde
            balance_info = firebase_management.get_balance_info(
                mandate_path=mandate_path,
                user_id=user_id
            )
            
            current_balance = balance_info.get('current_balance', 0.0)
            
            # üí∞ Calculer le solde requis (marge de s√©curit√© de 20%)
            required_balance = estimated_cost * 1.2
            
            is_sufficient = current_balance >= required_balance
            
            logger.info(
                f"[BALANCE_CHECK_{lpt_tool_name}] üí∞ V√©rification solde - "
                f"Solde actuel: {current_balance:.2f}$ | "
                f"Requis: {required_balance:.2f}$ (co√ªt estim√©: {estimated_cost:.2f}$) | "
                f"Statut: {'‚úÖ SUFFISANT' if is_sufficient else '‚ùå INSUFFISANT'}"
            )
            
            if not is_sufficient:
                # üì¢ Message clair √† retourner √† l'agent
                insufficient_message = (
                    f"‚ö†Ô∏è **SOLDE INSUFFISANT** ‚ö†Ô∏è\n\n"
                    f"L'ex√©cution de l'outil **{lpt_tool_name}** n√©cessite un solde minimum.\n\n"
                    f"üìä **√âtat du compte :**\n"
                    f"‚Ä¢ Solde actuel : **{current_balance:.2f} $**\n"
                    f"‚Ä¢ Solde requis : **{required_balance:.2f} $**\n"
                    f"‚Ä¢ Montant manquant : **{(required_balance - current_balance):.2f} $**\n\n"
                    f"üí° **Action requise :**\n"
                    f"Veuillez inviter l'utilisateur √† **recharger son compte** depuis le tableau de bord "
                    f"pour continuer √† utiliser les services.\n\n"
                    f"üîó L'utilisateur peut recharger son compte dans la section **Facturation** du tableau de bord."
                )
                
                logger.warning(
                    f"[BALANCE_CHECK_{lpt_tool_name}] ‚ö†Ô∏è SOLDE INSUFFISANT - "
                    f"Besoin de {(required_balance - current_balance):.2f}$ suppl√©mentaires"
                )
                
                return {
                    "sufficient": False,
                    "current_balance": current_balance,
                    "required_balance": required_balance,
                    "estimated_cost": estimated_cost,
                    "missing_amount": required_balance - current_balance,
                    "message": insufficient_message
                }
            
            return {
                "sufficient": True,
                "current_balance": current_balance,
                "required_balance": required_balance,
                "estimated_cost": estimated_cost
            }
            
        except Exception as e:
            logger.error(f"[BALANCE_CHECK_{lpt_tool_name}] ‚ùå Erreur v√©rification solde: {e}", exc_info=True)
            # En cas d'erreur, on autorise par d√©faut (failsafe)
            return {
                "sufficient": True,
                "current_balance": 0.0,
                "required_balance": 0.0,
                "error": str(e),
                "message": "‚ö†Ô∏è Impossible de v√©rifier le solde. Op√©ration autoris√©e par d√©faut."
            }
    
    def get_tools_definitions_and_mapping(
        self, 
        user_id: str, 
        company_id: str, 
        thread_key: str,
        session=None,  # ‚≠ê LLMSession pour cache contexte thread
        brain=None     # ‚≠ê NOUVEAU: PinnokioBrain pour acc√®s au contexte utilisateur
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Retourne les d√©finitions d'outils LPT SIMPLIFI√âES et leur mapping.
        
        Important : L'agent ne voit QUE les champs essentiels (IDs + instructions).
        Le reste est automatiquement construit par les fonctions de payload.
        
        Args:
            session: LLMSession (optionnel) pour cache contexte thread
            brain: PinnokioBrain (optionnel) pour acc√®s au contexte utilisateur d√©j√† charg√©
        """
        # D√©finitions d'outils SIMPLIFI√âES pour l'agent
        tools_list = [
            {
                "name": "LPT_APBookkeeper",
                "description": """üìã Saisie automatique de factures fournisseur (AP Bookkeeper).
                
Utilisez cet outil pour traiter et saisir des factures fournisseur dans l'ERP.

INSTRUCTIONS POUR L'AGENT :
- Fournissez UNIQUEMENT les IDs des fichiers (job_ids) √† traiter
- Ajoutez des instructions sp√©cifiques si n√©cessaire (optionnel)
- Tout le reste (collection, user, settings, approbations) est automatique

‚öôÔ∏è PARAM√àTRES AUTOMATIQUES (configur√©s dans les param√®tres syst√®me) :
- approval_required : Configur√© dans workflow_params
- approval_contact_creation : Configur√© dans workflow_params

EXEMPLE D'UTILISATION :
{
    "job_ids": ["file_abc123", "file_def456"],
    "general_instructions": "V√©rifier les montants HT/TTC",
    "file_instructions": {
        "file_abc123": "Facture urgente, prioriser"
    }
}""",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "job_ids": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Liste des IDs de fichiers (drive_file_id) √† traiter"
                        },
                        "general_instructions": {
                            "type": "string",
                            "description": "Instructions g√©n√©rales pour toutes les factures (optionnel)"
                        },
                        "file_instructions": {
                            "type": "object",
                            "description": "Instructions sp√©cifiques par fichier {file_id: instructions} (optionnel)"
                        }
                    },
                    "required": ["job_ids"]
                }
            },
            {
                "name": "LPT_Router",
                "description": """üóÇÔ∏è Routage automatique de documents (Router).
                
Utilisez cet outil pour router et classifier des documents automatiquement.

INSTRUCTIONS POUR L'AGENT :
- Fournissez l'ID du fichier Drive √† router
- Ajoutez des instructions si n√©cessaire (optionnel)
- Tout le reste (approbations, workflows) est automatique

‚öôÔ∏è PARAM√àTRES AUTOMATIQUES (configur√©s dans les param√®tres syst√®me) :
- approval_required : Configur√© dans workflow_params
- automated_workflow : Configur√© dans workflow_params

EXEMPLE D'UTILISATION :
{
    "drive_file_id": "file_xyz789",
    "instructions": "Router vers le dossier Factures"
}""",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "drive_file_id": {
                            "type": "string",
                            "description": "ID du fichier Drive √† router"
                        },
                        "instructions": {
                            "type": "string",
                            "description": "Instructions de routage (optionnel)"
                        }
                    },
                    "required": ["drive_file_id"]
                }
            },
            {
                "name": "LPT_Banker",
                "description": """üè¶ R√©conciliation bancaire automatique (Banker).
                
Utilisez cet outil pour r√©concilier des transactions bancaires avec l'ERP.

INSTRUCTIONS POUR L'AGENT :
- Fournissez le compte bancaire et les IDs de transactions
- Ajoutez des instructions si n√©cessaire (optionnel)

üìù TYPES D'INSTRUCTIONS :
- `instructions` : Instructions sp√©cifiques pour ce job (plac√©es dans jobs_data[].instructions)
- `start_instructions` : Instructions g√©n√©rales pour tout le batch (plac√©es au niveau racine du payload)
- `transaction_instructions` : Instructions par transaction {transaction_id: instructions} (optionnel)

‚öôÔ∏è PARAM√àTRES AUTOMATIQUES (configur√©s dans les param√®tres syst√®me) :
- approval_required : Configur√© dans workflow_params

EXEMPLE D'UTILISATION :
{
    "bank_account": "FR76 1234 5678 9012 3456",
    "transaction_ids": ["tx_001", "tx_002", "tx_003"],
    "instructions": "V√©rifier les doublons",
    "start_instructions": "Instructions g√©n√©rales pour tout le batch",
    "transaction_instructions": {
        "tx_001": "Transaction urgente √† traiter en priorit√©"
    }
}""",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "bank_account": {
                            "type": "string",
                            "description": "Compte bancaire concern√©"
                        },
                        "transaction_ids": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Liste des IDs de transactions √† r√©concilier"
                        },
                        "instructions": {
                            "type": "string",
                            "description": "Instructions sp√©cifiques pour ce job (plac√©es dans jobs_data[].instructions) - optionnel"
                        },
                        "start_instructions": {
                            "type": "string",
                            "description": "Instructions g√©n√©rales pour tout le batch (plac√©es au niveau racine) - optionnel"
                        },
                        "transaction_instructions": {
                            "type": "object",
                            "description": "Instructions par transaction {transaction_id: instructions} - optionnel",
                            "additionalProperties": {"type": "string"}
                        }
                    },
                    "required": ["bank_account", "transaction_ids"]
                }
            },
            {
                "name": "LPT_APBookkeeper_ALL",
                "description": """üöÄ Traitement complet des factures fournisseur disponibles.
                
Utilisez cet outil pour lancer automatiquement *toutes* les factures pr√™tes dans APBookkeeper (statut `to_do`).
L'outil construit le payload complet (job_ids, instructions, param√®tres syst√®me) sans intervention.

‚ö†Ô∏è Si aucune facture n'est disponible, l'outil retourne une alerte sans lancer de traitement.""",
                "input_schema": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            },
            {
                "name": "LPT_Router_ALL",
                "description": """üöÄ Routage automatique de tous les documents disponibles.
                
Utilisez cet outil pour router *tous* les documents en attente (statut `to_process`).
Le payload complet est g√©n√©r√© automatiquement avec instructions et param√®tres workflow.

‚ö†Ô∏è Aucun param√®tre requis. Si aucun document n'est disponible, l'outil retourne une alerte.""",
                "input_schema": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            },
            {
                "name": "LPT_Banker_ALL",
                "description": """üöÄ R√©conciliation bancaire globale.
                
Lance automatiquement la r√©conciliation de toutes les transactions disponibles.
- Sans argument ‚Üí toutes les banques
- Avec `bank_account` ‚Üí uniquement la banque cibl√©e (journal_id ou nom)

üìù INSTRUCTIONS :
- `start_instructions` : Instructions g√©n√©rales pour tout le batch (plac√©es au niveau racine) - optionnel

Le payload respecte le format notifications Banker (transactions regroup√©es par compte).
‚ö†Ô∏è Si aucune transaction √† traiter, l'outil retourne une alerte.""",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "bank_account": {
                            "type": "string",
                            "description": "Journal bancaire (ID ou nom) √† traiter en priorit√© (optionnel)."
                        },
                        "start_instructions": {
                            "type": "string",
                            "description": "Instructions g√©n√©rales pour tout le batch (plac√©es au niveau racine) - optionnel"
                        }
                    },
                    "required": []
                }
            },
            {
                "name": "LPT_STOP_APBookkeeper",
                "description": "‚èπÔ∏è Arr√™ter une t√¢che APBookkeeper en cours d'ex√©cution",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "job_id": {
                            "type": "string",
                            "description": "ID de la t√¢che √† arr√™ter"
                        }
                    },
                    "required": ["job_id"]
                }
            },
            {
                "name": "LPT_STOP_Router",
                "description": "‚èπÔ∏è Arr√™ter une t√¢che Router en cours d'ex√©cution",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "job_id": {
                            "type": "string",
                            "description": "ID de la t√¢che √† arr√™ter"
                        }
                    },
                    "required": ["job_id"]
                }
            },
            {
                "name": "LPT_STOP_Banker",
                "description": "‚èπÔ∏è Arr√™ter une t√¢che Banker en cours d'ex√©cution",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "job_id": {
                            "type": "string",
                            "description": "ID de la t√¢che √† arr√™ter"
                        },
                        "batch_id": {
                            "type": "string",
                            "description": "ID du batch (optionnel)"
                        }
                    },
                    "required": ["job_id"]
                }
            }
        ]
        
        # Mapping des outils vers leurs fonctions
        # ‚≠ê Utiliser des fonctions async explicites au lieu de lambdas
        # pour que asyncio.iscoroutinefunction() fonctionne correctement
        
        async def handle_lpt_apbookeeper(**kwargs):
            return await self.launch_apbookeeper(
                user_id=user_id, 
                company_id=company_id, 
                thread_key=thread_key,
                session=session,
                brain=brain,
                **kwargs
            )
        
        async def handle_lpt_router(**kwargs):
            return await self.launch_router(
                user_id=user_id, 
                company_id=company_id, 
                thread_key=thread_key,
                session=session,
                brain=brain,
                **kwargs
            )
        
        async def handle_lpt_banker(**kwargs):
            return await self.launch_banker(
                user_id=user_id, 
                company_id=company_id, 
                thread_key=thread_key,
                session=session,
                brain=brain,
                **kwargs
            )
        
        async def handle_lpt_apbookeeper_all(**kwargs):
            return await self.launch_apbookeeper_all(
                user_id=user_id,
                company_id=company_id,
                thread_key=thread_key,
                session=session,
                brain=brain,
                **kwargs
            )
        
        async def handle_lpt_router_all(**kwargs):
            return await self.launch_router_all(
                user_id=user_id,
                company_id=company_id,
                thread_key=thread_key,
                session=session,
                brain=brain,
                **kwargs
            )
        
        async def handle_lpt_banker_all(**kwargs):
            return await self.launch_banker_all(
                user_id=user_id,
                company_id=company_id,
                thread_key=thread_key,
                session=session,
                brain=brain,
                **kwargs
            )
        
        async def handle_lpt_stop_apbookeeper(**kwargs):
            return await self.stop_apbookeeper(
                user_id=user_id, 
                company_id=company_id, 
                **kwargs
            )
        
        async def handle_lpt_stop_router(**kwargs):
            return await self.stop_router(
                user_id=user_id, 
                company_id=company_id, 
                **kwargs
            )
        
        async def handle_lpt_stop_banker(**kwargs):
            return await self.stop_banker(
                user_id=user_id, 
                company_id=company_id, 
                **kwargs
            )
        
        tools_mapping = {
            "LPT_APBookkeeper": handle_lpt_apbookeeper,
            "LPT_Router": handle_lpt_router,
            "LPT_Banker": handle_lpt_banker,
            "LPT_APBookkeeper_ALL": handle_lpt_apbookeeper_all,
            "LPT_Router_ALL": handle_lpt_router_all,
            "LPT_Banker_ALL": handle_lpt_banker_all,
            "LPT_STOP_APBookkeeper": handle_lpt_stop_apbookeeper,
            "LPT_STOP_Router": handle_lpt_stop_router,
            "LPT_STOP_Banker": handle_lpt_stop_banker
        }
        
        return tools_list, tools_mapping
    
    async def launch_apbookeeper_all(
        self,
        user_id: str,
        company_id: str,
        thread_key: str,
        session=None,
        brain=None,
        execution_id: Optional[str] = None,
        execution_plan: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Lance automatiquement toutes les factures APBookkeeper disponibles (statut to_do).
        """
        try:
            if not brain:
                raise ValueError("Brain est requis pour lancer APBookkeeper_ALL")
            
            context = brain.get_user_context()
            mandate_path = context.get('mandate_path')
            
            # üîç Compter le nombre de factures √† traiter
            apbookeeper_jobs = ((brain.jobs_data or {}).get("APBOOKEEPER", {}) if brain else {}).get("to_do", []) or []
            nb_invoices = len(apbookeeper_jobs)
            
            # üõ°Ô∏è V√âRIFICATION DU SOLDE AVANT L'ENVOI
            # Co√ªt estim√© : 1.0$ par facture
            estimated_cost = nb_invoices * 1.0
            
            balance_check = self.check_balance_before_lpt(
                mandate_path=mandate_path,
                user_id=user_id,
                estimated_cost=estimated_cost,
                lpt_tool_name="APBookkeeper_ALL"
            )
            
            if not balance_check.get("sufficient", False):
                # ‚ùå SOLDE INSUFFISANT - Retourner le message √† l'agent
                logger.warning(
                    f"[LPT_APBookkeeper_ALL] ‚ùå BLOCAGE - Solde insuffisant "
                    f"({balance_check.get('current_balance', 0):.2f}$ < {balance_check.get('required_balance', 0):.2f}$)"
                )
                return {
                    "status": "insufficient_balance",
                    "error": "Solde insuffisant pour ex√©cuter cette op√©ration",
                    "balance_info": {
                        "current_balance": balance_check.get("current_balance", 0.0),
                        "required_balance": balance_check.get("required_balance", 0.0),
                        "missing_amount": balance_check.get("missing_amount", 0.0)
                    },
                    "nb_invoices_to_process": nb_invoices,
                    "message": balance_check.get("message", "Solde insuffisant")
                }
            
            # ‚úÖ SOLDE SUFFISANT - Continuer l'ex√©cution
            logger.info(
                f"[LPT_APBookkeeper_ALL] ‚úÖ Solde v√©rifi√© et suffisant "
                f"({balance_check.get('current_balance', 0):.2f}$ >= {balance_check.get('required_balance', 0):.2f}$) "
                f"pour {nb_invoices} factures"
            )
            
            ap_jobs = (brain.jobs_data or {}).get("APBOOKEEPER", {}) if brain else {}
            to_do_jobs = ap_jobs.get("to_do", []) or []
            
            job_ids: List[str] = []
            file_instructions: Dict[str, str] = {}
            
            for job in to_do_jobs:
                job_id = job.get("job_id")
                if not job_id:
                    continue
                job_ids.append(job_id)
                instruction = job.get("instructions") or job.get("instruction") or ""
                if instruction:
                    file_instructions[job_id] = instruction
            
            if not job_ids:
                logger.warning("[LPT_APBookkeeper_ALL] Aucun job to_do disponible pour lancement automatique.")
                return {
                    "status": "no_jobs_available",
                    "message": "Aucune facture APBookkeeper au statut 'to_do' n'est disponible pour lancement."
                }
            
            logger.info(f"[LPT_APBookkeeper_ALL] Lancement automatique de {len(job_ids)} facture(s).")
            
            result = await self.launch_apbookeeper(
                user_id=user_id,
                company_id=company_id,
                thread_key=thread_key,
                job_ids=job_ids,
                general_instructions=None,
                file_instructions=file_instructions if file_instructions else None,
                session=session,
                brain=brain,
                execution_id=execution_id,
                execution_plan=execution_plan
            )
            
            if isinstance(result, dict) and result.get("status") == "queued":
                result["nb_jobs_launched"] = len(job_ids)
                result["launched_from"] = "LPT_APBookkeeper_ALL"
            
            return result
        
        except Exception as e:
            logger.error(f"[LPT_APBookkeeper_ALL] Erreur: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def launch_apbookeeper(
        self,
        user_id: str,
        company_id: str,
        thread_key: str,
        job_ids: List[str],
        general_instructions: Optional[str] = None,
        file_instructions: Optional[Dict[str, str]] = None,
        session=None,  # ‚≠ê LLMSession pour cache (non utilis√©, contexte vient du brain)
        brain=None,    # ‚≠ê PinnokioBrain pour acc√®s au contexte utilisateur
        execution_id: Optional[str] = None,  # ‚≠ê ID d'ex√©cution pour tra√ßabilit√©
        execution_plan: Optional[str] = None  # ‚≠ê Mode d'ex√©cution (NOW, ON_DEMAND, etc.)
    ) -> Dict[str, Any]:
        """
        Lance une t√¢che APBookkeeper pour la saisie de factures fournisseur.
        
        Cette fonction construit automatiquement le payload complet et l'envoie √† l'agent APBookkeeper.
        Les param√®tres d'approbation (approval_required, approval_contact_creation) sont r√©cup√©r√©s
        automatiquement depuis workflow_params.
        """
        try:
            # ‚≠ê R√©cup√©rer le contexte depuis le brain (OBLIGATOIRE)
            if not brain:
                raise ValueError("Brain est requis pour lancer APBookkeeper")
            
            context = brain.get_user_context()
            mandate_path = context.get('mandate_path')
            
            # üõ°Ô∏è V√âRIFICATION DU SOLDE AVANT L'ENVOI
            # Co√ªt estim√© : 1.0$ par facture (ajustable selon vos tarifs)
            estimated_cost = len(job_ids) * 1.0
            
            balance_check = self.check_balance_before_lpt(
                mandate_path=mandate_path,
                user_id=user_id,
                estimated_cost=estimated_cost,
                lpt_tool_name="APBookkeeper"
            )
            
            if not balance_check.get("sufficient", False):
                # ‚ùå SOLDE INSUFFISANT - Retourner le message √† l'agent
                logger.warning(
                    f"[LPT_APBookkeeper] ‚ùå BLOCAGE - Solde insuffisant "
                    f"({balance_check.get('current_balance', 0):.2f}$ < {balance_check.get('required_balance', 0):.2f}$)"
                )
                return {
                    "status": "insufficient_balance",
                    "error": "Solde insuffisant pour ex√©cuter cette op√©ration",
                    "balance_info": {
                        "current_balance": balance_check.get("current_balance", 0.0),
                        "required_balance": balance_check.get("required_balance", 0.0),
                        "missing_amount": balance_check.get("missing_amount", 0.0)
                    },
                    "message": balance_check.get("message", "Solde insuffisant")
                }
            
            # ‚úÖ SOLDE SUFFISANT - Continuer l'ex√©cution
            logger.info(
                f"[LPT_APBookkeeper] ‚úÖ Solde v√©rifi√© et suffisant "
                f"({balance_check.get('current_balance', 0):.2f}$ >= {balance_check.get('required_balance', 0):.2f}$)"
            )
            logger.info(f"[LPT_APBookkeeper] Contexte r√©cup√©r√© depuis brain: mandate_path={context.get('mandate_path')}")
            
            # ‚≠ê NOUVEAU: R√©cup√©rer les param√®tres d'approbation depuis workflow_params
            workflow_params = context.get('workflow_params', {})
            apbookeeper_params = workflow_params.get('Apbookeeper_param', {})
            
            approval_required = apbookeeper_params.get('apbookeeper_approval_required', False)
            approval_contact_creation = apbookeeper_params.get('apbookeeper_approval_contact_creation', False)
            
            logger.info(
                f"[LPT_APBookkeeper] Param√®tres workflow: "
                f"approval_required={approval_required}, "
                f"approval_contact_creation={approval_contact_creation}"
            )
            
            # ‚≠ê R√âSOLUTION DES FILE_NAMES depuis le cache APBookkeeper
            # Chercher les job_ids dans brain.jobs_data['APBOOKEEPER']
            apbookeeper_jobs = (brain.jobs_data or {}).get("APBOOKEEPER", {}) if brain else {}
            
            # Agr√©ger toutes les listes possibles
            aggregated_ap_jobs = []
            for key in ["to_do", "in_process", "pending", "processed"]:
                jobs_list = apbookeeper_jobs.get(key)
                if isinstance(jobs_list, list):
                    aggregated_ap_jobs.extend(jobs_list)
            
            logger.info(f"[LPT_APBookkeeper] üîç Cache charg√©: {len(aggregated_ap_jobs)} factures disponibles")
            
            # Valider et r√©soudre chaque job_id
            jobs_data = []
            invalid_ids = []
            
            for job_id in job_ids:
                found = False
                resolved_file_name = None
                
                # Rechercher le job_id dans le cache
                for job in aggregated_ap_jobs:
                    if job.get("job_id") == job_id:
                        resolved_file_name = job.get("file_name", f"document_{job_id}")
                        found = True
                        logger.info(f"[LPT_APBookkeeper] ‚úÖ job_id={job_id} r√©solu ‚Üí file_name={resolved_file_name}")
                        break
                
                if not found:
                    invalid_ids.append(job_id)
                    logger.warning(f"[LPT_APBookkeeper] ‚ö†Ô∏è job_id={job_id} non trouv√© dans le cache")
                else:
                    # Construire job_data avec le vrai nom de fichier
                    job_data = {
                        "file_name": resolved_file_name,
                        "job_id": job_id,
                        "instructions": file_instructions.get(job_id, "") if file_instructions else "",
                        "status": "to_process",
                        "approval_required": approval_required,
                        "approval_contact_creation": approval_contact_creation
                    }
                    jobs_data.append(job_data)
            
            # ‚≠ê Si AUCUN job_id valide : retourner erreur avec liste des IDs disponibles
            if len(jobs_data) == 0:
                error_msg = (
                    f"‚ùå Aucune facture valide trouv√©e parmi les job_ids fournis.\n\n"
                    f"üìã job_ids invalides ({len(invalid_ids)}) : {invalid_ids}\n\n"
                    f"‚ö†Ô∏è Les valeurs fournies ne correspondent √† aucun job. Fournissez le `job_id` exact ; "
                    f"sinon la facture ne sera pas lanc√©e.\n\n"
                    f"üìÑ Factures disponibles ({len(aggregated_ap_jobs)} factures) :\n"
                )
                # Lister les 10 premi√®res factures disponibles
                for idx, job in enumerate(aggregated_ap_jobs[:10], 1):
                    doc_name = job.get('file_name', 'Sans nom')
                    doc_id = job.get('job_id', 'Sans ID')
                    error_msg += f"  {idx}. {doc_name} (job_id: {doc_id})\n"
                
                if len(aggregated_ap_jobs) > 10:
                    error_msg += f"  ... et {len(aggregated_ap_jobs) - 10} autres factures.\n"
                
                error_msg += (
                    f"\nüí° Pour traiter des factures, utilisez l'un des 'job_id' list√©s ci-dessus.\n"
                    f"Vous pouvez demander √† l'utilisateur de pr√©ciser quelles factures traiter si besoin."
                )
                
                logger.warning(f"[LPT_APBookkeeper] ‚ùå Aucun job_id valide - Retour erreur √† l'agent")
                
                return {
                    "status": "error",
                    "error": error_msg,
                    "invalid_ids": invalid_ids,
                    "available_invoices_count": len(aggregated_ap_jobs)
                }
            
            # ‚≠ê Si certains job_ids invalides : continuer avec warning
            if len(invalid_ids) > 0:
                logger.warning(
                    f"[LPT_APBookkeeper] ‚ö†Ô∏è {len(invalid_ids)} job_id(s) invalide(s) ignor√©(s): {invalid_ids}. "
                    f"Continuation avec {len(jobs_data)} facture(s) valide(s)."
                )
            
            # G√©n√©rer batch_id
            batch_id = f'batch_{uuid.uuid4().hex[:10]}'
            
            # ‚≠ê Construire settings (ALIGN√â avec Router)
            settings = [
                {'communication_mode': context['communication_mode']},
                {'log_communication_mode': context['log_communication_mode']},
                {'dms_system': context['dms_system']}
            ]
            
            # ‚≠ê NOUVEAU: R√©cup√©rer les informations de tra√ßabilit√© depuis le brain
            execution_id = execution_id or (brain.active_task_data.get("execution_id") if brain and brain.active_task_data else None)
            execution_plan = execution_plan or (brain.active_task_data.get("execution_plan") if brain and brain.active_task_data else None)

            # ‚≠ê R√©cup√©rer le vrai nom du thread depuis RTDB (ALIGN√â avec Router)
            from ...firebase_providers import FirebaseRealtimeChat
            rtdb = FirebaseRealtimeChat()
            thread_name = rtdb.get_thread_name(
                space_code=company_id,
                thread_key=thread_key,
                mode='chats'  # Mode par d√©faut pour les conversations agent
            ) or thread_key  # Fallback sur thread_key si non trouv√©
            
            # ‚≠ê Informations de tra√ßabilit√© pour le callback (ALIGN√â avec Router)
            traceability_info = {
                "thread_key": thread_key,
                "thread_name": thread_name,  # ‚úÖ Vrai nom du thread depuis RTDB
                "execution_id": execution_id,  # ‚≠ê ID d'ex√©cution pour tra√ßabilit√© compl√®te
                "execution_plan": execution_plan,  # ‚≠ê mode d'ex√©cution (NOW, ON_DEMAND, etc.)
                "initiated_at": datetime.now(timezone.utc).isoformat(),  # ‚≠ê timestamp d'initiation
                "source": "pinnokio_brain"  # ‚≠ê source de l'appel
            }

            payload = {
                # Informations de base
                "collection_name": company_id,
                "user_id": user_id,
                "client_uuid": context['client_uuid'],
                "mandates_path": context['mandate_path'],
                "batch_id": batch_id,

                # Donn√©es de la t√¢che
                "jobs_data": jobs_data,
                "start_instructions": general_instructions,

                # Configuration
                "settings": settings,

                # Informations de tra√ßabilit√© pour callback
                "traceability": traceability_info,  # ‚≠ê NOUVEAU: Section tra√ßabilit√© compl√®te
                "pub_sub_id": batch_id  # Utiliser batch_id comme pub_sub_id
            }
            
            logger.info(f"[LPT_APBookkeeper] Lancement - batch_id={batch_id}, nb_jobs={len(jobs_data)}, thread={thread_key}, execution_id={execution_id}, plan={execution_plan}")
            
            # Envoyer la requ√™te HTTP
            url = self.apbookeeper_url
            
            # ‚≠ê LOG: Afficher l'URL et le payload avant l'envoi
            logger.info(f"[LPT_APBookkeeper] üì§ Envoi HTTP POST vers: {url}")
            logger.info(f"[LPT_APBookkeeper] üì¶ Payload complet: {payload}")
            
            try:
                async with aiohttp.ClientSession() as session:
                    logger.info(f"[LPT_APBookkeeper] üîÑ Connexion en cours vers {url}...")
                    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        status = response.status
                        logger.info(f"[LPT_APBookkeeper] ‚úÖ R√©ponse HTTP re√ßue: status={status}")
                        
                        if status == 202:
                            logger.info(f"[LPT_APBookkeeper] ‚úì Lanc√© avec succ√®s - batch_id={batch_id}")
                            
                            # Sauvegarder la t√¢che dans Firebase
                            task_id = batch_id
                            await self._save_task_to_firebase(
                                user_id=user_id,
                                thread_key=thread_key,
                                task_id=task_id,
                                task_type="APBookkeeper",
                                payload=payload,
                                status="queued"
                            )
                            
                            # Cr√©er les notifications Firebase
                            await self._create_apbookeeper_notifications(
                                user_id=user_id,
                                company_id=company_id,
                                company_name=context['company_name'],
                                batch_id=batch_id,
                                jobs_data=jobs_data
                            )
                            
                            # Message de retour avec warning si certains job_ids invalides
                            success_message = f"‚úì APBookkeeper lanc√© : {len(jobs_data)} facture(s) en cours de traitement"
                            if len(invalid_ids) > 0:
                                success_message += f"\n‚ö†Ô∏è {len(invalid_ids)} job_id(s) invalide(s) ignor√©(s): {invalid_ids}"
                            
                            return {
                                "status": "queued",
                                "task_id": task_id,
                                "batch_id": batch_id,
                                "nb_jobs_valid": len(jobs_data),
                                "nb_jobs_invalid": len(invalid_ids),
                                "invalid_ids": invalid_ids if len(invalid_ids) > 0 else [],
                                "thread_key": thread_key,
                                "message": success_message
                            }
                        else:
                            error_text = await response.text()
                            logger.error(f"[LPT_APBookkeeper] ‚ùå Erreur HTTP {status}: {error_text}")
                            return {
                                "status": "error",
                                "error": f"HTTP {status}: {error_text}"
                            }
            
            except aiohttp.ClientError as ce:
                # Erreur de connexion ou r√©seau
                logger.error(f"[LPT_APBookkeeper] ‚ùå Erreur de connexion HTTP: {ce}", exc_info=True)
                return {
                    "status": "error",
                    "error": f"Erreur de connexion: {str(ce)}",
                    "error_type": "connection_error"
                }
            
            except asyncio.TimeoutError:
                # Timeout
                logger.error(f"[LPT_APBookkeeper] ‚è±Ô∏è Timeout apr√®s 30s vers {url}")
                return {
                    "status": "error",
                    "error": "Timeout de connexion (30s)",
                    "error_type": "timeout"
                }
        
        except ValueError as ve:
            # Erreur de configuration (donn√©es manquantes)
            logger.error(f"[LPT_APBookkeeper] Erreur de configuration: {ve}")
            return {
                "status": "configuration_error",
                "error": str(ve),
                "error_type": "missing_user_data",
                "message": "‚ö†Ô∏è Configuration utilisateur incompl√®te. V√©rifiez que l'utilisateur est correctement enregistr√©."
            }
        
        except Exception as e:
            # Erreur technique
            logger.error(f"[LPT_APBookkeeper] Erreur technique: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": "technical_error"
            }
    
    async def launch_router_all(
        self,
        user_id: str,
        company_id: str,
        thread_key: str,
        session=None,
        brain=None,
        execution_id: Optional[str] = None,
        execution_plan: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Lance automatiquement tous les documents Router disponibles (statut to_process).
        """
        try:
            if not brain:
                raise ValueError("Brain est requis pour lancer Router_ALL")
            
            context = brain.get_user_context()
            mandate_path = context.get('mandate_path')
            
            # üîç Compter le nombre de documents √† router
            router_jobs = ((brain.jobs_data or {}).get("ROUTER", {}) if brain else {}).get("to_process", []) or []
            nb_documents = len(router_jobs)
            
            # üõ°Ô∏è V√âRIFICATION DU SOLDE AVANT L'ENVOI
            # Co√ªt estim√© : 0.5$ par document
            estimated_cost = nb_documents * 0.5
            
            balance_check = self.check_balance_before_lpt(
                mandate_path=mandate_path,
                user_id=user_id,
                estimated_cost=estimated_cost,
                lpt_tool_name="Router_ALL"
            )
            
            if not balance_check.get("sufficient", False):
                # ‚ùå SOLDE INSUFFISANT - Retourner le message √† l'agent
                logger.warning(
                    f"[LPT_Router_ALL] ‚ùå BLOCAGE - Solde insuffisant "
                    f"({balance_check.get('current_balance', 0):.2f}$ < {balance_check.get('required_balance', 0):.2f}$)"
                )
                return {
                    "status": "insufficient_balance",
                    "error": "Solde insuffisant pour ex√©cuter cette op√©ration",
                    "balance_info": {
                        "current_balance": balance_check.get("current_balance", 0.0),
                        "required_balance": balance_check.get("required_balance", 0.0),
                        "missing_amount": balance_check.get("missing_amount", 0.0)
                    },
                    "nb_documents_to_route": nb_documents,
                    "message": balance_check.get("message", "Solde insuffisant")
                }
            
            # ‚úÖ SOLDE SUFFISANT - Continuer l'ex√©cution
            logger.info(
                f"[LPT_Router_ALL] ‚úÖ Solde v√©rifi√© et suffisant "
                f"({balance_check.get('current_balance', 0):.2f}$ >= {balance_check.get('required_balance', 0):.2f}$) "
                f"pour {nb_documents} documents"
            )
            logger.info(f"[LPT_Router_ALL] Contexte r√©cup√©r√©: mandate_path={mandate_path}")
            
            workflow_params = context.get('workflow_params', {})
            router_params = workflow_params.get('Router_param', {})
            
            approval_required = router_params.get('router_approval_required', False)
            automated_workflow = router_params.get('router_automated_workflow', True)
            
            router_jobs = ((brain.jobs_data or {}).get("ROUTER", {}) if brain else {}).get("to_process", []) or []
            
            documents_to_route: List[Dict[str, Any]] = []
            for job in router_jobs:
                drive_file_id = job.get("id") or job.get("drive_file_id")
                if not drive_file_id:
                    continue
                documents_to_route.append({
                    "drive_file_id": drive_file_id,
                    "file_name": job.get("name") or job.get("file_name") or drive_file_id,
                    "instructions": job.get("instructions") or ""
                })
            
            if not documents_to_route:
                logger.warning("[LPT_Router_ALL] Aucun document √† router disponible.")
                return {
                    "status": "no_jobs_available",
                    "message": "Aucun document au statut 'to_process' n'est disponible pour routage."
                }
            
            execution_id = execution_id or (brain.active_task_data.get("execution_id") if brain and brain.active_task_data else None)
            execution_plan = execution_plan or (brain.active_task_data.get("execution_plan") if brain and brain.active_task_data else None)
            
            from ...firebase_providers import FirebaseRealtimeChat
            rtdb = FirebaseRealtimeChat()
            thread_name = rtdb.get_thread_name(
                space_code=company_id,
                thread_key=thread_key,
                mode='chats'
            ) or thread_key
            
            pub_sub_id = f"router_batch_{uuid.uuid4().hex[:10]}"
            traceability_info = {
                "thread_key": thread_key,
                "thread_name": thread_name,
                "execution_id": execution_id,
                "execution_plan": execution_plan,
                "initiated_at": datetime.now(timezone.utc).isoformat(),
                "source": "pinnokio_brain"
            }
            
            jobs_data_payload = [
                {
                    "file_name": doc["file_name"],
                    "drive_file_id": doc["drive_file_id"],
                    "instructions": doc["instructions"],
                    "status": 'to_route',
                    "approval_required": approval_required,
                    "automated_workflow": automated_workflow
                }
                for doc in documents_to_route
            ]
            
            payload = {
                "collection_name": company_id,
                "user_id": user_id,
                "client_uuid": context['client_uuid'],
                "mandates_path": context['mandate_path'],
                "batch_id": pub_sub_id,
                "jobs_data": jobs_data_payload,
                "settings": [
                    {"communication_mode": context['communication_mode']},
                    {"log_communication_mode": context['log_communication_mode']},
                    {"dms_system": context['dms_system']}
                ],
                "traceability": traceability_info,
                "pub_sub_id": pub_sub_id,
                "start_instructions": None
            }
            
            logger.info(f"[LPT_Router_ALL] Lancement batch - documents={len(documents_to_route)}, pub_sub_id={pub_sub_id}")
            logger.info(f"[LPT_Router_ALL] Payload: {payload}")
            
            url = self.router_url
            try:
                async with aiohttp.ClientSession() as session_http:
                    async with session_http.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        status = response.status
                        logger.info(f"[LPT_Router_ALL] R√©ponse HTTP: {status}")
                        
                        if status == 202:
                            task_id = pub_sub_id
                            await self._save_task_to_firebase(
                                user_id=user_id,
                                thread_key=thread_key,
                                task_id=task_id,
                                task_type="Router",
                                payload=payload,
                                status="queued"
                            )
                            
                            for doc in documents_to_route:
                                await self._create_router_notification(
                                    user_id=user_id,
                                    company_id=company_id,
                                    company_name=context['company_name'],
                                    drive_file_id=doc["drive_file_id"],
                                    file_name=doc["file_name"],
                                    pub_sub_id=pub_sub_id,
                                    instructions=doc["instructions"]
                                )
                            
                            return {
                                "status": "queued",
                                "task_id": task_id,
                                "batch_id": pub_sub_id,
                                "nb_documents": len(documents_to_route),
                                "thread_key": thread_key,
                                "message": f"‚úì Router lanc√© pour {len(documents_to_route)} document(s)."
                            }
                        else:
                            error_text = await response.text()
                            logger.error(f"[LPT_Router_ALL] Erreur HTTP {status}: {error_text}")
                            return {
                                "status": "error",
                                "error": f"HTTP {status}: {error_text}"
                            }
            except aiohttp.ClientError as ce:
                logger.error(f"[LPT_Router_ALL] Erreur connexion HTTP: {ce}", exc_info=True)
                return {
                    "status": "error",
                    "error": f"Erreur de connexion: {str(ce)}",
                    "error_type": "connection_error"
                }
            except asyncio.TimeoutError:
                logger.error("[LPT_Router_ALL] Timeout HTTP (30s)")
                return {
                    "status": "error",
                    "error": "Timeout de connexion (30s)",
                    "error_type": "timeout"
                }
        
        except ValueError as ve:
            logger.error(f"[LPT_Router_ALL] Erreur de configuration: {ve}")
            return {
                "status": "configuration_error",
                "error": str(ve),
                "error_type": "missing_user_data"
            }
        except Exception as e:
            logger.error(f"[LPT_Router_ALL] Erreur technique: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": "technical_error"
            }
    
    async def launch_router(
        self,
        user_id: str,
        company_id: str,
        thread_key: str,
        drive_file_id: str,
        instructions: Optional[str] = None,
        session=None,  # ‚≠ê LLMSession pour cache (non utilis√©, contexte vient du brain)
        brain=None,    # ‚≠ê PinnokioBrain pour acc√®s au contexte utilisateur
        execution_id: Optional[str] = None,  # ‚≠ê ID d'ex√©cution pour tra√ßabilit√©
        execution_plan: Optional[str] = None  # ‚≠ê Mode d'ex√©cution (NOW, ON_DEMAND, etc.)
    ) -> Dict[str, Any]:
        """
        Lance une t√¢che Router pour le routage de documents.
        Les param√®tres (approval_required, automated_workflow) sont r√©cup√©r√©s automatiquement
        depuis workflow_params.
        """
        try:
            # ‚≠ê R√©cup√©rer le contexte depuis le brain (OBLIGATOIRE)
            if not brain:
                raise ValueError("Brain est requis pour lancer Router")
            
            context = brain.get_user_context()
            mandate_path = context.get('mandate_path')
            
            # üõ°Ô∏è V√âRIFICATION DU SOLDE AVANT L'ENVOI
            # Co√ªt estim√© : 0.5$ par document (ajustable selon vos tarifs)
            estimated_cost = 0.5
            
            balance_check = self.check_balance_before_lpt(
                mandate_path=mandate_path,
                user_id=user_id,
                estimated_cost=estimated_cost,
                lpt_tool_name="Router"
            )
            
            if not balance_check.get("sufficient", False):
                # ‚ùå SOLDE INSUFFISANT - Retourner le message √† l'agent
                logger.warning(
                    f"[LPT_Router] ‚ùå BLOCAGE - Solde insuffisant "
                    f"({balance_check.get('current_balance', 0):.2f}$ < {balance_check.get('required_balance', 0):.2f}$)"
                )
                return {
                    "status": "insufficient_balance",
                    "error": "Solde insuffisant pour ex√©cuter cette op√©ration",
                    "balance_info": {
                        "current_balance": balance_check.get("current_balance", 0.0),
                        "required_balance": balance_check.get("required_balance", 0.0),
                        "missing_amount": balance_check.get("missing_amount", 0.0)
                    },
                    "message": balance_check.get("message", "Solde insuffisant")
                }
            
            # ‚úÖ SOLDE SUFFISANT - Continuer l'ex√©cution
            logger.info(
                f"[LPT_Router] ‚úÖ Solde v√©rifi√© et suffisant "
                f"({balance_check.get('current_balance', 0):.2f}$ >= {balance_check.get('required_balance', 0):.2f}$)"
            )
            logger.info(f"[LPT_Router] Contexte r√©cup√©r√© depuis brain: mandate_path={mandate_path}")
            
            # ‚≠ê NOUVEAU: R√©cup√©rer les param√®tres depuis workflow_params
            workflow_params = context.get('workflow_params', {})
            router_params = workflow_params.get('Router_param', {})
            
            approval_required = router_params.get('router_approval_required', False)
            automated_workflow = router_params.get('router_automated_workflow', True)
            
            logger.info(
                f"[LPT_Router] Param√®tres workflow: "
                f"approval_required={approval_required}, "
                f"automated_workflow={automated_workflow}"
            )
            
            # Construire le payload
            job_id = str(uuid.uuid4())
            pub_sub_id = f"router_{drive_file_id}_{uuid.uuid4().hex[:6]}"

            # ‚≠ê NOUVEAU: R√©cup√©rer les informations de tra√ßabilit√© depuis le brain
            execution_id = execution_id or (brain.active_task_data.get("execution_id") if brain and brain.active_task_data else None)
            execution_plan = execution_plan or (brain.active_task_data.get("execution_plan") if brain and brain.active_task_data else None)

            # ‚≠ê R√âSOLUTION DU FILE_NAME depuis le cache jobs_data
            # Chercher le job correspondant au drive_file_id dans brain.jobs_data['ROUTER']
            resolved_file_name = drive_file_id  # Fallback par d√©faut
            
            # üîç DEBUG: Afficher la structure compl√®te de ROUTER
            if brain and brain.jobs_data:
                logger.info(f"[LPT_Router] üîç DEBUG brain.jobs_data keys: {list(brain.jobs_data.keys())}")
                if 'ROUTER' in brain.jobs_data:
                    logger.info(f"[LPT_Router] üîç DEBUG brain.jobs_data['ROUTER'] keys: {list(brain.jobs_data['ROUTER'].keys())}")
                else:
                    logger.warning(f"[LPT_Router] ‚ö†Ô∏è brain.jobs_data existe MAIS pas de cl√© 'ROUTER'")
            else:
                logger.warning(f"[LPT_Router] ‚ö†Ô∏è brain.jobs_data est None ou vide")
            
            if brain and brain.jobs_data and 'ROUTER' in brain.jobs_data:
                # ‚úÖ Correction: c'est 'to_process' et non 'unprocessed'
                router_jobs = brain.jobs_data['ROUTER'].get('to_process', [])
                
                # üîç DEBUG: Afficher la structure
                logger.info(f"[LPT_Router] üîç DEBUG - Nombre de jobs to_process: {len(router_jobs)}")
                logger.info(f"[LPT_Router] üîç DEBUG - Recherche drive_file_id: {drive_file_id}")
                
                if router_jobs and len(router_jobs) > 0:
                    # Afficher un exemple de structure
                    logger.info(f"[LPT_Router] üîç DEBUG - Exemple job[0] keys: {list(router_jobs[0].keys()) if router_jobs else 'vide'}")
                
                found = False
                for job in router_jobs:
                    job_id = job.get('id')
                    logger.info(f"[LPT_Router] üîç DEBUG - Comparaison job_id={job_id} vs drive_file_id={drive_file_id}")
                    if job_id == drive_file_id:
                        # Les jobs Router utilisent 'name' pour le nom du fichier
                        resolved_file_name = job.get('name', drive_file_id)
                        logger.info(f"[LPT_Router] ‚úÖ file_name r√©solu depuis cache: {resolved_file_name}")
                        found = True
                        break
                
                # ‚≠ê CORRECTION : Si fichier non trouv√©, retourner une ERREUR au lieu de continuer
                if not found:
                    error_msg = (
                        f"‚ùå Le document avec drive_file_id '{drive_file_id}' n'existe pas dans la liste des documents √† router.\n\n"
                        f"‚ö†Ô∏è La valeur fournie ne correspond √† aucun document connu. Fournissez le `drive_file_id` exact ; "
                        f"sinon le routage ne sera pas lanc√©.\n\n"
                        f"üìã Documents disponibles ({len(router_jobs)} documents) :\n"
                    )
                    # Lister les 10 premiers documents disponibles pour aider l'agent
                    for idx, job in enumerate(router_jobs[:10], 1):
                        doc_name = job.get('name', 'Sans nom')
                        doc_id = job.get('id', 'Sans ID')
                        error_msg += f"  {idx}. {doc_name} (ID: {doc_id})\n"
                    
                    if len(router_jobs) > 10:
                        error_msg += f"  ... et {len(router_jobs) - 10} autres documents.\n"
                    
                    error_msg += (
                        f"\nüí° Pour router un document, utilisez l'un des 'id' list√©s ci-dessus.\n"
                        f"Vous pouvez demander √† l'utilisateur de pr√©ciser quel document router si besoin."
                    )
                    
                    logger.warning(f"[LPT_Router] ‚ö†Ô∏è drive_file_id={drive_file_id} non trouv√© dans {len(router_jobs)} jobs to_process")
                    
                    # Retourner imm√©diatement l'erreur √† l'agent
                    return {
                        "status": "error",
                        "error": error_msg,
                        "available_documents_count": len(router_jobs)
                    }
            else:
                # Si jobs_data non disponible, retourner une erreur explicite
                error_msg = (
                    "‚ùå Impossible d'acc√©der √† la liste des documents √† router.\n"
                    "Les donn√©es des jobs Router ne sont pas disponibles dans le contexte actuel.\n"
                    "Veuillez rafra√Æchir les donn√©es ou contacter le support."
                )
                logger.warning(f"[LPT_Router] ‚ö†Ô∏è jobs_data['ROUTER'] non disponible")
                return {
                    "status": "error",
                    "error": error_msg
                }
            
            # R√©cup√©rer le vrai nom du thread depuis RTDB
            from ...firebase_providers import FirebaseRealtimeChat
            rtdb = FirebaseRealtimeChat()
            thread_name = rtdb.get_thread_name(
                space_code=company_id,
                thread_key=thread_key,
                mode='chats'  # Mode par d√©faut pour les conversations agent
            ) or thread_key  # Fallback sur thread_key si non trouv√©
            
            # Informations de tra√ßabilit√© pour le callback
            traceability_info = {
                "thread_key": thread_key,
                "thread_name": thread_name,  # ‚úÖ Vrai nom du thread depuis RTDB
                "execution_id": execution_id,  # ‚≠ê NOUVEAU: ID d'ex√©cution pour tra√ßabilit√© compl√®te
                "execution_plan": execution_plan,  # ‚≠ê NOUVEAU: mode d'ex√©cution (NOW, ON_DEMAND, etc.)
                "initiated_at": datetime.now(timezone.utc).isoformat(),  # ‚≠ê NOUVEAU: timestamp d'initiation
                "source": "pinnokio_brain"  # ‚≠ê NOUVEAU: source de l'appel
            }

            payload = {
                # Informations de base
                "collection_name": company_id,
                "user_id": user_id,
                "client_uuid": context['client_uuid'],
                "mandates_path": context['mandate_path'],

                # Donn√©es de la t√¢che
                "jobs_data": [{
                    "file_name": resolved_file_name,
                    "drive_file_id": drive_file_id,
                    "instructions": instructions or "",
                    "status": 'to_route',
                    "approval_required": approval_required,
                    "automated_workflow": automated_workflow
                }],

                # Configuration
                "settings": [
                    {"communication_mode": context['communication_mode']},
                    {"log_communication_mode": context['log_communication_mode']},
                    {"dms_system": context['dms_system']}
                ],

                # Informations de tra√ßabilit√© pour callback
                "traceability": traceability_info,  # ‚≠ê NOUVEAU: Section tra√ßabilit√© compl√®te
                "pub_sub_id": pub_sub_id,  # ID unique pour cette ex√©cution

                # Instructions
                "start_instructions": None
            }
            
            logger.info(f"[LPT_Router] Lancement - file_id={drive_file_id}, thread={thread_key}, execution_id={execution_id}, plan={execution_plan}")
            
            # Envoyer la requ√™te HTTP
            url = self.router_url
            
            # ‚≠ê LOG: Afficher l'URL et le payload avant l'envoi
            logger.info(f"[LPT_Router] üì§ Envoi HTTP POST vers: {url}")
            logger.info(f"[LPT_Router] üì¶ Payload complet: {payload}")
            
            try:
                async with aiohttp.ClientSession() as session:
                    logger.info(f"[LPT_Router] üîÑ Connexion en cours vers {url}...")
                    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        status = response.status
                        logger.info(f"[LPT_Router] ‚úÖ R√©ponse HTTP re√ßue: status={status}")
                        
                        if status == 202:
                            logger.info(f"[LPT_Router] ‚úì Lanc√© avec succ√®s - file_id={drive_file_id}")
                            
                            # Sauvegarder la t√¢che
                            task_id = pub_sub_id
                            await self._save_task_to_firebase(
                                user_id=user_id,
                                thread_key=thread_key,
                                task_id=task_id,
                                task_type="Router",
                                payload=payload,
                                status="queued"
                            )
                            
                            # Cr√©er la notification
                            await self._create_router_notification(
                                user_id=user_id,
                                company_id=company_id,
                                company_name=context['company_name'],
                                drive_file_id=drive_file_id,
                                file_name=resolved_file_name,  # ‚≠ê NOUVEAU: Passer le vrai nom du fichier
                                pub_sub_id=pub_sub_id,
                                instructions=instructions
                            )
                            
                            return {
                                "status": "queued",
                                "task_id": task_id,
                                "file_id": drive_file_id,
                                "thread_key": thread_key,
                                "message": f"‚úì Router lanc√© pour le document {drive_file_id}"
                            }
                        else:
                            error_text = await response.text()
                            logger.error(f"[LPT_Router] ‚ùå Erreur HTTP {status}: {error_text}")
                            return {
                                "status": "error",
                                "error": f"HTTP {status}: {error_text}"
                            }
            
            except aiohttp.ClientError as ce:
                # Erreur de connexion ou r√©seau
                logger.error(f"[LPT_Router] ‚ùå Erreur de connexion HTTP: {ce}", exc_info=True)
                return {
                    "status": "error",
                    "error": f"Erreur de connexion: {str(ce)}",
                    "error_type": "connection_error"
                }
            
            except asyncio.TimeoutError:
                # Timeout
                logger.error(f"[LPT_Router] ‚è±Ô∏è Timeout apr√®s 30s vers {url}")
                return {
                    "status": "error",
                    "error": "Timeout de connexion (30s)",
                    "error_type": "timeout"
                }
        
        except ValueError as ve:
            # Erreur de configuration (donn√©es manquantes)
            logger.error(f"[LPT_Router] Erreur de configuration: {ve}")
            return {
                "status": "configuration_error",
                "error": str(ve),
                "error_type": "missing_user_data",
                "message": "‚ö†Ô∏è Configuration utilisateur incompl√®te. V√©rifiez que l'utilisateur est correctement enregistr√©."
            }
        
        except Exception as e:
            # Erreur technique
            logger.error(f"[LPT_Router] Erreur technique: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": "technical_error"
            }
    
    async def launch_onboarding_job(
        self,
        user_id: str,
        company_id: str,
        thread_key: str,
        session=None,
        brain=None,
        job_id: Optional[str] = None,
        execution_id: Optional[str] = None,
        execution_plan: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Lance une t√¢che Onboarding pour le processus d'onboarding client.
        Format standard LPT (identique √† Router/APBookkeeper/Banker).
        """
        try:
            # ‚≠ê R√©cup√©rer le contexte depuis le brain (OBLIGATOIRE)
            if not brain:
                raise ValueError("Brain est requis pour lancer le job d'onboarding")
            
            context = brain.get_user_context()
            logger.info(f"[LPT_Onboarding] Contexte r√©cup√©r√© depuis brain: mandate_path={context.get('mandate_path')}")
            
            # Charger les donn√©es onboarding
            onboarding_data = brain.onboarding_data or await brain.load_onboarding_data()
            onboarding_data = onboarding_data or {}

            if not onboarding_data:
                raise ValueError("Donn√©es d'onboarding introuvables pour ce client")

            # Construire les identifiants (format standard LPT)
            job_id = job_id or onboarding_data.get("job_id") or f"onboarding_{uuid.uuid4().hex[:8]}"
            onboarding_data.setdefault("job_id", job_id)
            
            # ‚≠ê Utiliser batch_id au niveau racine (format standard LPT)
            batch_id = job_id  # Pour onboarding, batch_id = job_id (un seul job par batch)
            pub_sub_id = f"onboarding_{job_id}_{uuid.uuid4().hex[:6]}"

            # ‚≠ê R√©cup√©rer les informations de tra√ßabilit√© depuis le brain
            execution_id = execution_id or (brain.active_task_data.get("execution_id") if brain and brain.active_task_data else None) or f"exec_{uuid.uuid4().hex}"
            execution_plan = execution_plan or (brain.active_task_data.get("execution_plan") if brain and brain.active_task_data else None) or "NOW"

            # R√©cup√©rer le vrai nom du thread depuis RTDB
            from ...firebase_providers import FirebaseRealtimeChat
            rtdb = FirebaseRealtimeChat()
            thread_name = rtdb.get_thread_name(
                space_code=company_id,
                thread_key=thread_key,
                mode='chats'
            ) or thread_key

            # Informations de tra√ßabilit√© pour le callback (format standard LPT)
            traceability_info = {
                "thread_key": thread_key,
                "thread_name": thread_name,
                "execution_id": execution_id,
                "execution_plan": execution_plan,
                "initiated_at": datetime.now(timezone.utc).isoformat(),
                "source": "pinnokio_onboarding"  # ‚≠ê Source de l'appel
            }

            # ‚≠ê R√©cup√©rer les donn√©es n√©cessaires pour le payload
            context_data = onboarding_data.get("initial_context_data", "")
            setup_coa_type = onboarding_data.get("analysis_method")
            accounting_systems = onboarding_data.get("accounting_systems", {})
            erp_system = accounting_systems.get("accounting_system") if isinstance(accounting_systems, dict) else None
            mandate_path = context.get("mandate_path")

            # ‚≠ê Payload au format attendu par l'endpoint onboarding_manager_agent
            payload = {
                "firebase_user_id": user_id,
                "job_id": job_id,
                "mandate_path": mandate_path,
                "mode": "onboarding",
                "setup_coa_type": setup_coa_type,
                "erp_system": erp_system,
                "context": context_data
            }
            
            logger.info(f"[LPT_Onboarding] Lancement - job_id={job_id}, thread={thread_key}, execution_id={execution_id}, plan={execution_plan}")
            
            # ‚≠ê Placer le verrou persistant avant d'appeler le service (best-effort)
            try:
                from ...firebase_providers import FirebaseManagement
                fbm = FirebaseManagement()
                onboarding_path = f"clients/{user_id}/temp_data/onboarding"
                # Utiliser asyncio.to_thread car set_document est synchrone
                await asyncio.to_thread(fbm.set_document, onboarding_path, {
                    "job_active": True,
                    "job_id": job_id,
                    "lock_timestamp": datetime.now(timezone.utc).isoformat()
                }, True)  # merge=True
                logger.info(f"[LPT_Onboarding] üîí Verrou d'onboarding plac√© pour job_id={job_id}")
            except Exception as e:
                logger.warning(f"[LPT_Onboarding] ‚ö†Ô∏è Impossible d'√©crire le verrou d'onboarding: {e}")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚ö° OPTIMISATION : MODE FIRE-AND-FORGET (comme general_chat)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # Au lieu d'attendre la r√©ponse HTTP (bloquant 5-20s), on :
            # 1. Lance le POST HTTP en arri√®re-plan (non-bloquant)
            # 2. Retourne imm√©diatement "queued"
            # 3. L'agent notifie via /lpt/callback quand termin√©
            # ‚úÖ Identique au flux de send_message (homog√©n√©it√© totale)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
            url = self.onboarding_url
            
            # ‚≠ê LOG: Afficher l'URL et le payload avant l'envoi
            logger.info(f"[LPT_Onboarding] üì§ Envoi HTTP POST vers: {url}")
            logger.info(f"[LPT_Onboarding] üì¶ Payload complet: {payload}")
            
            # Sauvegarder la t√¢che AVANT d'envoyer (pour avoir le contexte si callback arrive vite)
            task_id = pub_sub_id
            await self._save_task_to_firebase(
                user_id=user_id,
                thread_key=thread_key,
                task_id=task_id,
                task_type="Onboarding",
                payload=payload,
                status="queued"
            )
            
            # ‚ö° Lancer le POST HTTP en arri√®re-plan (fire-and-forget)
            async def _fire_and_forget_post():
                """Envoie le POST HTTP sans bloquer, notifie via logs."""
                try:
                    # ‚ö° OPTIMISATION: Cr√©er la notification en arri√®re-plan (ne bloque pas le retour)
                    await self._create_onboarding_notification(
                        user_id=user_id,
                        company_id=company_id,
                        company_name=context.get("company_name"),
                        thread_key=thread_key,
                        job_id=job_id,
                        execution_id=execution_id
                    )
                    
                    async with aiohttp.ClientSession() as session_http:
                        logger.info(f"[LPT_Onboarding] üîÑ [BG] Connexion en cours vers {url}...")
                        async with session_http.post(
                            url, 
                            json=payload, 
                            timeout=aiohttp.ClientTimeout(total=60)  # Timeout g√©n√©reux pour agent externe
                        ) as response:
                            status = response.status
                            logger.info(f"[LPT_Onboarding] ‚úÖ [BG] R√©ponse HTTP re√ßue: status={status}")
                            
                            if status in (200, 202):
                                logger.info(f"[LPT_Onboarding] ‚úì [BG] Job envoy√© avec succ√®s - job_id={job_id}")
                            else:
                                error_text = await response.text()
                                logger.error(f"[LPT_Onboarding] ‚ùå [BG] Erreur HTTP {status}: {error_text}")
                                
                                # Mettre √† jour le statut de la t√¢che en erreur
                                await self._update_task_status(
                                    user_id=user_id,
                                    task_id=task_id,
                                    status="error",
                                    error=f"HTTP {status}: {error_text}"
                                )
                
                except aiohttp.ClientError as ce:
                    logger.error(f"[LPT_Onboarding] ‚ùå [BG] Erreur de connexion HTTP: {ce}", exc_info=True)
                    await self._update_task_status(
                        user_id=user_id,
                        task_id=task_id,
                        status="error",
                        error=f"Erreur de connexion: {str(ce)}"
                    )
                
                except asyncio.TimeoutError:
                    logger.error(f"[LPT_Onboarding] ‚è±Ô∏è [BG] Timeout apr√®s 60s vers {url}")
                    await self._update_task_status(
                        user_id=user_id,
                        task_id=task_id,
                        status="error",
                        error="Timeout de connexion (60s)"
                    )
                
                except Exception as e:
                    logger.error(f"[LPT_Onboarding] ‚ùå [BG] Erreur inattendue: {e}", exc_info=True)
                    await self._update_task_status(
                        user_id=user_id,
                        task_id=task_id,
                        status="error",
                        error=str(e)
                    )
            
            # Lancer en arri√®re-plan (non-bloquant)
            asyncio.create_task(_fire_and_forget_post())
            
            logger.info(
                f"[LPT_Onboarding] ‚ö° Retour imm√©diat (fire-and-forget) - "
                f"job_id={job_id}, task_id={task_id}"
            )
            
            # ‚úÖ RETOUR IMM√âDIAT (comme send_message)
            return {
                "status": "queued",
                "task_id": task_id,
                "job_id": job_id,
                "batch_id": batch_id,
                "execution_id": execution_id,
                "thread_key": thread_key,
                "message": "‚úì Onboarding lanc√© (mode asynchrone)"
            }

        except Exception as e:
            logger.error(f"[LPT_Onboarding] ‚ùå Erreur lancement onboarding: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }

    async def launch_banker_all(
        self,
        user_id: str,
        company_id: str,
        thread_key: str,
        bank_account: Optional[str] = None,
        start_instructions: Optional[str] = None,
        session=None,
        brain=None,
        execution_id: Optional[str] = None,
        execution_plan: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Lance automatiquement la r√©conciliation de toutes les transactions bancaires disponibles.
        Optionnellement filtr√©e par compte bancaire.
        """
        try:
            if not brain:
                raise ValueError("Brain est requis pour lancer Banker_ALL")
            
            context = brain.get_user_context()
            mandate_path = context.get('mandate_path')
            
            # üîç Compter le nombre de transactions √† traiter
            bank_data = (brain.jobs_data or {}).get("BANK", {}) if brain else {}
            unprocessed_transactions = bank_data.get("unprocessed", []) or []
            nb_transactions = len(unprocessed_transactions)
            
            # üõ°Ô∏è V√âRIFICATION DU SOLDE AVANT L'ENVOI
            # Co√ªt estim√© : 0.3$ par transaction
            estimated_cost = nb_transactions * 0.3
            
            balance_check = self.check_balance_before_lpt(
                mandate_path=mandate_path,
                user_id=user_id,
                estimated_cost=estimated_cost,
                lpt_tool_name="Banker_ALL"
            )
            
            if not balance_check.get("sufficient", False):
                # ‚ùå SOLDE INSUFFISANT - Retourner le message √† l'agent
                logger.warning(
                    f"[LPT_Banker_ALL] ‚ùå BLOCAGE - Solde insuffisant "
                    f"({balance_check.get('current_balance', 0):.2f}$ < {balance_check.get('required_balance', 0):.2f}$)"
                )
                return {
                    "status": "insufficient_balance",
                    "error": "Solde insuffisant pour ex√©cuter cette op√©ration",
                    "balance_info": {
                        "current_balance": balance_check.get("current_balance", 0.0),
                        "required_balance": balance_check.get("required_balance", 0.0),
                        "missing_amount": balance_check.get("missing_amount", 0.0)
                    },
                    "nb_transactions_to_process": nb_transactions,
                    "message": balance_check.get("message", "Solde insuffisant")
                }
            
            # ‚úÖ SOLDE SUFFISANT - Continuer l'ex√©cution
            logger.info(
                f"[LPT_Banker_ALL] ‚úÖ Solde v√©rifi√© et suffisant "
                f"({balance_check.get('current_balance', 0):.2f}$ >= {balance_check.get('required_balance', 0):.2f}$) "
                f"pour {nb_transactions} transactions"
            )
            
            workflow_params = context.get('workflow_params', {})
            banker_params = workflow_params.get('Banker_param', {})
            approval_required = banker_params.get('banker_approval_required', False)
            approval_threshold = banker_params.get('banker_approval_thresholdworkflow', '95')
            
            bank_jobs = (brain.jobs_data or {}).get("BANK", {}) if brain else {}
            aggregated_transactions: List[Dict[str, Any]] = []
            for key in ["to_reconcile", "pending", "in_process"]:
                tx_list = bank_jobs.get(key)
                if isinstance(tx_list, list):
                    aggregated_transactions.extend(tx_list)
            
            if not aggregated_transactions:
                logger.warning("[LPT_Banker_ALL] Aucune transaction disponible pour lancement.")
                return {
                    "status": "no_transactions_available",
                    "message": "Aucune transaction bancaire disponible (to_reconcile/pending/in_process)."
                }
            
            if bank_account:
                target = bank_account.lower()
                filtered_transactions = [
                    tx for tx in aggregated_transactions
                    if str(tx.get('journal_id', '')).lower() == target
                    or str(tx.get('journal_name', '')).lower() == target
                ]
                if not filtered_transactions:
                    logger.warning(f"[LPT_Banker_ALL] Aucun mouvement pour le compte {bank_account}.")
                    return {
                        "status": "no_transactions_for_account",
                        "message": f"Aucune transaction trouv√©e pour le compte bancaire '{bank_account}'."
                    }
            else:
                filtered_transactions = aggregated_transactions
            
            grouped_transactions: Dict[str, Dict[str, Any]] = {}
            for tx in filtered_transactions:
                account_id = str(tx.get('journal_id', '') or '')
                account_name = str(tx.get('journal_name') or account_id or 'Compte bancaire')
                group_key = account_id or account_name
                if group_key not in grouped_transactions:
                    grouped_transactions[group_key] = {
                        "account_id": account_id,
                        "account_name": account_name,
                        "transactions": []
                    }
                grouped_transactions[group_key]["transactions"].append(tx)
            
            if not grouped_transactions:
                logger.warning("[LPT_Banker_ALL] Aucune transaction regroup√©e apr√®s filtrage.")
                return {
                    "status": "no_transactions_available",
                    "message": "Aucune transaction √† r√©concilier apr√®s filtrage."
                }
            
            execution_id = execution_id or (brain.active_task_data.get("execution_id") if brain and brain.active_task_data else None)
            execution_plan = execution_plan or (brain.active_task_data.get("execution_plan") if brain and brain.active_task_data else None)
            
            from ...firebase_providers import FirebaseRealtimeChat
            rtdb = FirebaseRealtimeChat()
            thread_name = rtdb.get_thread_name(
                space_code=company_id,
                thread_key=thread_key,
                mode='chats'
            ) or thread_key
            
            batch_id = f'bank_batch_{uuid.uuid4().hex[:10]}'
            traceability_info = {
                "thread_key": thread_key,
                "thread_name": thread_name,
                "execution_id": execution_id,
                "execution_plan": execution_plan,
                "initiated_at": datetime.now(timezone.utc).isoformat(),
                "source": "pinnokio_brain"
            }
            
            jobs_data_payload: List[Dict[str, Any]] = []
            notifications_payload: List[Dict[str, Any]] = []
            
            for idx, (_group_key, group_data) in enumerate(grouped_transactions.items(), start=1):
                account_name = group_data["account_name"]
                account_id = group_data["account_id"]
                tx_payload = []
                
                for tx in group_data["transactions"]:
                    transaction_id = tx.get('transaction_id')
                    if transaction_id is None:
                        continue

                    # Formater transaction_name avec (#ref) si ref existe
                    display_name = tx.get('display_name', f"Transaction {transaction_id}")
                    ref = str(tx.get('ref', '')).strip()
                    if ref and not ref.startswith('#'):
                        ref = f"#{ref}"
                    transaction_name = f"{display_name} ({ref})" if ref else display_name
                    
                    # Normaliser partner_name et transaction_type ('' au lieu de 'None'/'False')
                    partner_name = str(tx.get('partner_name', '') or '').strip()
                    if partner_name.lower() in ('none', 'null', 'false'):
                        partner_name = ''
                    
                    transaction_type = str(tx.get('transaction_type', '') or '').strip()
                    if transaction_type.lower() in ('none', 'null', 'false'):
                        transaction_type = ''
                    
                    tx_payload.append({
                        "transaction_id": str(transaction_id),
                        "transaction_name": transaction_name,
                        "date": str(tx.get('date', '')),
                        "ref": ref,
                        "amount": float(tx.get('amount', 0)),
                        "currency_name": str(tx.get('currency_id', 'EUR')),
                        "partner_name": partner_name,
                        "transaction_type": transaction_type,
                        "payment_ref": str(tx.get('payment_ref', '')),
                        "journal_name": str(tx.get('journal_name') or account_name),
                        "journal_id": str(tx.get('journal_id') or account_id),
                        "instructions": tx.get('instructions', ''),
                        "pending": str(tx.get('status', '')).lower() == 'pending',
                        "status": 'in_queue'  # Toujours 'in_queue' pour les nouvelles transactions
                    })
                
                if not tx_payload:
                    continue
                
                # Note: job_id utilis√© uniquement pour les notifications internes
                job_id = f"{batch_id}_{idx}"
                
                jobs_data_payload.append({
                    "bank_account": account_name,
                    "bank_account_id": account_id,
                    "transactions": tx_payload,
                    "instructions": "",  # Instructions sp√©cifiques au job (vide par d√©faut)
                    "banker_approval_required": approval_required,
                    "banker_approval_thresholdworkflow": str(approval_threshold)
                })
                notifications_payload.append({
                    "job_id": job_id,
                    "bank_account": account_name,
                    "transactions": tx_payload
                })
            
            if not jobs_data_payload:
                logger.warning("[LPT_Banker_ALL] Aucun payload valide apr√®s transformation.")
                return {
                    "status": "no_transactions_available",
                    "message": "Les transactions disponibles ne contiennent pas d'identifiants exploitables."
                }
            
            # R√©cup√©rer le journal_name du premier job pour le niveau racine
            first_journal_name = jobs_data_payload[0].get("transactions", [{}])[0].get("journal_name", "") if jobs_data_payload and jobs_data_payload[0].get("transactions") else ""
            
            payload = {
                "collection_name": company_id,
                "batch_id": batch_id,
                "journal_name": first_journal_name,
                "jobs_data": jobs_data_payload,
                "start_instructions": start_instructions or "",  # Instructions g√©n√©rales pour tout le batch
                "settings": [
                    {'communication_mode': context['communication_mode']},
                    {'log_communication_mode': context['log_communication_mode']},
                    {'dms_system': context['dms_system']}
                ],
                "client_uuid": context['client_uuid'],
                "user_id": user_id,
                "mandates_path": context['mandate_path'],
                "proxy": False,
                "traceability": traceability_info
            }
            
            logger.info(f"[LPT_Banker_ALL] Lancement batch - comptes={len(jobs_data_payload)}, batch_id={batch_id}")
            logger.info(f"[LPT_Banker_ALL] Payload: {payload}")
            
            url = self.banker_url
            try:
                async with aiohttp.ClientSession() as session_http:
                    async with session_http.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        status = response.status
                        logger.info(f"[LPT_Banker_ALL] R√©ponse HTTP: {status}")
                        
                        if status == 202:
                            task_id = batch_id
                            await self._save_task_to_firebase(
                                user_id=user_id,
                                thread_key=thread_key,
                                task_id=task_id,
                                task_type="Banker",
                                payload=payload,
                                status="queued"
                            )
                            
                            for notif in notifications_payload:
                                await self._create_banker_notification(
                                    user_id=user_id,
                                    company_id=company_id,
                                    company_name=context['company_name'],
                                    batch_id=batch_id,
                                    job_id=notif["job_id"],
                                    bank_account=notif["bank_account"],
                                    transactions=notif["transactions"]
                                )
                            
                            return {
                                "status": "queued",
                                "task_id": task_id,
                                "batch_id": batch_id,
                                "nb_accounts": len(jobs_data_payload),
                                "nb_transactions": sum(len(n["transactions"]) for n in notifications_payload),
                                "thread_key": thread_key,
                                "message": f"‚úì Banker lanc√© pour {len(jobs_data_payload)} compte(s) bancaire(s)."
                            }
                        else:
                            error_text = await response.text()
                            logger.error(f"[LPT_Banker_ALL] Erreur HTTP {status}: {error_text}")
                            return {
                                "status": "error",
                                "error": f"HTTP {status}: {error_text}"
                            }
            except aiohttp.ClientError as ce:
                logger.error(f"[LPT_Banker_ALL] Erreur connexion HTTP: {ce}", exc_info=True)
                return {
                    "status": "error",
                    "error": f"Erreur de connexion: {str(ce)}",
                    "error_type": "connection_error"
                }
            except asyncio.TimeoutError:
                logger.error("[LPT_Banker_ALL] Timeout HTTP (30s)")
                return {
                    "status": "error",
                    "error": "Timeout de connexion (30s)",
                    "error_type": "timeout"
                }
        
        except ValueError as ve:
            logger.error(f"[LPT_Banker_ALL] Erreur de configuration: {ve}")
            return {
                "status": "configuration_error",
                "error": str(ve),
                "error_type": "missing_user_data"
            }
        except Exception as e:
            logger.error(f"[LPT_Banker_ALL] Erreur technique: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": "technical_error"
            }
    
    async def launch_banker(
        self,
        user_id: str,
        company_id: str,
        thread_key: str,
        bank_account: str,
        transaction_ids: List[str],
        instructions: Optional[str] = None,
        start_instructions: Optional[str] = None,
        transaction_instructions: Optional[Dict[str, str]] = None,
        session=None,  # ‚≠ê LLMSession pour cache (non utilis√©, contexte vient du brain)
        brain=None,    # ‚≠ê PinnokioBrain pour acc√®s au contexte utilisateur
        execution_id: Optional[str] = None,  # ‚≠ê ID d'ex√©cution pour tra√ßabilit√©
        execution_plan: Optional[str] = None  # ‚≠ê Mode d'ex√©cution (NOW, ON_DEMAND, etc.)
    ) -> Dict[str, Any]:
        """
        Lance une t√¢che Banker pour la r√©conciliation bancaire.
        Le param√®tre approval_required est r√©cup√©r√© automatiquement depuis workflow_params.
        """
        try:
            # ‚≠ê R√©cup√©rer le contexte depuis le brain (OBLIGATOIRE)
            if not brain:
                raise ValueError("Brain est requis pour lancer Banker")
            
            context = brain.get_user_context()
            mandate_path = context.get('mandate_path')
            
            # üõ°Ô∏è V√âRIFICATION DU SOLDE AVANT L'ENVOI
            # Co√ªt estim√© : 0.3$ par transaction (ajustable selon vos tarifs)
            estimated_cost = len(transaction_ids) * 0.3
            
            balance_check = self.check_balance_before_lpt(
                mandate_path=mandate_path,
                user_id=user_id,
                estimated_cost=estimated_cost,
                lpt_tool_name="Banker"
            )
            
            if not balance_check.get("sufficient", False):
                # ‚ùå SOLDE INSUFFISANT - Retourner le message √† l'agent
                logger.warning(
                    f"[LPT_Banker] ‚ùå BLOCAGE - Solde insuffisant "
                    f"({balance_check.get('current_balance', 0):.2f}$ < {balance_check.get('required_balance', 0):.2f}$)"
                )
                return {
                    "status": "insufficient_balance",
                    "error": "Solde insuffisant pour ex√©cuter cette op√©ration",
                    "balance_info": {
                        "current_balance": balance_check.get("current_balance", 0.0),
                        "required_balance": balance_check.get("required_balance", 0.0),
                        "missing_amount": balance_check.get("missing_amount", 0.0)
                    },
                    "message": balance_check.get("message", "Solde insuffisant")
                }
            
            # ‚úÖ SOLDE SUFFISANT - Continuer l'ex√©cution
            logger.info(
                f"[LPT_Banker] ‚úÖ Solde v√©rifi√© et suffisant "
                f"({balance_check.get('current_balance', 0):.2f}$ >= {balance_check.get('required_balance', 0):.2f}$)"
            )
            logger.info(f"[LPT_Banker] Contexte r√©cup√©r√© depuis brain: mandate_path={mandate_path}")
            
            # ‚≠ê NOUVEAU: R√©cup√©rer les param√®tres depuis workflow_params
            workflow_params = context.get('workflow_params', {})
            banker_params = workflow_params.get('Banker_param', {})
            
            approval_required = banker_params.get('banker_approval_required', False)
            approval_threshold = banker_params.get('banker_approval_thresholdworkflow', '95')
            
            logger.info(
                f"[LPT_Banker] Param√®tres workflow: "
                f"approval_required={approval_required}"
            )
            
            # ‚≠ê R√âSOLUTION DES TRANSACTIONS depuis le cache BANK
            # Chercher les transaction_ids dans brain.jobs_data['BANK']
            bank_jobs = (brain.jobs_data or {}).get("BANK", {}) if brain else {}
            
            # Agr√©ger toutes les listes possibles
            all_transactions = []
            for key in ["to_reconcile", "pending", "in_process"]:
                txs_list = bank_jobs.get(key)
                if isinstance(txs_list, list):
                    all_transactions.extend(txs_list)
            
            logger.info(f"[LPT_Banker] üîç Cache charg√©: {len(all_transactions)} transactions disponibles")
            
            # Valider et r√©soudre chaque transaction_id
            valid_transactions = []
            invalid_ids = []
            
            for tx_id in transaction_ids:
                found = False

                # Rechercher le transaction_id dans le cache
                for tx in all_transactions:
                    if str(tx.get('transaction_id')) == str(tx_id):
                        # Formater transaction_name avec (#ref) si ref existe
                        display_name = tx.get('display_name', f"Transaction {tx_id}")
                        ref = str(tx.get('ref', '')).strip()
                        if ref and not ref.startswith('#'):
                            ref = f"#{ref}"
                        transaction_name = f"{display_name} ({ref})" if ref else display_name

                        # Normaliser partner_name et transaction_type ('' au lieu de 'None'/'False')
                        partner_name = str(tx.get('partner_name', '') or '').strip()
                        if partner_name.lower() in ('none', 'null', 'false'):
                            partner_name = ''

                        transaction_type = str(tx.get('transaction_type', '') or '').strip()
                        if transaction_type.lower() in ('none', 'null', 'false'):
                            transaction_type = ''

                        # R√©cup√©rer journal_name et journal_id correctement
                        journal_name = str(tx.get('journal_name') or tx.get('journal_id') or bank_account or 'Bank')
                        journal_id = str(tx.get('journal_id') or '')

                        # R√©cup√©rer les instructions sp√©cifiques pour cette transaction si fournies
                        tx_instructions = ""
                        if transaction_instructions and str(tx_id) in transaction_instructions:
                            tx_instructions = transaction_instructions[str(tx_id)]

                        # Construire transaction compl√®te selon le format attendu
                        transaction_data = {
                            "transaction_id": str(tx.get('transaction_id')),
                            "transaction_name": transaction_name,
                            "date": str(tx.get('date', '')),
                            "ref": ref,
                            "amount": float(tx.get('amount', 0)),
                            "currency_name": str(tx.get('currency_id', 'EUR')),
                            "partner_name": partner_name,
                            "transaction_type": transaction_type,
                            "payment_ref": str(tx.get('payment_ref', '')),
                            "journal_name": journal_name,
                            "journal_id": journal_id,
                            "instructions": tx_instructions,
                            "pending": str(tx.get('status', '')).lower() == 'pending',
                            "status": 'in_queue'  # Toujours 'in_queue' pour les nouvelles transactions
                        }
                        valid_transactions.append(transaction_data)
                        found = True
                        logger.info(
                            f"[LPT_Banker] ‚úÖ transaction_id={tx_id} r√©solu ‚Üí "
                            f"{tx.get('partner_name')} - {tx.get('amount')}‚Ç¨"
                        )
                        break
                
                if not found:
                    invalid_ids.append(tx_id)
                    logger.warning(f"[LPT_Banker] ‚ö†Ô∏è transaction_id={tx_id} non trouv√© dans le cache")
            
            # ‚≠ê Si AUCUNE transaction valide : retourner erreur avec liste des IDs disponibles
            if len(valid_transactions) == 0:
                error_msg = (
                    f"‚ùå Aucune transaction valide trouv√©e parmi les transaction_ids fournis.\n\n"
                    f"üìã transaction_ids invalides ({len(invalid_ids)}) : {invalid_ids}\n\n"
                    f"‚ö†Ô∏è Chaque valeur doit correspondre exactement √† un `transaction_id`. "
                    f"Si le `transaction_id` pr√©cis n'est pas fourni, la transaction ne sera pas ex√©cut√©e.\n\n"
                    f"üí≥ Transactions disponibles ({len(all_transactions)} transactions) :\n"
                )
                # Lister les 10 premi√®res transactions disponibles
                for idx, tx in enumerate(all_transactions[:10], 1):
                    tx_name = tx.get('display_name', 'Sans nom')
                    tx_id = tx.get('transaction_id', 'Sans ID')
                    tx_amount = tx.get('amount', 0)
                    tx_partner = tx.get('partner_name', 'N/A')
                    error_msg += f"  {idx}. {tx_name} - {tx_partner} - {tx_amount}‚Ç¨ (transaction_id: {tx_id})\n"

                if len(all_transactions) > 10:
                    error_msg += f"  ... et {len(all_transactions) - 10} autres transactions.\n"

                error_msg += (
                    f"\nüí° Pour r√©concilier des transactions, utilisez l'un des 'transaction_id' list√©s ci-dessus.\n"
                    f"Vous pouvez demander √† l'utilisateur de pr√©ciser quelles transactions traiter si besoin."
                )
                
                logger.warning(f"[LPT_Banker] ‚ùå Aucune transaction valide - Retour erreur √† l'agent")
                
                return {
                    "status": "error",
                    "error": error_msg,
                    "invalid_ids": invalid_ids,
                    "available_transactions_count": len(all_transactions)
                }
            
            # ‚≠ê Si certaines transactions invalides : continuer avec warning
            if len(invalid_ids) > 0:
                logger.warning(
                    f"[LPT_Banker] ‚ö†Ô∏è {len(invalid_ids)} transaction_id(s) invalide(s) ignor√©(s): {invalid_ids}. "
                    f"Continuation avec {len(valid_transactions)} transaction(s) valide(s)."
                )
            
            # Construire le payload
            batch_id = f'bank_batch_{uuid.uuid4().hex[:10]}'
            
            # ‚≠ê NOUVEAU: R√©cup√©rer les informations de tra√ßabilit√© depuis le brain
            execution_id = execution_id or (brain.active_task_data.get("execution_id") if brain and brain.active_task_data else None)
            execution_plan = execution_plan or (brain.active_task_data.get("execution_plan") if brain and brain.active_task_data else None)

            # ‚≠ê R√©cup√©rer le vrai nom du thread depuis RTDB (ALIGN√â avec Router)
            from ...firebase_providers import FirebaseRealtimeChat
            rtdb = FirebaseRealtimeChat()
            thread_name = rtdb.get_thread_name(
                space_code=company_id,
                thread_key=thread_key,
                mode='chats'  # Mode par d√©faut pour les conversations agent
            ) or thread_key  # Fallback sur thread_key si non trouv√©

            # ‚≠ê Informations de tra√ßabilit√© pour le callback (ALIGN√â avec Router)
            traceability_info = {
                "thread_key": thread_key,
                "thread_name": thread_name,  # ‚úÖ Vrai nom du thread depuis RTDB
                "execution_id": execution_id,  # ‚≠ê ID d'ex√©cution pour tra√ßabilit√© compl√®te
                "execution_plan": execution_plan,  # ‚≠ê mode d'ex√©cution (NOW, ON_DEMAND, etc.)
                "initiated_at": datetime.now(timezone.utc).isoformat(),  # ‚≠ê timestamp d'initiation
                "source": "pinnokio_brain"  # ‚≠ê source de l'appel
            }

            # R√©cup√©rer le journal_name et bank_account_id depuis la premi√®re transaction
            first_journal_name = valid_transactions[0].get("journal_name", bank_account) if valid_transactions else bank_account
            first_bank_account_id = valid_transactions[0].get("journal_id", "") if valid_transactions else ""

            payload = {
                "collection_name": company_id,
                "batch_id": batch_id,
                "journal_name": first_journal_name,
                "jobs_data": [{
                    "bank_account": bank_account,
                    "bank_account_id": first_bank_account_id,
                    "transactions": valid_transactions,  # ‚úÖ Utiliser valid_transactions r√©solues depuis le cache
                    "instructions": instructions or "",  # Instructions sp√©cifiques au job
                    "banker_approval_required": approval_required,
                    "banker_approval_thresholdworkflow": str(approval_threshold)
                }],
                "start_instructions": start_instructions or "",  # Instructions g√©n√©rales pour tout le batch
                "settings": [
                    {'communication_mode': context['communication_mode']},
                    {'log_communication_mode': context['log_communication_mode']},
                    {'dms_system': context['dms_system']}
                ],
                "client_uuid": context['client_uuid'],
                "user_id": user_id,
                "mandates_path": context['mandate_path'],
                "proxy": False,
                "traceability": traceability_info
            }
            
            logger.info(
                f"[LPT_Banker] Lancement - batch_id={batch_id}, "
                f"nb_tx_valides={len(valid_transactions)}, nb_tx_invalides={len(invalid_ids)}, "
                f"thread={thread_key}, execution_id={execution_id}, plan={execution_plan}"
            )
            
            # Envoyer la requ√™te HTTP
            url = self.banker_url
            
            # ‚≠ê LOG: Afficher l'URL et le payload avant l'envoi
            logger.info(f"[LPT_Banker] üì§ Envoi HTTP POST vers: {url}")
            logger.info(f"[LPT_Banker] üì¶ Payload complet: {payload}")
            
            try:
                async with aiohttp.ClientSession() as session:
                    logger.info(f"[LPT_Banker] üîÑ Connexion en cours vers {url}...")
                    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        status = response.status
                        logger.info(f"[LPT_Banker] ‚úÖ R√©ponse HTTP re√ßue: status={status}")
                        
                        if status == 202:
                            logger.info(f"[LPT_Banker] ‚úì Lanc√© avec succ√®s - batch_id={batch_id}")
                            
                            # Sauvegarder la t√¢che
                            task_id = batch_id
                            await self._save_task_to_firebase(
                                user_id=user_id,
                                thread_key=thread_key,
                                task_id=task_id,
                                task_type="Banker",
                                payload=payload,
                                status="queued"
                            )
                            
                            # Cr√©er la notification
                            await self._create_banker_notification(
                                user_id=user_id,
                                company_id=company_id,
                                company_name=context['company_name'],
                                batch_id=batch_id,
                                job_id=batch_id,
                                bank_account=bank_account,
                                transactions=valid_transactions
                            )
                            
                            # Message de retour avec warning si certaines transactions invalides
                            success_message = f"‚úì Banker lanc√© : {len(valid_transactions)} transaction(s) en cours de traitement"
                            if len(invalid_ids) > 0:
                                success_message += f"\n‚ö†Ô∏è {len(invalid_ids)} transaction(s) invalide(s) ignor√©e(s): {invalid_ids}"
                            
                            return {
                                "status": "queued",
                                "task_id": task_id,
                                "batch_id": batch_id,
                                "nb_transactions_valid": len(valid_transactions),
                                "nb_transactions_invalid": len(invalid_ids),
                                "invalid_ids": invalid_ids if len(invalid_ids) > 0 else [],
                                "thread_key": thread_key,
                                "message": success_message
                            }
                        else:
                            error_text = await response.text()
                            logger.error(f"[LPT_Banker] ‚ùå Erreur HTTP {status}: {error_text}")
                            return {
                                "status": "error",
                                "error": f"HTTP {status}: {error_text}"
                            }
            
            except aiohttp.ClientError as ce:
                # Erreur de connexion ou r√©seau
                logger.error(f"[LPT_Banker] ‚ùå Erreur de connexion HTTP: {ce}", exc_info=True)
                return {
                    "status": "error",
                    "error": f"Erreur de connexion: {str(ce)}",
                    "error_type": "connection_error"
                }
            
            except asyncio.TimeoutError:
                # Timeout
                logger.error(f"[LPT_Banker] ‚è±Ô∏è Timeout apr√®s 30s vers {url}")
                return {
                    "status": "error",
                    "error": "Timeout de connexion (30s)",
                    "error_type": "timeout"
                }
        
        except ValueError as ve:
            # Erreur de configuration (donn√©es manquantes)
            logger.error(f"[LPT_Banker] Erreur de configuration: {ve}")
            return {
                "status": "configuration_error",
                "error": str(ve),
                "error_type": "missing_user_data",
                "message": "‚ö†Ô∏è Configuration utilisateur incompl√®te. V√©rifiez que l'utilisateur est correctement enregistr√©."
            }
        
        except Exception as e:
            # Erreur technique
            logger.error(f"[LPT_Banker] Erreur technique: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "error_type": "technical_error"
            }
    
    async def stop_apbookeeper(self, user_id: str, company_id: str, job_id: str) -> Dict[str, Any]:
        """Arr√™te une t√¢che APBookkeeper en cours."""
        try:
            payload = {"collection_name": company_id, "user_id": user_id, "job_id": job_id}

            # Construire l'URL de stop selon l'environnement
            if self.environment == 'LOCAL':
                url = f"{self.base_url}:8081/stop_apbookeeper"
            else:  # PROD
                url = f"{self.apbookeeper_url}/stop_apbookeeper".replace("/apbookeeper-event-trigger", "/stop_apbookeeper")
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 200:
                        return {"success": True, "message": f"APBookkeeper {job_id} arr√™t√©"}
                    else:
                        return {"success": False, "error": await response.text()}
        except Exception as e:
            logger.error(f"Erreur stop_apbookeeper: {e}")
            return {"success": False, "error": str(e)}
    
    async def stop_router(self, user_id: str, company_id: str, job_id: str) -> Dict[str, Any]:
        """Arr√™te une t√¢che Router en cours."""
        try:
            payload = {"collection_name": company_id, "user_id": user_id, "job_id": job_id}

            # Construire l'URL de stop selon l'environnement
            if self.environment == 'LOCAL':
                url = f"{self.base_url}:8080/stop_router"
            else:  # PROD
                url = f"{self.router_url}/stop_router".replace("/event-trigger", "/stop_router")
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 200:
                        return {"success": True, "message": f"Router {job_id} arr√™t√©"}
                    else:
                        return {"success": False, "error": await response.text()}
        except Exception as e:
            logger.error(f"Erreur stop_router: {e}")
            return {"success": False, "error": str(e)}
    
    async def stop_banker(self, user_id: str, company_id: str, job_id: str, batch_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Arr√™te une t√¢che Banker en cours.
        
        Args:
            user_id: ID utilisateur Firebase
            company_id: ID de la soci√©t√©
            job_id: ID du job √† arr√™ter
            batch_id: ID du batch (optionnel, pour coh√©rence avec APBookkeeper)
        """
        try:
            payload = {
                "collection_name": company_id, 
                "user_id": user_id, 
                "job_id": job_id
            }
            
            # Ajouter batch_id si fourni
            if batch_id:
                payload["batch_id"] = batch_id

            # Construire l'URL de stop selon l'environnement
            if self.environment == 'LOCAL':
                url = f"{self.base_url}:8082/stop_banker"
            else:  # PROD
                url = f"{self.banker_url}/stop_banker".replace("/banker-event-trigger", "/stop_banker")
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 200:
                        return {"success": True, "message": f"Banker {job_id} arr√™t√©"}
                    else:
                        return {"success": False, "error": await response.text()}
        except Exception as e:
            logger.error(f"Erreur stop_banker: {e}")
            return {"success": False, "error": str(e)}
    
    async def _save_task_to_firebase(
        self,
        user_id: str,
        thread_key: str,
        task_id: str,
        task_type: str,
        payload: Dict[str, Any],
        status: str
    ):
        """
        Sauvegarde une t√¢che LPT dans Firebase pour suivi UI.
        
        Path : clients/{user_id}/workflow_pinnokio/{doc_id}
        Structure : Dictionnaire index√© par thread_key
        """
        try:
            from ...firebase_providers import FirebaseManagement
            
            firebase_service = FirebaseManagement()
            
            # Path de sauvegarde
            workflow_path = f"clients/{user_id}/workflow_pinnokio"
            
            # R√©cup√©rer ou cr√©er le document pour ce thread
            # Le document sera cr√©√© avec le thread_key comme ID
            doc_ref = firebase_service.db.collection(workflow_path).document(thread_key)
            
            # Donn√©es de la t√¢che
            task_data = {
                "task_id": task_id,
                "task_type": task_type,
                "status": status,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "updated_at": datetime.now(timezone.utc).isoformat(),
                "payload_summary": {
                    "collection_name": payload.get("collection_name"),
                    "user_id": payload.get("user_id"),
                    "thread_key": thread_key
                }
            }
            
            # Mettre √† jour ou cr√©er le document
            doc_ref.set({
                "thread_key": thread_key,
                "user_id": user_id,
                "updated_at": datetime.now(timezone.utc).isoformat(),
                f"tasks.{task_id}": task_data
            }, merge=True)
            
            logger.info(f"T√¢che sauvegard√©e dans Firebase: {workflow_path}/{thread_key}/tasks/{task_id}")
        
        except Exception as e:
            logger.error(f"Erreur sauvegarde t√¢che Firebase: {e}", exc_info=True)
    
    async def _update_task_status(
        self,
        user_id: str,
        task_id: str,
        status: str,
        error: str = None
    ):
        """
        Met √† jour le statut d'une t√¢che LPT dans Firebase.
        Utilis√© en mode fire-and-forget pour notifier les erreurs HTTP.
        """
        try:
            from ...firebase_providers import FirebaseManagement
            
            firebase_service = FirebaseManagement()
            
            # Trouver le document contenant cette t√¢che
            # Format : clients/{user_id}/workflow_pinnokio/{thread_key}
            workflow_path = f"clients/{user_id}/workflow_pinnokio"
            
            # Requ√™te pour trouver le document contenant cette task_id
            query = firebase_service.db.collection(workflow_path).where(f"tasks.{task_id}.task_id", "==", task_id).limit(1)
            docs = query.get()
            
            if not docs:
                logger.warning(f"Aucun document trouv√© pour task_id={task_id}")
                return
            
            doc = docs[0]
            
            # Mettre √† jour le statut
            update_data = {
                f"tasks.{task_id}.status": status,
                f"tasks.{task_id}.updated_at": datetime.now(timezone.utc).isoformat()
            }
            
            if error:
                update_data[f"tasks.{task_id}.error"] = error
            
            doc.reference.update(update_data)
            
            logger.info(f"Statut de t√¢che mis √† jour: task_id={task_id}, status={status}")
        
        except Exception as e:
            logger.error(f"Erreur mise √† jour statut t√¢che Firebase: {e}", exc_info=True)
    
    async def _create_apbookeeper_notifications(
        self,
        user_id: str,
        company_id: str,
        company_name: str,
        batch_id: str,
        jobs_data: List[Dict[str, Any]]
    ):
        """Cr√©e les notifications Firebase pour APBookkeeper."""
        try:
            from ...firebase_providers import FirebaseManagement
            
            firebase_service = FirebaseManagement()
            notification_path = f"clients/{user_id}/notifications"
            
            # Cr√©er une notification par fichier
            for index, job in enumerate(jobs_data):
                notification_data = {
                    'function_name': 'APbookeeper',
                    'file_id': job['job_id'],
                    'job_id': job['job_id'],
                    'file_name': job['file_name'],
                    'journal_entries': "",
                    'status': 'in queue',
                    'read': False,
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'collection_id': company_id,
                    'collection_name': company_name,
                    'total_files': 1,
                    'batch_index': index + 1,
                    'batch_total': len(jobs_data),
                    'batch_id': batch_id
                }
                
                # Utiliser la m√©thode existante de FirebaseManagement
                firebase_service.add_or_update_job_by_file_id(notification_path, notification_data)
                
            logger.info(f"Notifications APBookkeeper cr√©√©es: {len(jobs_data)} notifications")
        
        except Exception as e:
            logger.error(f"Erreur cr√©ation notifications APBookkeeper: {e}", exc_info=True)
    
    async def _create_router_notification(
        self,
        user_id: str,
        company_id: str,
        company_name: str,
        drive_file_id: str,
        file_name: str,  # ‚≠ê NOUVEAU: Nom du fichier r√©solu
        pub_sub_id: str,
        instructions: Optional[str]
    ):
        """Cr√©e la notification Firebase pour Router."""
        try:
            from ...firebase_providers import FirebaseManagement
            
            firebase_service = FirebaseManagement()
            notification_path = f"clients/{user_id}/notifications"
            
            notification_data = {
                'function_name': 'Router',
                'file_id': drive_file_id,
                'job_id': "",
                'file_name': file_name,  # ‚úÖ Utiliser le vrai nom du fichier r√©solu
                'journal_entries': "",
                'pub_sub_id': pub_sub_id,
                'status': 'in queue',
                'read': False,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'collection_id': company_id,
                'collection_name': company_name,
                "instructions": instructions or ""
            }
            
            firebase_service.add_or_update_job_by_file_id(notification_path, notification_data)
            logger.info(f"Notification Router cr√©√©e: {drive_file_id}")
        
        except Exception as e:
            logger.error(f"Erreur cr√©ation notification Router: {e}", exc_info=True)
    
    async def _create_banker_notification(
        self,
        user_id: str,
        company_id: str,
        company_name: str,
        batch_id: str,
        job_id: str,
        bank_account: str,
        transactions: List[Dict[str, Any]]
    ):
        """Cr√©e la notification Firebase pour Banker."""
        try:
            from ...firebase_providers import FirebaseManagement
            
            firebase_service = FirebaseManagement()
            notification_path = f"clients/{user_id}/notifications"
            
            notification_data = {
                'function_name': 'Bankbookeeper',
                'job_id': job_id,
                'batch_id': batch_id,
                'bank_account': bank_account,
                'transactions': transactions,
                'status': 'in queue',
                'read': False,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'collection_id': company_id,
                'collection_name': company_name
            }
            
            firebase_service.add_or_update_job_by_job_id(notification_path, notification_data)
            logger.info(f"Notification Banker cr√©√©e: {batch_id}")
        
        except Exception as e:
            logger.error(f"Erreur cr√©ation notification Banker: {e}", exc_info=True)

    async def _create_onboarding_notification(
        self,
        user_id: str,
        company_id: str,
        company_name: Optional[str],
        thread_key: str,
        job_id: str,
        execution_id: str
    ):
        """Cr√©e la notification Firebase pour le job d'onboarding."""
        try:
            from ...firebase_providers import FirebaseManagement

            firebase_service = FirebaseManagement()
            notification_path = f"clients/{user_id}/notifications"

            notification_data = {
                'function_name': 'Onboarding',
                'job_id': job_id,
                'thread_key': thread_key,
                'execution_id': execution_id,
                'status': 'in queue',
                'read': False,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'collection_id': company_id,
                'collection_name': company_name,
                'message': 'Processus onboarding d√©marr√©'
            }

            firebase_service.add_or_update_job_by_job_id(notification_path, notification_data)
            logger.info(f"[LPT_Onboarding] Notification cr√©√©e: {job_id}")

        except Exception as e:
            logger.error(f"[LPT_Onboarding] Erreur cr√©ation notification Onboarding: {e}", exc_info=True)

    
    async def _reconstruct_full_company_profile(self, user_id: str, collection_name: str) -> Dict[str, Any]:
        """
        Reconstruit le profil complet de l'entreprise depuis Firebase.
        
        Cette m√©thode charge toutes les donn√©es de contexte n√©cessaires :
        - Informations client (client_uuid, client_name, etc.)
        - Informations mandat (mandate_*, contact_space_*, etc.)
        - Configuration ERP (erp_*, communication_mode, dms_system, etc.)
        - ‚≠ê Param√®tres de workflow (workflow_params avec approbations)
        
        Args:
            user_id: ID utilisateur Firebase
            collection_name: ID de la soci√©t√© (contact_space_id)
        
        Returns:
            Dict contenant le profil complet avec workflow_params
        """
        try:
            logger.info(
                f"[LPT_CONTEXT] Reconstruction profil complet - "
                f"user_id={user_id}, collection_name={collection_name}"
            )
            
            from ...firebase_providers import FirebaseManagement
            import asyncio
            
            firebase_service = FirebaseManagement()
            
            # √âtape 1 : R√©cup√©rer le client_uuid depuis bo_clients/{user_id}
            doc_ref = firebase_service.db.collection(
                f'clients/{user_id}/bo_clients'
            ).document(user_id)
            
            doc = await asyncio.to_thread(doc_ref.get)
            
            if not doc.exists:
                raise ValueError(
                    f"Document client non trouv√©: clients/{user_id}/bo_clients/{user_id}"
                )
            
            client_data = doc.to_dict()
            client_uuid = client_data.get('client_uuid')
            
            if not client_uuid:
                raise ValueError(
                    f"client_uuid manquant dans le document: clients/{user_id}/bo_clients/{user_id}"
                )
            
            logger.info(f"[LPT_CONTEXT] ‚úÖ client_uuid r√©cup√©r√©: {client_uuid}")
            
            # √âtape 2 : Appeler reconstruct_full_client_profile pour tout charger
            # ‚≠ê Cette m√©thode charge TOUT : client, mandat, ERP, ET workflow_params
            full_profile = await asyncio.to_thread(
                firebase_service.reconstruct_full_client_profile,
                user_id,
                client_uuid,
                collection_name  # contact_space_id
            )
            
            if not full_profile:
                raise ValueError(
                    f"Profil vide depuis reconstruct_full_client_profile - "
                    f"user_id={user_id}, client_uuid={client_uuid}, collection_name={collection_name}"
                )
            
            # Extraire les IDs pour construire mandate_path complet
            client_id = full_profile.get('_client_id')
            mandate_id = full_profile.get('_mandate_id')
            
            if not client_id or not mandate_id:
                raise ValueError(
                    f"client_id ou mandate_id manquant dans full_profile - "
                    f"client_id={client_id}, mandate_id={mandate_id}"
                )
            
            # ‚≠ê Construire le mandate_path complet (chemin Firebase r√©el)
            mandate_path = f'clients/{user_id}/bo_clients/{client_id}/mandates/{mandate_id}'
            
            # ‚≠ê Construire le contexte avec les noms de champs standardis√©s
            context = {
                # Identifiants
                "user_id": user_id,
                "collection_name": collection_name,
                "client_uuid": client_uuid,
                "client_id": client_id,
                "mandate_id": mandate_id,
                "mandate_path": mandate_path,
                "contact_space_id": full_profile.get("mandate_contact_space_id", collection_name),
                
                # Entreprise
                "company_name": full_profile.get("mandate_contact_space_name") or full_profile.get("client_name", "Entreprise"),
                "legal_name": full_profile.get("mandate_legal_name", ""),
                
                # Syst√®mes
                "dms_system": full_profile.get("mandate_dms_type", "google_drive"),
                "communication_mode": full_profile.get("mandate_communication_chat_type", "webhook"),
                "log_communication_mode": full_profile.get("mandate_communication_log_type", "firebase"),
                
                # Drive
                "drive_space_parent_id": full_profile.get("mandate_drive_space_parent_id", ""),
                "input_drive_doc_id": full_profile.get("mandate_input_drive_doc_id", ""),
                "output_drive_doc_id": full_profile.get("mandate_output_drive_doc_id", ""),
                
                # ERP
                "bank_erp": full_profile.get("mandate_bank_erp", ""),
                "ap_erp": full_profile.get("mandate_ap_erp", ""),
                "ar_erp": full_profile.get("mandate_ar_erp", ""),
                "gl_accounting_erp": full_profile.get("mandate_gl_accounting_erp", ""),
                
                # Configuration ERP Odoo
                "erp_odoo_url": full_profile.get("erp_odoo_url", ""),
                "erp_odoo_db": full_profile.get("erp_odoo_db", ""),
                "erp_odoo_username": full_profile.get("erp_odoo_username", ""),
                "erp_odoo_company_name": full_profile.get("erp_odoo_company_name", ""),
                "erp_secret_manager": full_profile.get("erp_secret_manager", ""),
                "erp_erp_type": full_profile.get("erp_erp_type", ""),
                
                # Localisation
                "country": full_profile.get("mandate_country", ""),
                "timezone": full_profile.get("mandate_timezone", "UTC"),
                "user_language": full_profile.get("mandate_user_language", "fr"),
                "base_currency": full_profile.get("mandate_base_currency", "CHF"),
                
                # ‚≠ê WORKFLOW PARAMS (param√®tres d'approbation)
                "workflow_params": full_profile.get("workflow_params", {})
            }
            
            logger.info(
                f"[LPT_CONTEXT] ‚úÖ Contexte reconstruit - "
                f"mandate_path={mandate_path}, "
                f"company_name={context['company_name']}, "
                f"dms_system={context['dms_system']}"
            )
            
            # üîç DEBUG : Afficher les workflow_params
            workflow_params = context.get("workflow_params", {})
            logger.info(f"[LPT_CONTEXT] üîç workflow_params charg√©s: {workflow_params}")
            
            if "Apbookeeper_param" in workflow_params:
                logger.info(
                    f"[LPT_CONTEXT] üîç Apbookeeper_param: "
                    f"approval_required={workflow_params['Apbookeeper_param'].get('apbookeeper_approval_required')}, "
                    f"approval_contact_creation={workflow_params['Apbookeeper_param'].get('apbookeeper_approval_contact_creation')}"
                )
            
            return context
        
        except Exception as e:
            logger.error(
                f"[LPT_CONTEXT] ‚ùå Erreur reconstruction profil - "
                f"user_id={user_id}, collection_name={collection_name}: {e}",
                exc_info=True
            )
            raise


