"""
Pinnokio Brain - Agent Cerveau Principal
Agent orchestrateur intelligent avec capacit√© de raisonnement pour g√©rer SPT et LPT

‚≠ê Architecture Stateless (Multi-Instance Ready):
L'historique de chat est externalis√© dans Redis via ChatHistoryManager.
Cela permet le scaling horizontal : n'importe quelle instance peut reprendre
une conversation cr√©√©e par une autre instance.
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone
import json

from ...llm.klk_agents import BaseAIAgent, ModelProvider, ModelSize, NEW_Anthropic_Agent, NEW_OpenAiAgent
from ...llm_service.chat_history_manager import get_chat_history_manager, ChatHistoryManager
from .agent_modes import get_agent_mode_config

logger = logging.getLogger("pinnokio.brain")


class PinnokioBrain:
    """
    Agent cerveau principal (Pinnokio) avec capacit√© d'orchestration SPT/LPT
    
    Responsabilit√©s:
    - Comprendre les requ√™tes utilisateur complexes
    - √âlaborer des plans d'action structur√©s
    - Orchestrer l'ex√©cution SPT (synchrone) et LPT (asynchrone)
    - Maintenir le contexte pendant l'ex√©cution
    - Communiquer avec l'utilisateur pendant les LPT
    """
    
    def __init__(self, 
                 collection_name: str,
                 firebase_user_id: str,
                 dms_system: str = "google_drive",
                 dms_mode: str = "prod"):
        """
        Initialise l'agent cerveau Pinnokio
        
        Args:
            collection_name: Nom de la collection (soci√©t√©)
            firebase_user_id: ID utilisateur Firebase
            dms_system: Syst√®me DMS (google_drive, etc.)
            dms_mode: Mode DMS (prod, test)
        """
        self.collection_name = collection_name
        self.firebase_user_id = firebase_user_id
        self.dms_system = dms_system
        self.dms_mode = dms_mode
        
        # ‚≠ê NOUVELLE ARCHITECTURE: Agent principal cr√©√© via initialize_agents()
        # Chaque brain a son propre agent principal isol√©
        self.pinnokio_agent: Optional[BaseAIAgent] = None
        
        # Configuration du provider (mod√®le de raisonnement)
        self.default_provider = ModelProvider.OPENAI
        self.default_size = ModelSize.MEDIUM  # Kimi K2 pour raisonnement + streaming + tools
        
        # ‚≠ê NOUVELLE ARCHITECTURE: L'historique est g√©r√© par self.pinnokio_agent
        # Plus de duplication d'historique au niveau du brain
        
        # √âtat de l'orchestration
        self.active_plans: Dict[str, Dict] = {}  # {thread_key: plan_data}
        self.active_lpt_tasks: Dict[str, List[str]] = {}  # {thread_key: [task_ids]}
        
        # ‚≠ê NOUVEAU: Contexte utilisateur (m√©tadonn√©es soci√©t√©)
        # Contient: mandate_path, dms_system, communication_mode, etc.
        # Accessible par tous les outils (SPT et LPT)
        self.user_context: Optional[Dict[str, Any]] = None
        
        # ‚≠ê NOUVEAU: Agent SPT ContextManager (sera initialis√© dans initialize_spt_agents)
        # Chaque agent SPT a son propre BaseAIAgent et chat_history isol√©
        self.context_manager = None
        
        # ‚≠ê NOUVEAU: Jobs data et m√©triques (assign√©s depuis LLMSession)
        # Ces donn√©es sont charg√©es √† l'initialisation de la session pour all√©ger le contexte
        self.jobs_data: Dict[str, Any] = {}  # Donn√©es compl√®tes des jobs (pour GET_JOBS)
        self.jobs_metrics: Dict[str, Any] = {}  # M√©triques pour le system prompt
        
        # ‚≠ê NOUVEAU: Thread actif (pour workflows d'approbation avec cartes)
        self.active_thread_key: Optional[str] = None
        
        # ‚≠ê NOUVEAU: Proposition de contexte en attente (pour UPDATE_CONTEXT ‚Üí PUBLISH_CONTEXT)
        self.context_proposal: Optional[Dict[str, Any]] = None

        # ‚≠ê NOUVEAU: Donn√©es de la t√¢che en cours d'ex√©cution (si mode task_execution)
        self.active_task_data: Optional[Dict[str, Any]] = None

        # ‚≠ê Mode de chat courant (utilis√© pour la config prompt/outils)
        self.current_chat_mode: str = "general_chat"

        # ‚≠ê Donn√©es sp√©cifiques onboarding (charg√©es √† la demande, uniquement pour onboarding_chat)
        self.onboarding_data: Optional[Dict[str, Any]] = None
        
        # ‚≠ê Donn√©es sp√©cifiques job (charg√©es √† la demande, pour router_chat, banker_chat, etc.)
        self.job_data: Optional[Dict[str, Any]] = None
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚≠ê ARCHITECTURE STATELESS (Multi-Instance Ready)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ChatHistoryManager externalise l'historique dans Redis
        # Approche hybride: cache local (performance) + Redis (durabilit√©)
        self._chat_history_manager: ChatHistoryManager = get_chat_history_manager()
        self._redis_sync_enabled: bool = True  # Activer la sync Redis

        logger.info(f"PinnokioBrain initialis√© pour user={firebase_user_id}, collection={collection_name}")
    
    async def initialize_agents(self):
        """
        Cr√©e les agents du brain (principal + outils SPT).
        
        ‚≠ê NOUVELLE ARCHITECTURE : Chaque brain a ses propres agents isol√©s
        
        Cr√©ation:
        1. Agent principal (pinnokio_agent) - BaseAIAgent pour interaction utilisateur
        2. Agents SPT (context_manager, etc.) - Pour outils rapides
        
        Cette m√©thode doit √™tre appel√©e imm√©diatement apr√®s la cr√©ation du brain,
        avant d'injecter les donn√©es de session et d'initialiser le system prompt.
        """
        try:
            logger.info(f"[BRAIN] ü§ñ Cr√©ation agents pour brain (user={self.firebase_user_id}, collection={self.collection_name})")
            
            # ‚ïê‚ïê‚ïê 1. Cr√©er l'agent principal ‚ïê‚ïê‚ïê
            self.pinnokio_agent = BaseAIAgent(
                collection_name=self.collection_name,
                dms_system=self.dms_system,
                dms_mode=self.dms_mode,
                firebase_user_id=self.firebase_user_id
            )
            
            # Configurer le provider et la taille par d√©faut
            self.pinnokio_agent.default_provider = self.default_provider
            self.pinnokio_agent.default_model_size = self.default_size
            
            # ‚ïê‚ïê‚ïê 2. Cr√©er et enregistrer l'instance du provider ‚ïê‚ïê‚ïê
            # Cr√©er l'instance OpenAI (sans arguments)
            openai_instance = NEW_OpenAiAgent()
            
            # Enregistrer le provider dans BaseAIAgent
            # BaseAIAgent a d√©j√† collection_name, dms_system, dms_mode, firebase_user_id
            self.pinnokio_agent.register_provider(
                provider=self.default_provider,
                instance=openai_instance,
                default_model_size=self.default_size
            )
            
            logger.info(f"[BRAIN] ‚úÖ Agent principal cr√©√© (provider={self.default_provider.value}, size={self.default_size.value}, model=Kimi K2)")
            
            # ‚ïê‚ïê‚ïê 3. Cr√©er les agents SPT ‚ïê‚ïê‚ïê
            
            logger.info(f"[BRAIN] ‚úÖ Agents SPT cr√©√©s")
            
            logger.info(f"[BRAIN] üéâ Tous les agents cr√©√©s avec succ√®s")
            
        except Exception as e:
            logger.error(f"[BRAIN] ‚ùå Erreur cr√©ation agents: {e}", exc_info=True)
            raise
    
    def initialize_system_prompt(self, chat_mode: str = "general_chat", jobs_metrics: Dict = None):
        """Initialise le system prompt en fonction du mode d√©clar√©."""

        config = get_agent_mode_config(chat_mode)

        if not self.pinnokio_agent:
            raise RuntimeError("Pinnokio agent non initialis√© avant initialize_system_prompt")

        prompt = config.prompt_builder(self, jobs_metrics, chat_mode)
        self.pinnokio_agent.update_system_prompt(prompt)
        self.current_chat_mode = config.name

        logger.info(
            f"System prompt initialis√© pour mode={chat_mode} (config={config.name})"
        )
    
    
    def create_workflow_tools(
        self,
        thread_key: str,
        session=None,
        chat_mode: str = "general_chat",
        mode: str = "UI",  # ‚≠ê NOUVEAU : Mode UI ou BACKEND pour rechargement Redis
    ) -> Tuple[List[Dict], Dict]:
        """
        Retourne l'ensemble d'outils configur√© pour le mode de chat.
        
        Args:
            thread_key: Cl√© du thread
            session: Session LLM (optionnel)
            chat_mode: Mode de chat (general_chat, router_chat, etc.)
            mode: "UI" (utilisateur connect√©, cache Redis √† jour) ou "BACKEND" (utilisateur d√©connect√©)
        """

        config = get_agent_mode_config(chat_mode)
        tool_set, tool_mapping = config.tool_builder(self, thread_key, session, chat_mode, mode=mode)

        logger.info(
            f"Outils initialis√©s pour mode={chat_mode} (config={config.name}) : {len(tool_set)} outils"
        )
        return tool_set, tool_mapping


    def _build_general_chat_tools(self, thread_key: str, session=None, mode: str = "UI") -> Tuple[List[Dict], Dict]:
        """
        Construit l'ensemble d'outils standard (mode g√©n√©ral).
        
        Args:
            thread_key: Cl√© du thread
            session: Session LLM (optionnel)
            mode: "UI" (utilisateur connect√©, cache Redis √† jour) ou "BACKEND" (utilisateur d√©connect√©)
        """
        # ‚≠ê Stocker le mode pour les handlers d'outils
        self._current_mode = mode
        
        from ..tools.spt_tools import SPTTools
        from ..tools.lpt_client import LPTClient
        
        
        # Cr√©er les outils SPT
        # ‚≠ê IMPORTANT : Passer le brain pour acc√®s au contexte utilisateur
        spt_tools = SPTTools(
            firebase_user_id=self.firebase_user_id,
            collection_name=self.collection_name,
            brain=self
        )
        spt_tools_list = spt_tools.get_tools_definitions()
        spt_tools_mapping = spt_tools.get_tools_mapping()

        # ‚ö†Ô∏è SPT_CONTEXT_MANAGER D√âSACTIV√â TEMPORAIREMENT
        # Les outils de contexte sont maintenant int√©gr√©s directement dans l'agent principal
        # via ContextTools (job_tools.py) pour un acc√®s plus rapide et direct.
        # Le code SPT est conserv√© pour usage futur avec d'autres agents SPT.
        #
        # from ..tools.spt_context_manager import create_spt_context_manager_wrapper
        # tool_def, handler = create_spt_context_manager_wrapper(self)
        # spt_tools_list.append(tool_def)
        # spt_tools_mapping["SPT_CONTEXT_MANAGER"] = handler
        
        # Cr√©er les outils LPT avec session pour cache
        lpt_client = LPTClient()
        lpt_tools_list, lpt_tools_mapping = lpt_client.get_tools_definitions_and_mapping(
            user_id=self.firebase_user_id,
            company_id=self.collection_name,
            thread_key=thread_key,
            session=session,  # ‚≠ê Passer la session pour le cache
            brain=self        # ‚≠ê IMPORTANT: Passer le brain pour acc√®s au contexte utilisateur
        )
        
        # ‚ïê‚ïê‚ïê OUTILS JOBS (3 outils s√©par√©s par d√©partement) ‚ïê‚ïê‚ïê
        # Cr√©er les 3 outils jobs avec leurs handlers
        from ..tools.job_tools import APBookkeeperJobTools, RouterJobTools, BankJobTools, ExpenseJobTools, ContextTools
        
        # üîç LOGS DE DIAGNOSTIC - V√©rifier jobs_data avant cr√©ation outils
        logger.info(f"[BRAIN] üîç DIAGNOSTIC self.jobs_data avant cr√©ation outils - "
                   f"Cl√©s: {list(self.jobs_data.keys()) if self.jobs_data else 'None'}")
        if self.jobs_data and 'ROUTER' in self.jobs_data:
            router_to_process = self.jobs_data['ROUTER'].get('to_process', [])  # ‚úÖ Corrig√© : format Reflex utilise 'to_process'
            logger.info(f"[BRAIN] üîç DIAGNOSTIC self.jobs_data['ROUTER']['to_process'] - "
                       f"Longueur: {len(router_to_process) if isinstance(router_to_process, list) else 'N/A'}")
        else:
            logger.warning(f"[BRAIN] ‚ö†Ô∏è DIAGNOSTIC - Pas de donn√©es ROUTER dans self.jobs_data !")
        
        # ‚≠ê D√©terminer le mode (UI si user_context existe, BACKEND sinon)
        # Le mode UI signifie que l'utilisateur est connect√© et que le cache Redis est √† jour
        mode = "UI" if self.user_context else "BACKEND"
        
        # 1. APBookkeeper Jobs - ‚≠ê Passer param√®tres pour rechargement Redis
        apbookeeper_tools = APBookkeeperJobTools(
            jobs_data=self.jobs_data,
            user_id=self.firebase_user_id,
            company_id=self.collection_name,
            user_context=self.user_context,
            mode=mode
        )
        get_apbookeeper_jobs_def = apbookeeper_tools.get_tool_definition()
        
        async def handle_get_apbookeeper_jobs(**kwargs):
            return await apbookeeper_tools.search(**kwargs)
        
        # 2. Router Jobs - ‚≠ê Passer param√®tres pour rechargement Redis
        router_tools = RouterJobTools(
            jobs_data=self.jobs_data,
            user_id=self.firebase_user_id,
            company_id=self.collection_name,
            user_context=self.user_context,
            mode=mode
        )
        get_router_jobs_def = router_tools.get_tool_definition()
        
        async def handle_get_router_jobs(**kwargs):
            return await router_tools.search(**kwargs)
        
        # 3. Bank Transactions - ‚≠ê Passer param√®tres pour rechargement Redis
        bank_tools = BankJobTools(
            jobs_data=self.jobs_data,
            user_id=self.firebase_user_id,
            company_id=self.collection_name,
            user_context=self.user_context,
            mode=mode
        )
        get_bank_transactions_def = bank_tools.get_tool_definition()
        
        async def handle_get_bank_transactions(**kwargs):
            return await bank_tools.search(**kwargs)
        
        # 4. Expenses - ‚≠ê Passer param√®tres pour rechargement Redis
        expenses_tools = ExpenseJobTools(
            jobs_data=self.jobs_data,
            user_id=self.firebase_user_id,
            company_id=self.collection_name,
            user_context=self.user_context,
            mode=mode
        )
        get_expenses_info_def = expenses_tools.get_tool_definition()
        
        async def handle_get_expenses_info(**kwargs):
            return await expenses_tools.search(**kwargs)
        
        # ‚ïê‚ïê‚ïê OUTILS CONTEXT + TASK_MANAGER (Firestore) ‚ïê‚ïê‚ïê
        # Cr√©er l'acc√®s Firebase (r√©utilis√© par plusieurs outils)
        from ...firebase_providers import FirebaseManagement
        firebase_management = FirebaseManagement()

        # ‚ïê‚ïê‚ïê OUTILS TASK_MANAGER (index + d√©tails audit) ‚ïê‚ïê‚ïê
        # Outils contractuels : clients/{userId}/task_manager/{job_id} + events/
        from ..tools.task_manager_tools import TaskManagerTools

        task_manager_tools = TaskManagerTools(firebase_management=firebase_management, brain=self)

        get_task_manager_index_def = task_manager_tools.get_task_manager_index_definition()
        get_task_manager_details_def = task_manager_tools.get_task_manager_details_definition()

        async def handle_get_task_manager_index(**kwargs):
            return await task_manager_tools.get_index(**kwargs)

        async def handle_get_task_manager_details(**kwargs):
            return await task_manager_tools.get_details(**kwargs)
        # ‚ïê‚ïê‚ïê OUTILS CONTEXT (5 outils d'acc√®s et modification des contextes) ‚ïê‚ïê‚ïê
        context_tools = ContextTools(
            firebase_management=firebase_management,
            firebase_user_id=self.firebase_user_id,
            collection_name=self.collection_name,
            brain=self  # ‚úÖ Passer le brain pour acc√®s au user_context
        )
        
        # D√©finitions des outils de contexte
        router_prompt_def = context_tools.get_router_prompt_definition()
        apbookeeper_context_def = context_tools.get_apbookeeper_context_definition()
        bank_context_def = context_tools.get_bank_context_definition()
        company_context_def = context_tools.get_company_context_definition()
        update_context_def = context_tools.get_update_context_definition()
        
        # Handlers pour les outils de contexte
        async def handle_router_prompt(**kwargs):
            return await context_tools.get_router_prompt(**kwargs)
        
        async def handle_apbookeeper_context(**kwargs):
            return await context_tools.get_apbookeeper_context(**kwargs)
        
        async def handle_company_context(**kwargs):
            return await context_tools.get_company_context(**kwargs)

        async def handle_bank_context(**kwargs):
            return await context_tools.get_bank_context(**kwargs)
        
        async def handle_update_context(**kwargs):
            return await context_tools.update_context(**kwargs)

        # ‚ïê‚ïê‚ïê OUTIL VISION DOCUMENT DRIVE ‚ïê‚ïê‚ïê
        view_drive_document_def = {
            "name": "VIEW_DRIVE_DOCUMENT",
            "description": "üñºÔ∏è Visualiser un document Google Drive (PDF, image, facture). Requis: file_id obtenu via GET_APBOOKEEPER_JOBS ou GET_ROUTER_JOBS. GET_TOOL_HELP pour d√©tails.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_id": {
                        "type": "string",
                        "description": "ID du fichier Google Drive √† visionner (ex: '1A2B3C4D5E')"
                    },
                    "question": {
                        "type": "string",
                        "description": "Question sp√©cifique sur le document (optionnel). Si non fourni, fait une analyse g√©n√©rale."
                    }
                },
                "required": ["file_id"]
            }
        }
        
        async def handle_view_drive_document(**kwargs):
            """Handler pour visionner un document Google Drive."""
            try:
                file_id = kwargs.get("file_id")
                question = kwargs.get("question", "D√©cris le contenu de ce document en d√©tail.")
                
                # ‚úÖ VALIDATION : V√©rifier que file_id est fourni et non vide
                if not file_id or not isinstance(file_id, str) or len(file_id.strip()) == 0:
                    error_msg = (
                        "‚ùå Param√®tre 'file_id' manquant ou invalide. "
                        "Pour voir un document, tu DOIS d'abord r√©cup√©rer son drive_file_id "
                        "en utilisant GET_APBOOKEEPER_JOBS, GET_ROUTER_JOBS, GET_BANK_TRANSACTIONS ou GET_EXPENSES_INFO."
                    )
                    logger.warning(f"[VIEW_DRIVE_DOCUMENT] {error_msg}")
                    return {
                        "type": "error",
                        "message": error_msg
                    }
                
                # V√©rifier que le DMS est disponible
                if not self.pinnokio_agent or not self.pinnokio_agent.dms_system:
                    return {
                        "type": "error",
                        "message": "Syst√®me DMS non initialis√©. Impossible d'acc√©der aux documents Drive."
                    }
                
                logger.info(f"[VIEW_DRIVE_DOCUMENT] üñºÔ∏è Vision du document: file_id={file_id}")
                
                # Utiliser process_vision de BaseAIAgent avec Groq (Llama Scout)
                response = await asyncio.to_thread(
                    self.pinnokio_agent.process_vision,
                    text=question,
                    provider=self.default_provider,  # GROQ
                    size=ModelSize.MEDIUM,  # Llama Scout 17B (vision)
                    file_ids=[file_id],  # üî• CORRECTION: param√®tre renomm√© drive_file_ids -> file_ids
                    method='batch',
                    max_tokens=2000,
                    final_resume=True
                )
                
                logger.info(f"[VIEW_DRIVE_DOCUMENT] ‚úÖ Analyse termin√©e")
                
                return {
                    "type": "success",
                    "file_id": file_id,
                    "analysis": response if isinstance(response, str) else response.get('text_output', str(response))
                }
                
            except ImportError as e:
                # ‚úÖ G√©rer sp√©cifiquement les erreurs d'import de pdf2image
                error_msg = str(e)
                if "pdf2image" in error_msg.lower() or "poppler" in error_msg.lower():
                    detailed_msg = (
                        f"Le module 'pdf2image' n'est pas disponible pour analyser le fichier '{file_id}'. "
                        f"Installez-le avec: pip install pdf2image. "
                        f"Sur Windows, vous devez aussi installer poppler: "
                        f"https://github.com/oschwartz10612/poppler-windows/releases/"
                    )
                else:
                    detailed_msg = f"Erreur d'import: {error_msg}"
                logger.error(f"[VIEW_DRIVE_DOCUMENT] ‚ùå Erreur d'import: {e}")
                return {
                    "type": "error",
                    "message": detailed_msg,
                    "file_id": file_id
                }
            except FileNotFoundError as e:
                # ‚úÖ G√©rer sp√©cifiquement les erreurs 404 (fichier non trouv√©)
                error_msg = f"Le fichier Google Drive '{file_id}' n'a pas √©t√© trouv√©. Il a peut-√™tre √©t√© supprim√©, d√©plac√©, ou vous n'avez pas les permissions n√©cessaires pour y acc√©der."
                logger.error(f"[VIEW_DRIVE_DOCUMENT] ‚ùå Fichier non trouv√©: {e}")
                return {
                    "type": "error",
                    "message": error_msg,
                    "file_id": file_id
                }
            except ValueError as e:
                # ‚úÖ G√©rer les erreurs de conversion/transformation
                error_msg = str(e)
                if "Aucun contenu d'image" in error_msg or "Aucune image" in error_msg:
                    logger.error(f"[VIEW_DRIVE_DOCUMENT] ‚ùå Erreur de traitement: {e}")
                    return {
                        "type": "error",
                        "message": f"Impossible de traiter le fichier '{file_id}'. {error_msg}",
                        "file_id": file_id
                    }
                else:
                    logger.error(f"[VIEW_DRIVE_DOCUMENT] ‚ùå Erreur de validation: {e}")
                    return {
                        "type": "error",
                        "message": error_msg,
                        "file_id": file_id
                    }
            except Exception as e:
                logger.error(f"[VIEW_DRIVE_DOCUMENT] ‚ùå Erreur: {e}", exc_info=True)
                return {
                    "type": "error",
                    "message": f"Erreur lors de la vision du document: {str(e)}"
                }

        # ‚ïê‚ïê‚ïê OUTILS TASK (gestion t√¢ches planifi√©es) ‚ïê‚ïê‚ïê
        from ..tools.task_tools import TaskTools

        task_tools = TaskTools(brain=self)
        create_task_def = task_tools.get_tool_definition()

        async def handle_create_task(**kwargs):
            return await task_tools.create_task(**kwargs)

        # ‚ïê‚ïê‚ïê OUTILS WORKFLOW CHECKLIST (pour t√¢ches planifi√©es) ‚ïê‚ïê‚ïê
        create_checklist_tool = {
            "name": "CREATE_CHECKLIST",
            "description": "üìã Cr√©er la checklist de workflow (mode task_execution). Chaque √©tape: id + name. GET_TOOL_HELP pour d√©tails.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "steps": {
                        "type": "array",
                        "description": "Liste des √©tapes √† r√©aliser",
                        "items": {
                            "type": "object",
                            "properties": {
                                "id": {
                                    "type": "string",
                                    "description": "ID unique (ex: 'STEP_1_GET_TRANSACTIONS')"
                                },
                                "name": {
                                    "type": "string",
                                    "description": "Nom descriptif de l'√©tape"
                                }
                            },
                            "required": ["id", "name"]
                        }
                    }
                },
                "required": ["steps"]
            }
        }

        update_step_tool = {
            "name": "UPDATE_STEP",
            "description": "üìä Mettre √† jour le statut d'une √©tape (in_progress/completed/error). GET_TOOL_HELP pour d√©tails.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "step_id": {
                        "type": "string",
                        "description": "ID de l'√©tape"
                    },
                    "status": {
                        "type": "string",
                        "enum": ["in_progress", "completed", "error"],
                        "description": "Nouveau statut"
                    },
                    "message": {
                        "type": "string",
                        "description": "Message descriptif"
                    }
                },
                "required": ["step_id", "status", "message"]
            }
        }

        async def handle_create_checklist(**kwargs):
            """Cr√©e la workflow checklist."""
            try:
                steps = kwargs["steps"]

                # Valider qu'on est en mode t√¢che
                if not self.active_task_data:
                    return {"type": "error", "message": "Non disponible (mode normal)"}

                task_id = self.active_task_data["task_id"]
                execution_id = self.active_task_data["execution_id"]
                mandate_path = self.active_task_data["mandate_path"]
                thread_key = self.active_thread_key

                # Pr√©parer les √©tapes
                formatted_steps = []
                for step in steps:
                    formatted_steps.append({
                        "id": step["id"],
                        "name": step["name"],
                        "status": "pending",
                        "timestamp": "",
                        "message": ""
                    })

                checklist_data = {
                    "total_steps": len(formatted_steps),
                    "current_step": 0,
                    "steps": formatted_steps
                }

                # Sauvegarder dans execution
                from ...firebase_providers import get_firebase_management
                fbm = get_firebase_management()

                fbm.update_task_execution(
                    mandate_path, task_id, execution_id,
                    {"workflow_checklist": checklist_data}
                )

                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # ENVOI PAR WEBSOCKET + RTDB (comme pour les messages de chat)
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                
                from ...ws_hub import hub
                from ...firebase_providers import get_firebase_realtime
                import uuid
                
                checklist_message_id = str(uuid.uuid4())
                timestamp = datetime.now(timezone.utc).isoformat()
                
                # Construire le message de commande
                user_language = self.user_context.get("user_language", "fr") if self.user_context else "fr"
                
                checklist_command = {
                    "action": "SET_WORKFLOW_CHECKLIST",
                    "params": {
                        "checklist": checklist_data,
                        "user_language": user_language
                    }
                }
                
                # 1. Envoi imm√©diat par WebSocket
                ws_message = {
                    "type": "WORKFLOW_CHECKLIST",
                    "thread_key": thread_key,
                    "timestamp": timestamp,
                    "message_id": checklist_message_id,
                    "content": json.dumps({
                        "message": {
                            "cmmd": checklist_command
                        }
                    })
                }
                
                ws_channel = f"chat:{self.firebase_user_id}:{self.collection_name}:{thread_key}"
                
                # ‚≠ê Broadcast conditionnel selon le mode (UI/BACKEND)
                current_mode = getattr(self, "_current_mode", "UI")
                if current_mode == "UI":
                    await hub.broadcast(self.firebase_user_id, {
                        "type": "WORKFLOW_CHECKLIST",
                        "channel": ws_channel,
                        "payload": ws_message
                    })
                    logger.info(f"[CREATE_CHECKLIST] üì° Checklist envoy√©e via WebSocket (mode={current_mode})")
                else:
                    logger.info(f"[CREATE_CHECKLIST] ‚è≠Ô∏è Broadcast WebSocket ignor√© (mode={current_mode})")
                
                # 2. Sauvegarde dans RTDB pour persistence
                rtdb = get_firebase_realtime()
                rtdb_path = f"{self.collection_name}/chats/{thread_key}/messages/{checklist_message_id}"
                
                message_data = {
                    'content': json.dumps({
                        'message': {
                            'cmmd': checklist_command
                        }
                    }),
                    'sender_id': self.firebase_user_id,
                    'timestamp': timestamp,
                    'message_type': 'CMMD',
                    'read': False,
                    'role': 'assistant'
                }
                
                # Utiliser push() pour g√©n√©rer une cl√© unique
                thread_path = f"{self.collection_name}/chats/{thread_key}"
                messages_ref = rtdb.db.child(f'{thread_path}/messages')
                messages_ref.push(message_data)
                
                logger.info(f"[CREATE_CHECKLIST] üíæ Checklist sauvegard√©e dans RTDB")
                logger.info(f"[CREATE_CHECKLIST] ‚úÖ {len(formatted_steps)} √©tapes cr√©√©es")

                return {
                    "type": "success",
                    "message": f"Checklist cr√©√©e : {len(formatted_steps)} √©tapes",
                    "total_steps": len(formatted_steps)
                }

            except Exception as e:
                logger.error(f"[CREATE_CHECKLIST] Erreur: {e}", exc_info=True)
                return {"type": "error", "message": str(e)}

        async def handle_update_step(**kwargs):
            """Met √† jour une √©tape de la checklist."""
            try:
                step_id = kwargs["step_id"]
                status = kwargs["status"]
                message = kwargs["message"]

                # Valider mode t√¢che
                if not self.active_task_data:
                    return {"type": "error", "message": "Non disponible (mode normal)"}

                task_id = self.active_task_data["task_id"]
                execution_id = self.active_task_data["execution_id"]
                mandate_path = self.active_task_data["mandate_path"]
                thread_key = self.active_thread_key

                # R√©cup√©rer l'ex√©cution
                from ...firebase_providers import get_firebase_management
                fbm = get_firebase_management()

                execution = fbm.get_task_execution(mandate_path, task_id, execution_id)

                if not execution:
                    return {"type": "error", "message": "Ex√©cution non trouv√©e"}

                checklist = execution.get("workflow_checklist", {})
                steps = checklist.get("steps", [])

                # Trouver et mettre √† jour l'√©tape
                step_found = False
                for step in steps:
                    if step["id"] == step_id:
                        step["status"] = status
                        step["timestamp"] = datetime.now(timezone.utc).isoformat()
                        step["message"] = message
                        step_found = True
                        break

                if not step_found:
                    return {"type": "error", "message": f"√âtape {step_id} non trouv√©e"}

                # Sauvegarder
                fbm.update_task_execution(
                    mandate_path, task_id, execution_id,
                    {"workflow_checklist.steps": steps}
                )

                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # ENVOI PAR WEBSOCKET + RTDB (comme pour les messages de chat)
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                
                from ...ws_hub import hub
                from ...firebase_providers import get_firebase_realtime
                import uuid
                
                update_message_id = str(uuid.uuid4())
                timestamp = datetime.now(timezone.utc).isoformat()
                
                # Construire le message de commande
                update_command = {
                    "action": "UPDATE_STEP_STATUS",
                    "params": {
                        "step_id": step_id,
                        "status": status,
                        "timestamp": timestamp,
                        "message": message
                    }
                }
                
                # 1. Envoi imm√©diat par WebSocket
                ws_message = {
                    "type": "WORKFLOW_STEP_UPDATE",
                    "thread_key": thread_key,
                    "timestamp": timestamp,
                    "message_id": update_message_id,
                    "content": json.dumps({
                        "message": {
                            "cmmd": update_command
                        }
                    })
                }
                
                ws_channel = f"chat:{self.firebase_user_id}:{self.collection_name}:{thread_key}"
                
                # ‚≠ê Broadcast conditionnel selon le mode (UI/BACKEND)
                current_mode = getattr(self, "_current_mode", "UI")
                if current_mode == "UI":
                    await hub.broadcast(self.firebase_user_id, {
                        "type": "WORKFLOW_STEP_UPDATE",
                        "channel": ws_channel,
                        "payload": ws_message
                    })
                    logger.info(f"[UPDATE_STEP] üì° Mise √† jour envoy√©e via WebSocket (mode={current_mode})")
                else:
                    logger.info(f"[UPDATE_STEP] ‚è≠Ô∏è Broadcast WebSocket ignor√© (mode={current_mode})")
                
                # 2. Sauvegarde dans RTDB pour persistence
                rtdb = get_firebase_realtime()
                
                message_data = {
                    'content': json.dumps({
                        'message': {
                            'cmmd': update_command
                        }
                    }),
                    'sender_id': self.firebase_user_id,
                    'timestamp': timestamp,
                    'message_type': 'CMMD',
                    'read': False,
                    'role': 'assistant'
                }
                
                # Utiliser push() pour g√©n√©rer une cl√© unique
                thread_path = f"{self.collection_name}/chats/{thread_key}"
                messages_ref = rtdb.db.child(f'{thread_path}/messages')
                messages_ref.push(message_data)
                
                logger.info(f"[UPDATE_STEP] üíæ Mise √† jour sauvegard√©e dans RTDB")
                logger.info(f"[UPDATE_STEP] ‚úÖ {step_id} ‚Üí {status}: {message}")

                return {
                    "type": "success",
                    "message": f"√âtape {step_id} mise √† jour : {status}"
                }

            except Exception as e:
                logger.error(f"[UPDATE_STEP] Erreur: {e}", exc_info=True)
                return {"type": "error", "message": str(e)}

        # Outil GET_CURRENT_DATETIME
        get_current_datetime_tool = {
            "name": "GET_CURRENT_DATETIME",
            "description": "‚è∞ Date/heure actuelles. Optionnel: timezone IANA, format (ISO/READABLE/BOTH). GET_TOOL_HELP pour d√©tails.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "timezone": {
                        "type": "string",
                        "description": "Timezone IANA optionnelle (ex: 'Europe/Paris'). Si non fournie, utilise la timezone de la soci√©t√©."
                    },
                    "format": {
                        "type": "string",
                        "enum": ["ISO", "READABLE", "BOTH"],
                        "description": "Format de sortie souhait√© (d√©faut: BOTH)"
                    }
                },
                "required": []
            }
        }
        
        # Handler GET_CURRENT_DATETIME
        def handle_get_current_datetime(timezone: str = None, format: str = "BOTH"):
            return self._get_current_datetime(timezone, format)
        
        # Outil TERMINATE_TASK
        terminate_tool = {
            "name": "TERMINATE_TASK",
            "description": "üéØ Terminer la t√¢che quand la mission est accomplie. Utilisez cet outil d√®s que vous avez r√©solu la requ√™te de l'utilisateur et fourni une r√©ponse compl√®te.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "reason": {
                        "type": "string",
                        "description": "Raison de la terminaison (ex: 'Mission accomplie', 'Information fournie', 'T√¢che longue lanc√©e')"
                    },
                    "conclusion": {
                        "type": "string",
                        "description": "Votre r√©ponse finale COMPL√àTE pour l'utilisateur, r√©sumant les actions effectu√©es et les r√©sultats."
                    }
                },
                "required": ["reason", "conclusion"]
            }
        }
        
        # ‚ïê‚ïê‚ïê OUTIL WAIT_ON_LPT ‚ïê‚ïê‚ïê
        # Cr√©er l'outil WAIT_ON_LPT pour mettre en pause le workflow en attente d'un callback LPT
        from ..tools.wait_on_lpt import create_wait_on_lpt_tool
        
        wait_on_lpt_def, wait_on_lpt_mapping = create_wait_on_lpt_tool(
            brain=self,
            thread_key=thread_key,
            mode=mode
        )
        
        async def handle_wait_on_lpt(**kwargs):
            """Handler pour WAIT_ON_LPT."""
            return await wait_on_lpt_mapping["WAIT_ON_LPT"](**kwargs)
        
        # ‚ïê‚ïê‚ïê REGISTRE D'AIDE DYNAMIQUE (GET_TOOL_HELP) ‚ïê‚ïê‚ïê
        from ..tools.tool_help_registry import ToolHelpRegistry, DETAILED_HELP
        
        help_registry = ToolHelpRegistry()
        
        # Enregistrer la documentation d√©taill√©e de tous les outils
        help_registry.register_multiple(DETAILED_HELP)
        
        # Cr√©er l'outil GET_TOOL_HELP dynamiquement
        get_tool_help_def, handle_get_tool_help = help_registry.create_get_tool_help()
        
        # Combiner tous les outils (avec les 4 outils jobs + 4 outils context + VIEW_DRIVE_DOCUMENT + CREATE_TASK + checklist + datetime + WAIT_ON_LPT + GET_TOOL_HELP)
        tool_set = [
            get_apbookeeper_jobs_def,
            get_router_jobs_def,
            get_bank_transactions_def,
            get_expenses_info_def,
            get_task_manager_index_def,
            get_task_manager_details_def,
            router_prompt_def,
            apbookeeper_context_def,
            bank_context_def,
            company_context_def,
            update_context_def,
            view_drive_document_def,  # ‚≠ê Outil de vision Drive
            create_task_def,
            create_checklist_tool,
            update_step_tool,
            get_current_datetime_tool,  # ‚è∞ Outil date/heure actuelle
            wait_on_lpt_def,  # ‚è≥ Outil WAIT_ON_LPT
            get_tool_help_def  # üìö Outil GET_TOOL_HELP dynamique
        ] + spt_tools_list + lpt_tools_list + [terminate_tool]

        tool_mapping = {
            "GET_APBOOKEEPER_JOBS": handle_get_apbookeeper_jobs,
            "GET_ROUTER_JOBS": handle_get_router_jobs,
            "GET_BANK_TRANSACTIONS": handle_get_bank_transactions,
            "GET_EXPENSES_INFO": handle_get_expenses_info,
            "GET_TASK_MANAGER_INDEX": handle_get_task_manager_index,
            "GET_TASK_MANAGER_DETAILS": handle_get_task_manager_details,
            "ROUTER_PROMPT": handle_router_prompt,
            "APBOOKEEPER_CONTEXT": handle_apbookeeper_context,
            "BANK_CONTEXT": handle_bank_context,
            "COMPANY_CONTEXT": handle_company_context,
            "UPDATE_CONTEXT": handle_update_context,
            "VIEW_DRIVE_DOCUMENT": handle_view_drive_document,  # ‚≠ê Handler vision Drive
            "CREATE_TASK": handle_create_task,
            "CREATE_CHECKLIST": handle_create_checklist,
            "UPDATE_STEP": handle_update_step,
            "GET_CURRENT_DATETIME": handle_get_current_datetime,  # ‚è∞ Handler date/heure
            "WAIT_ON_LPT": handle_wait_on_lpt,  # ‚è≥ Handler WAIT_ON_LPT
            "TERMINATE_TASK": self._handle_terminate_task,  # üèÅ Handler terminaison
            "GET_TOOL_HELP": handle_get_tool_help,  # üìö Handler aide dynamique
            **spt_tools_mapping,
            **lpt_tools_mapping
        }
        
        # ‚≠ê RETOURNER LES NOUVEAUX OUTILS (SPT + LPT simplifi√©s + GET_TOOL_HELP)
        logger.info(f"Outils cr√©√©s: {len(tool_set)} outils (SPT: {len(spt_tools_list)}, LPT: {len(lpt_tools_list)}, HELP: {len(help_registry.get_available_tools())} outils document√©s)")
        return tool_set, tool_mapping

    async def load_onboarding_data(self) -> Dict[str, Any]:
        """Charge les donn√©es d'onboarding sp√©cifiques √† l'utilisateur."""

        if self.onboarding_data is not None:
            return self.onboarding_data

        try:
            from ...firebase_providers import FirebaseManagement

            firebase = FirebaseManagement()
            onboarding_path = f"clients/{self.firebase_user_id}/temp_data/onboarding"
            doc_ref = firebase.db.document(onboarding_path)
            doc = await asyncio.to_thread(doc_ref.get)

            if doc.exists:
                self.onboarding_data = doc.to_dict() or {}
                logger.info(
                    f"[BRAIN_ONBOARDING] Donn√©es onboarding charg√©es ({list(self.onboarding_data.keys())})"
                )
            else:
                logger.warning(
                    f"[BRAIN_ONBOARDING] Aucun document onboarding trouv√© pour path={onboarding_path}"
                )
                self.onboarding_data = {}

        except Exception as e:
            logger.error(f"[BRAIN_ONBOARDING] Erreur chargement donn√©es: {e}", exc_info=True)
            self.onboarding_data = {}

        return self.onboarding_data
    
    async def load_job_data(self, job_id: str, force_reload: bool = False) -> Dict[str, Any]:
        """
        Charge les donn√©es de job depuis notifications/{job_id}.
        
        Args:
            job_id: ID du job √† charger
            force_reload: Si True, force le rechargement depuis Firestore m√™me si d√©j√† en cache
        """
        
        if not force_reload and self.job_data is not None and self.job_data.get("job_id") == job_id:
            return self.job_data
        
        try:
            from ...firebase_providers import FirebaseManagement
            
            firebase = FirebaseManagement()
            job_path = f"clients/{self.firebase_user_id}/notifications/{job_id}"
            doc_ref = firebase.db.document(job_path)
            doc = await asyncio.to_thread(doc_ref.get)
            
            if doc.exists:
                doc_data = doc.to_dict() or {}
                # Extraire les champs requis : instructions, job_id, file_id, status
                self.job_data = {
                    "instructions": doc_data.get("instructions", ""),
                    "job_id": doc_data.get("job_id", job_id),
                    "file_id": doc_data.get("file_id", ""),
                    "status": doc_data.get("status", ""),
                    # Conserver les autres champs au cas o√π
                    **{k: v for k, v in doc_data.items() if k not in ["instructions", "job_id", "file_id", "status"]}
                }
                
                # ‚ïê‚ïê‚ïê EXTRACTION DES TRANSACTIONS POUR BANKER_CHAT ‚ïê‚ïê‚ïê
                # Extraire et formater les transactions depuis le champ 'transactions'
                transactions_raw = doc_data.get("transactions", {})
                formatted_transactions = []
                
                if transactions_raw:
                    # Cas 1: transactions est un dictionnaire avec des cl√©s num√©riques (0, 1, 2, ...)
                    if isinstance(transactions_raw, dict):
                        # Trier les cl√©s num√©riquement pour maintenir l'ordre
                        for key in sorted(transactions_raw.keys(), key=lambda x: int(x) if str(x).isdigit() else 999):
                            transaction = transactions_raw[key]
                            if isinstance(transaction, dict):
                                # Extraire uniquement les champs importants
                                formatted_transaction = {
                                    "amount": transaction.get("amount"),
                                    "currency_name": transaction.get("currency_name", ""),
                                    "date": transaction.get("date", ""),
                                    "payment_ref": transaction.get("payment_ref", ""),
                                    "status": transaction.get("status", ""),
                                    "transaction_id": transaction.get("transaction_id", "")
                                }
                                formatted_transactions.append(formatted_transaction)
                    # Cas 2: transactions est une liste
                    elif isinstance(transactions_raw, list):
                        for transaction in transactions_raw:
                            if isinstance(transaction, dict):
                                # Extraire uniquement les champs importants
                                formatted_transaction = {
                                    "amount": transaction.get("amount"),
                                    "currency_name": transaction.get("currency_name", ""),
                                    "date": transaction.get("date", ""),
                                    "payment_ref": transaction.get("payment_ref", ""),
                                    "status": transaction.get("status", ""),
                                    "transaction_id": transaction.get("transaction_id", "")
                                }
                                formatted_transactions.append(formatted_transaction)
                    
                    if formatted_transactions:
                        self.job_data["formatted_transactions"] = formatted_transactions
                        logger.info(
                            f"[BRAIN_JOB_DATA] {len(formatted_transactions)} transactions format√©es "
                            f"pour job_id={job_id}"
                        )
                
                logger.info(
                    f"[BRAIN_JOB_DATA] Donn√©es job charg√©es pour job_id={job_id} "
                    f"(instructions={bool(self.job_data.get('instructions'))}, "
                    f"file_id={self.job_data.get('file_id')}, "
                    f"status={self.job_data.get('status')}, "
                    f"transactions={len(self.job_data.get('formatted_transactions', []))})"
                )
            else:
                # C'est normal si le document n'existe pas encore (job pas encore lanc√©)
                # On initialise avec des valeurs par d√©faut
                self.job_data = {
                    "instructions": "",
                    "job_id": job_id,
                    "file_id": "",
                    "status": "pending"
                }
                logger.debug(
                    f"[BRAIN_JOB_DATA] Document job non trouv√© pour path={job_path} "
                    f"(job_id={job_id}) - Initialisation avec valeurs par d√©faut. "
                    f"C'est normal si le job n'a pas encore √©t√© lanc√©."
                )
        
        except Exception as e:
            logger.error(f"[BRAIN_JOB_DATA] Erreur chargement donn√©es: {e}", exc_info=True)
            self.job_data = {
                "instructions": "",
                "job_id": job_id,
                "file_id": "",
                "status": ""
            }
        
        return self.job_data
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTHODES SPT (synchrones)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _spt_read_firebase(self, collection_path: str, document_id: str) -> Dict:
        """SPT: Lecture Firebase"""
        try:
            from ...firebase_client import get_firestore
            db = get_firestore()
            doc = db.collection(f"{self.collection_name}/{collection_path}").document(document_id).get()
            
            if doc.exists:
                return {
                    'type': 'success',
                    'data': doc.to_dict(),
                    'document_id': document_id
                }
            else:
                return {
                    'type': 'not_found',
                    'message': f"Document {document_id} non trouv√©"
                }
        except Exception as e:
            logger.error(f"Erreur lecture Firebase: {e}")
            return {'type': 'error', 'message': str(e)}
    
    def _spt_search_chromadb(self, query: str, n_results: int = 5) -> Dict:
        """SPT: Recherche ChromaDB"""
        try:
            from ...chroma_vector_service import get_chroma_vector_service
            chroma = get_chroma_vector_service()
            
            results = chroma.query_collection(
                user_id=self.firebase_user_id,
                collection_name=self.collection_name,
                query_texts=[query],
                n_results=n_results
            )
            
            return {
                'type': 'success',
                'results': results,
                'count': len(results.get('documents', [[]])[0]) if results else 0
            }
        except Exception as e:
            logger.error(f"Erreur recherche ChromaDB: {e}")
            return {'type': 'error', 'message': str(e)}
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTHODES LPT (asynchrones via HTTP)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _lpt_file_manager(self, thread_key: str, action: str, params: Dict, task_title: str) -> Dict:
        """
        LPT: Appel √† l'Agent File Manager (HTTP)
        
        Cette m√©thode d√©clenche une t√¢che asynchrone et retourne imm√©diatement.
        Le r√©sultat arrivera plus tard via callback.
        """
        try:
            from .task_tracker import TaskTracker
            tracker = TaskTracker(self.firebase_user_id, self.collection_name)
            
            # Cr√©er une t√¢che LPT
            task_id = tracker.create_lpt_task(
                thread_key=thread_key,
                agent_type="file_manager",
                action=action,
                params=params,
                task_title=task_title
            )
            
            # Enregistrer la t√¢che active
            if thread_key not in self.active_lpt_tasks:
                self.active_lpt_tasks[thread_key] = []
            self.active_lpt_tasks[thread_key].append(task_id)
            
            logger.info(f"T√¢che LPT cr√©√©e: {task_id} pour File Manager")
            
            return {
                'type': 'lpt_started',
                'task_id': task_id,
                'agent': 'file_manager',
                'estimated_duration': '2-3 minutes',
                'message': f"‚úÖ T√¢che '{task_title}' envoy√©e √† l'Agent File Manager. Traitement en cours..."
            }
            
        except Exception as e:
            logger.error(f"Erreur d√©marrage LPT File Manager: {e}")
            return {'type': 'error', 'message': str(e)}
    
    def _lpt_accounting(self, thread_key: str, action: str, params: Dict, task_title: str) -> Dict:
        """
        LPT: Appel √† l'Agent Comptable (HTTP)
        
        Cette m√©thode d√©clenche une t√¢che asynchrone et retourne imm√©diatement.
        Le r√©sultat arrivera plus tard via callback.
        """
        try:
            from .task_tracker import TaskTracker
            tracker = TaskTracker(self.firebase_user_id, self.collection_name)
            
            # Cr√©er une t√¢che LPT
            task_id = tracker.create_lpt_task(
                thread_key=thread_key,
                agent_type="accounting",
                action=action,
                params=params,
                task_title=task_title
            )
            
            # Enregistrer la t√¢che active
            if thread_key not in self.active_lpt_tasks:
                self.active_lpt_tasks[thread_key] = []
            self.active_lpt_tasks[thread_key].append(task_id)
            
            logger.info(f"T√¢che LPT cr√©√©e: {task_id} pour Accounting")
            
            return {
                'type': 'lpt_started',
                'task_id': task_id,
                'agent': 'accounting',
                'estimated_duration': '5-10 minutes',
                'message': f"‚úÖ T√¢che '{task_title}' envoy√©e √† l'Agent Comptable. Traitement en cours..."
            }
            
        except Exception as e:
            logger.error(f"Erreur d√©marrage LPT Accounting: {e}")
            return {'type': 'error', 'message': str(e)}
    
    async def _handle_terminate_task(
        self, 
        reason: str, 
        conclusion: str, 
        **kwargs
    ) -> Dict:
        """
        Handler pour l'outil TERMINATE_TASK.
        
        Cette m√©thode est appel√©e automatiquement par le workflow pour g√©n√©rer
        un r√©sultat d'outil (tool_result) qui sera ajout√© au chat_history.
        
        ‚ö†Ô∏è VALIDATION : En mode execution (t√¢che planifi√©e), v√©rifie que toutes
        les √©tapes de la checklist sont "completed" avant d'autoriser la terminaison.
        
        Args:
            reason: Raison de la terminaison
            conclusion: Rapport final complet
            **kwargs: Param√®tres additionnels ignor√©s
            
        Returns:
            Dict avec le r√©sultat de la terminaison (succ√®s ou erreur avec d√©tails)
        """
        logger.info(f"[TERMINATE_TASK] üèÅ Terminaison demand√©e - raison: {reason}")
        
        # ‚≠ê VALIDATION : V√©rifier que toutes les √©tapes sont "completed" en mode execution
        from ..tools.terminate_task_validator import validate_terminate_task
        
        is_valid, validation_result = validate_terminate_task(
            brain=self,
            reason=reason,
            conclusion=conclusion
        )
        
        if not is_valid:
            # ‚ùå Validation √©chou√©e ‚Üí retourner l'erreur d√©taill√©e
            logger.warning(
                f"[TERMINATE_TASK] ‚ùå Terminaison refus√©e: "
                f"{len(validation_result.get('incomplete_steps', []))} √©tapes incompl√®tes"
            )
            return validation_result
        
        # ‚úÖ Validation OK ‚Üí terminaison autoris√©e
        logger.info("[TERMINATE_TASK] ‚úÖ Validation OK, terminaison autoris√©e")
        return {
            "success": True,
            "reason": reason,
            "conclusion": conclusion,
            "status": "terminated",
            "message": "Task terminated successfully",
            "validation": validation_result
        }
    
    def _get_current_datetime(self, timezone: str = None, format: str = "BOTH") -> Dict:
        """
        Obtient la date et l'heure actuelles dans un fuseau horaire sp√©cifique.
        
        Args:
            timezone: Timezone IANA optionnelle (ex: 'Europe/Paris').
                     Si None, utilise la timezone configur√©e pour la soci√©t√©.
            format: Format de sortie ("ISO", "READABLE", ou "BOTH")
            
        Returns:
            Dict contenant la date/heure dans le format demand√©
        """
        from datetime import datetime
        import pytz
        
        try:
            # Utiliser la timezone du mandat si non fournie.
            # ‚ö†Ô∏è D√©fensif: neutraliser anciennes valeurs de cache ("no timezone found") et values None/""
            if timezone == "no timezone found":
                timezone = None

            if not timezone:
                tz_from_context = None
                if self.user_context:
                    tz_from_context = self.user_context.get("timezone")
                    if tz_from_context == "no timezone found":
                        tz_from_context = None
                timezone = tz_from_context or "UTC"
            
            logger.info(f"[GET_CURRENT_DATETIME] Timezone: {timezone}, Format: {format}")
            
            # Obtenir l'heure actuelle dans la timezone
            try:
                tz = pytz.timezone(timezone)
            except Exception:
                logger.warning(
                    f"[GET_CURRENT_DATETIME] ‚ö†Ô∏è Timezone invalide '{timezone}', repli sur UTC"
                )
                timezone = "UTC"
                tz = pytz.timezone("UTC")
            now = datetime.now(tz)
            
            result = {
                "success": True,
                "timezone": timezone
            }
            
            # Format ISO
            if format in ["ISO", "BOTH"]:
                result["iso_format"] = now.isoformat()
                result["date_iso"] = now.date().isoformat()
                result["time_iso"] = now.time().isoformat()
            
            # Format lisible
            if format in ["READABLE", "BOTH"]:
                # Noms des jours et mois en fran√ßais
                days_fr = ["lundi", "mardi", "mercredi", "jeudi", "vendredi", "samedi", "dimanche"]
                months_fr = ["janvier", "f√©vrier", "mars", "avril", "mai", "juin", 
                            "juillet", "ao√ªt", "septembre", "octobre", "novembre", "d√©cembre"]
                
                day_name = days_fr[now.weekday()]
                month_name = months_fr[now.month - 1]
                
                result["readable_date"] = f"{day_name} {now.day} {month_name} {now.year}"
                result["readable_time"] = now.strftime("%H:%M:%S")
                result["readable_full"] = f"{day_name} {now.day} {month_name} {now.year} √† {now.strftime('%H:%M:%S')}"
            
            # Informations additionnelles utiles
            result["day_of_week"] = now.weekday() + 1  # 1 = lundi, 7 = dimanche
            result["day_of_month"] = now.day
            result["month"] = now.month
            result["year"] = now.year
            result["hour"] = now.hour
            result["minute"] = now.minute
            
            logger.info(f"[GET_CURRENT_DATETIME] ‚úÖ R√©sultat: {result.get('readable_full', result.get('iso_format'))}")
            
            return result
            
        except Exception as e:
            logger.error(f"[GET_CURRENT_DATETIME] ‚ùå Erreur: {e}", exc_info=True)
            return {
                "success": False,
                "error": f"Erreur lors de l'obtention de la date/heure: {str(e)}",
                "timezone": timezone or "UTC"
            }
    
    def has_active_lpt_tasks(self, thread_key: str) -> bool:
        """V√©rifie si des t√¢ches LPT sont en cours pour ce thread"""
        return thread_key in self.active_lpt_tasks and len(self.active_lpt_tasks[thread_key]) > 0
    
    def get_active_lpt_count(self, thread_key: str) -> int:
        """Retourne le nombre de t√¢ches LPT actives"""
        return len(self.active_lpt_tasks.get(thread_key, []))
    
    def reset_context_with_summary(self, summary: str) -> int:
        """
        R√©initialise le contexte avec un r√©sum√© int√©gr√© au system prompt.
        
        Cette m√©thode :
        1. Ajoute le r√©sum√© au system prompt de base
        2. Vide l'historique du chat
        3. Calcule et retourne le nombre de tokens du nouveau contexte
        
        Args:
            summary: R√©sum√© de la conversation √† int√©grer
        
        Returns:
            Nombre de tokens du nouveau contexte (system prompt + r√©sum√©)
        """
        logger.info("[RESET] R√©initialisation du contexte avec r√©sum√©")
        
        # R√©cup√©rer l'instance provider
        provider_instance = self.pinnokio_agent.get_provider_instance(self.default_provider)
        
        # Sauvegarder le system prompt de base (si pas d√©j√† sauvegard√©)
        if not hasattr(self, '_base_system_prompt'):
            self._base_system_prompt = provider_instance.system_prompt if hasattr(provider_instance, 'system_prompt') else ""
        
        # Cr√©er le nouveau system prompt avec r√©sum√© int√©gr√©
        new_system_prompt = f"""{self._base_system_prompt}

                ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                üìã CONTEXTE DE LA CONVERSATION PR√âC√âDENTE :

                {summary}

                ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

                Continue la conversation en tenant compte de ce contexte historique.
                """
        
        # Mettre √† jour le system prompt
        if hasattr(provider_instance, 'update_system_prompt'):
            provider_instance.update_system_prompt(new_system_prompt)
        elif hasattr(provider_instance, 'system_prompt'):
            provider_instance.system_prompt = new_system_prompt
        
        # ‚≠ê NOUVEAU: Vider l'historique du brain (isol√© par thread)
        self.clear_chat_history()
        
        # Calculer les tokens du nouveau contexte
        # ‚≠ê get_history_token_count() compte maintenant automatiquement :
        # - L'historique du chat (vide apr√®s clear)
        # - Le system prompt complet (avec r√©sum√© int√©gr√©)
        tokens_after_reset = self.get_history_token_count()
        
        logger.info(
            f"[RESET] Contexte r√©initialis√© - "
            f"Nouveau contexte: {tokens_after_reset:,} tokens "
            f"(system prompt avec r√©sum√© + historique vide)"
        )
        
        return tokens_after_reset
    
    def generate_conversation_summary(self, thread_key: str, total_tokens_used: int) -> str:
        """
        G√©n√®re un r√©sum√© compress√© de la conversation actuelle
        pour r√©initialiser le contexte tout en gardant l'essentiel.
        
        Cette m√©thode est appel√©e quand le budget de tokens est atteint (80K)
        pour compresser l'historique et permettre de continuer la conversation.
        
        Args:
            thread_key: Cl√© du thread de conversation
            total_tokens_used: Nombre total de tokens utilis√©s dans la session
        
        Returns:
            R√©sum√© compress√© de la conversation (max 500 tokens)
        """
        logger.info(f"[SUMMARY] G√©n√©ration r√©sum√© - thread={thread_key}, tokens={total_tokens_used}")
        
        summary_prompt = f"""R√©sume cette conversation en gardant UNIQUEMENT les informations critiques:

                **Instructions de R√©sum√©** :
                1. **Contexte initial**: Quelle √©tait la demande originale de l'utilisateur ?
                2. **Actions effectu√©es**: Quels outils ont √©t√© utilis√©s (SPT/LPT) et pourquoi ?
                3. **R√©sultats cl√©s**: Qu'avons-nous d√©couvert ou accompli ?
                4. **√âtat actuel**: O√π en sommes-nous maintenant ? Que reste-t-il √† faire ?
                5. **T√¢ches LPT en cours**: Y a-t-il des t√¢ches longues en cours d'ex√©cution ?

                **Contraintes** :
                - Maximum 500 tokens
                - Format concis et structur√©
                - Garde uniquement l'essentiel pour continuer efficacement

                Tokens utilis√©s dans cette session: {total_tokens_used:,}
                """
        
        try:
            # Utiliser l'agent pour g√©n√©rer le r√©sum√© (sans outils)
            summary_response = self.pinnokio_agent.process_tool_use(
                content=summary_prompt,
                tools=[],  # Pas d'outils pour le r√©sum√©
                tool_mapping={},
                provider=self.default_provider,
                size=ModelSize.SMALL,  # Mod√®le rapide suffit pour un r√©sum√©
                max_tokens=600,
                raw_output=True
            )
            
            # Extraire le texte du r√©sum√©
            summary_text = self._extract_text_from_summary_response(summary_response)
            
            logger.info(f"[SUMMARY] R√©sum√© g√©n√©r√© - longueur={len(summary_text)} caract√®res")
            
            return summary_text
            
        except Exception as e:
            logger.error(f"[SUMMARY] Erreur g√©n√©ration r√©sum√©: {e}", exc_info=True)
            
            # R√©sum√© de fallback en cas d'erreur
            return f"""R√©sum√© automatique de la session:
            - Tokens utilis√©s: {total_tokens_used:,}
            - Thread: {thread_key}
            - T√¢ches LPT actives: {self.get_active_lpt_count(thread_key)}
            - Budget tokens atteint, contexte r√©initialis√©.
            """
    
    def _extract_text_from_summary_response(self, response: Any) -> str:
        """
        Extrait le texte d'une r√©ponse de r√©sum√©.
        Helper method pour extract le texte peu importe le format de r√©ponse.
        """
        if not response:
            return "Aucun r√©sum√© g√©n√©r√©."
        
        # Si c'est une liste de r√©ponses
        if isinstance(response, list):
            for item in response:
                if isinstance(item, dict):
                    # Chercher text_output
                    if "text_output" in item:
                        text_block = item["text_output"]
                        if isinstance(text_block, dict) and "content" in text_block:
                            return str(text_block["content"])
                        elif isinstance(text_block, str):
                            return text_block
        
        # Si c'est directement un dict
        if isinstance(response, dict):
            if "text_output" in response:
                text_block = response["text_output"]
                if isinstance(text_block, dict) and "content" in text_block:
                    return str(text_block["content"])
                elif isinstance(text_block, str):
                    return text_block
        
        # Fallback: convertir en string
        return str(response)[:1000]  # Limiter √† 1000 chars par s√©curit√©

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GESTION HISTORIQUE CHAT (ISOL√â PAR THREAD)
    # ‚≠ê Multi-Instance Ready: Synchronisation Redis
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def add_user_message(self, content):
        """
        Ajoute un message utilisateur √† l'historique du chat.
        
        ‚≠ê Multi-Instance: Synchronise avec Redis apr√®s ajout.
        
        Args:
            content: Contenu du message utilisateur (str ou list pour tool_results)
        """
        if self.pinnokio_agent:
            self.pinnokio_agent.add_user_message(content, provider=self.default_provider)
            content_len = len(content) if isinstance(content, (str, list)) else 0
            logger.debug(f"[CHAT_HISTORY] Message utilisateur ajout√© via agent (type={type(content).__name__}, len={content_len})")
            
            # ‚≠ê Sync Redis (multi-instance)
            self._sync_history_to_redis()
        else:
            logger.warning(f"[CHAT_HISTORY] Agent non initialis√©, message non ajout√©")
    
    def add_assistant_message(self, content: Any):
        """
        Ajoute un message assistant √† l'historique du chat.
        
        ‚≠ê Multi-Instance: Synchronise avec Redis apr√®s ajout.
        
        Args:
            content: Contenu du message assistant (str, list ou dict)
                     - str: texte simple
                     - list: blocs Anthropic (text, tool_use, etc.)
                     - dict: ancien format (sera pr√©serv√©)
        """
        if self.pinnokio_agent:
            self.pinnokio_agent.add_ai_message(content, provider=self.default_provider)
            content_type = type(content).__name__
            content_len = len(content) if isinstance(content, (str, list)) else 1
            logger.debug(f"[CHAT_HISTORY] Message assistant ajout√© via agent (type={content_type}, len={content_len})")
            
            # ‚≠ê Sync Redis (multi-instance)
            self._sync_history_to_redis()
        else:
            logger.warning(f"[CHAT_HISTORY] Agent non initialis√©, message non ajout√©")
    
    def get_chat_history(self) -> List[Dict[str, Any]]:
        """
        Retourne l'historique complet du chat.
        
        ‚≠ê Multi-Instance: Utilise le cache local (performance).
        Pour cross-instance, utiliser get_chat_history_from_redis().
        
        Returns:
            Liste des messages du chat
        """
        if self.pinnokio_agent:
            return self.pinnokio_agent.chat_history.get(self.default_provider.value, []).copy()
        return []
    
    def get_chat_history_from_redis(self) -> List[Dict[str, Any]]:
        """
        R√©cup√®re l'historique depuis Redis (pour reprise cross-instance).
        
        ‚≠ê Multi-Instance: Lecture directe depuis Redis.
        
        Returns:
            Liste des messages depuis Redis, ou liste vide
        """
        if not self.active_thread_key:
            logger.warning("[CHAT_HISTORY] Pas de thread_key actif pour lecture Redis")
            return []
        
        return self._chat_history_manager.get_messages(
            self.firebase_user_id,
            self.collection_name,
            self.active_thread_key
        )
    
    def restore_history_from_redis(self) -> bool:
        """
        Restaure l'historique depuis Redis dans l'agent local.
        
        ‚≠ê Multi-Instance: Appel√© au d√©marrage du brain pour reprise.
        
        Returns:
            True si restauration r√©ussie
        """
        if not self.active_thread_key:
            logger.debug("[CHAT_HISTORY] Pas de thread_key actif pour restauration")
            return False
        
        try:
            history = self._chat_history_manager.load_chat_history(
                self.firebase_user_id,
                self.collection_name,
                self.active_thread_key
            )
            
            if history and self.pinnokio_agent:
                messages = history.get("messages", [])
                system_prompt = history.get("system_prompt", "")
                
                # Restaurer le system prompt
                if system_prompt:
                    self.pinnokio_agent.update_system_prompt(system_prompt)
                
                # Restaurer les messages
                # Note: On remplace directement le chat_history
                self.pinnokio_agent.chat_history[self.default_provider.value] = messages
                
                logger.info(
                    f"[CHAT_HISTORY] ‚úÖ Historique restaur√© depuis Redis: "
                    f"{len(messages)} messages, thread={self.active_thread_key}"
                )
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"[CHAT_HISTORY] ‚ö†Ô∏è Erreur restauration Redis: {e}")
            return False
    
    def clear_chat_history(self):
        """
        Vide l'historique du chat pour ce thread.
        
        ‚≠ê Multi-Instance: Synchronise avec Redis apr√®s vidage.
        """
        if self.pinnokio_agent:
            current_history = self.get_chat_history()
            message_count = len(current_history)
            self.pinnokio_agent.clear_chat_history()
            
            # ‚≠ê Sync Redis (vider aussi dans Redis)
            if self._redis_sync_enabled and self.active_thread_key:
                self._chat_history_manager.clear_messages(
                    self.firebase_user_id,
                    self.collection_name,
                    self.active_thread_key,
                    keep_system_prompt=True
                )
            
            logger.info(f"[CHAT_HISTORY] Historique vid√© via agent ({message_count} messages supprim√©s)")
        else:
            logger.warning(f"[CHAT_HISTORY] Agent non initialis√©, rien √† vider")
    
    def _sync_history_to_redis(self):
        """
        Synchronise l'historique local vers Redis (non-bloquant).
        
        ‚≠ê Multi-Instance: Appel√© apr√®s chaque modification pour durabilit√©.
        """
        if not self._redis_sync_enabled:
            return
        
        if not self.active_thread_key:
            logger.debug("[CHAT_HISTORY] Pas de thread_key actif pour sync Redis")
            return
        
        try:
            messages = self.get_chat_history()
            system_prompt = ""
            
            # R√©cup√©rer le system prompt si disponible
            if self.pinnokio_agent:
                provider_instance = self.pinnokio_agent.get_provider_instance(self.default_provider)
                if provider_instance and hasattr(provider_instance, 'system_prompt'):
                    system_prompt = provider_instance.system_prompt or ""
            
            self._chat_history_manager.save_chat_history(
                user_id=self.firebase_user_id,
                company_id=self.collection_name,
                thread_key=self.active_thread_key,
                messages=messages,
                system_prompt=system_prompt,
                metadata={
                    "chat_mode": self.current_chat_mode,
                    "provider": self.default_provider.value
                },
                status="active"
            )
            
            logger.debug(
                f"[CHAT_HISTORY] üíæ Sync Redis: {len(messages)} messages, "
                f"thread={self.active_thread_key}"
            )
            
        except Exception as e:
            logger.warning(f"[CHAT_HISTORY] ‚ö†Ô∏è Erreur sync Redis: {e}")
    
    async def load_user_context(self, mode: str = "UI") -> Dict[str, Any]:
        """
        Charge le contexte utilisateur (m√©tadonn√©es soci√©t√©) dans le brain SESSION.
        
        ‚≠ê NOUVEAU : Support dual-mode (UI/BACKEND)
        
        Mode UI (utilisateur connect√©) :
        1. Tenter cache Redis (TTL 1h)
        2. Si CACHE MISS ‚Üí Fallback Firebase
        3. Mettre en cache pour prochains appels
        
        Mode BACKEND (utilisateur d√©connect√©) :
        1. Acc√®s direct Firebase (source de v√©rit√©)
        2. Pas de cache
        
        Ce contexte contient toutes les m√©tadonn√©es importantes :
        - mandate_path, client_uuid, company_name
        - dms_system, drive_space_parent_id
        - communication_mode, log_communication_mode
        - bank_erp (odoo_url, odoo_db, etc.)
        
        ‚≠ê IMPORTANT : Appel√© lors de initialize_agent() (pas besoin de thread_key)
        
        Args:
            mode: "UI" (cache prioritaire) ou "BACKEND" (Firebase direct)
        
        Returns:
            Dict contenant le contexte utilisateur
        """
        try:
            logger.info(f"[BRAIN_CONTEXT] Chargement contexte utilisateur - Mode: {mode}")
            
            import json
            from ...redis_client import get_redis
            
            context = None
            
            # ‚ïê‚ïê‚ïê MODE UI : Cache Redis ‚Üí Fallback Firebase ‚ïê‚ïê‚ïê
            if mode == "UI":
                try:
                    redis_client = get_redis()
                    cache_key = f"context:{self.firebase_user_id}:{self.collection_name}"
                    
                    logger.info(f"[BRAIN_CONTEXT] üîç DEBUG - Tentative lecture cache: {cache_key}")
                    cached_data = redis_client.get(cache_key)
                    logger.info(f"[BRAIN_CONTEXT] üîç DEBUG - cached_data type: {type(cached_data)}, value: {cached_data[:100] if cached_data else None}")
                    
                    if cached_data:
                        context = json.loads(cached_data)
                        logger.info(f"[BRAIN_CONTEXT] ‚úÖ CACHE HIT: {cache_key}")
                    else:
                        logger.info(f"[BRAIN_CONTEXT] ‚ùå CACHE MISS: {cache_key} - Fallback Firebase")
                
                except Exception as e:
                    logger.warning(f"[BRAIN_CONTEXT] Erreur acc√®s cache: {e} - Fallback Firebase")
            
            # ‚ïê‚ïê‚ïê Si pas de cache OU mode BACKEND : Firebase direct ‚ïê‚ïê‚ïê
            if context is None:
                logger.info(f"[BRAIN_CONTEXT] R√©cup√©ration depuis Firebase...")
                
                from ..tools.lpt_client import LPTClient
                
                lpt_client = LPTClient()
                
                # R√©cup√©rer depuis Firebase (sans cache)
                context = await lpt_client._reconstruct_full_company_profile(
                    self.firebase_user_id,
                    self.collection_name
                )
                
                # Si mode UI, mettre en cache
                if mode == "UI" and context:
                    try:
                        redis_client = get_redis()
                        cache_key = f"context:{self.firebase_user_id}:{self.collection_name}"
                        
                        # Ajouter timestamp
                        context["cached_at"] = datetime.now().isoformat()
                        
                        redis_client.setex(
                            cache_key,
                            3600,  # TTL 1 heure
                            json.dumps(context)
                        )
                        
                        logger.info(f"[BRAIN_CONTEXT] ‚úÖ Contexte mis en cache: {cache_key}")
                    
                    except Exception as e:
                        logger.warning(f"[BRAIN_CONTEXT] Erreur mise en cache: {e}")
            
            # ‚ïê‚ïê‚ïê Stocker dans le brain ‚ïê‚ïê‚ïê
            if context:
                self.user_context = context
                
                logger.info(
                    f"[BRAIN_CONTEXT] ‚úÖ Contexte charg√©: mandate_path={context.get('mandate_path')}, "
                    f"dms_system={context.get('dms_system')}, "
                    f"client_uuid={context.get('client_uuid')}, "
                    f"mode={mode}"
                )
                
                # üîç DEBUG : Afficher les champs critiques pour Router et Bank
                logger.info(
                    f"[BRAIN_CONTEXT] üîç DEBUG - Champs Drive: "
                    f"drive_space_parent_id={context.get('drive_space_parent_id')}, "
                    f"input_drive_doc_id={context.get('input_drive_doc_id')}"
                )
                logger.info(
                    f"[BRAIN_CONTEXT] üîç DEBUG - Champs ERP Bank: "
                    f"mandate_bank_erp={context.get('mandate_bank_erp')}, "
                    f"erp_odoo_url={context.get('erp_odoo_url')}, "
                    f"erp_erp_type={context.get('erp_erp_type')}"
                )
                logger.info(
                    f"[BRAIN_CONTEXT] üîç DEBUG - Toutes les cl√©s: {list(context.keys())}"
                )
                
                return context
            
            else:
                raise ValueError("Contexte vide depuis Firebase")
        
        except Exception as e:
            logger.error(f"[BRAIN_CONTEXT] ‚ùå Erreur chargement contexte: {e}", exc_info=True)
            
            # Retourner un contexte minimal pour ne pas bloquer
            self.user_context = {
                "mandate_path": self.collection_name,
                "dms_system": "google_drive",
                "communication_mode": "webhook",
                "log_communication_mode": "firebase",
                "user_language": "fr",
                "mode": mode
            }
            
            return self.user_context
    
    def get_user_context(self) -> Dict[str, Any]:
        """
        R√©cup√®re le contexte utilisateur stock√© dans le brain.
        
        Returns:
            Dict contenant le contexte utilisateur, ou dict vide si non charg√©
        """
        if self.user_context is None:
            logger.warning(
                f"[BRAIN_CONTEXT] ‚ö†Ô∏è Contexte non charg√©. "
                f"Appelez load_user_context() apr√®s cr√©ation du brain."
            )
            return {}
        
        return self.user_context
    
    def set_active_thread(self, thread_key: str):
        """
        D√©finit le thread actif pour les workflows d'approbation.
        
        Cette m√©thode doit √™tre appel√©e au d√©but du traitement d'un message
        pour que les outils sachent sur quel thread envoyer les cartes d'approbation.
        
        Args:
            thread_key: Cl√© du thread de conversation actif
        """
        self.active_thread_key = thread_key
        logger.info(f"[BRAIN] Thread actif d√©fini: {thread_key}")
    
    def get_history_token_count(self) -> int:
        """
        Estime le nombre de tokens dans le contexte actuel complet.
        
        ‚≠ê PROXY vers BaseAIAgent.get_total_context_tokens()
        
        Calcule automatiquement :
        - Chat history (messages utilisateur + assistant + tool_results)
        - System prompt (avec r√©sum√©s √©ventuels)
        
        Returns:
            Nombre approximatif de tokens dans le contexte complet
        """
        if not self.pinnokio_agent:
            return 0
        
        # D√©l√©guer le calcul √† BaseAIAgent (√©vite duplication de code)
        return self.pinnokio_agent.get_total_context_tokens(self.default_provider)

