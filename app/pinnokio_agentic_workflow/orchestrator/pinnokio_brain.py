"""
Pinnokio Brain - Agent Cerveau Principal
Agent orchestrateur intelligent avec capacitÃ© de raisonnement pour gÃ©rer SPT et LPT
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone
import json

from ...llm.klk_agents import BaseAIAgent, ModelProvider, ModelSize, NEW_Anthropic_Agent, NEW_OpenAiAgent
from .agent_modes import get_agent_mode_config

logger = logging.getLogger("pinnokio.brain")


class PinnokioBrain:
    """
    Agent cerveau principal (Pinnokio) avec capacitÃ© d'orchestration SPT/LPT
    
    ResponsabilitÃ©s:
    - Comprendre les requÃªtes utilisateur complexes
    - Ã‰laborer des plans d'action structurÃ©s
    - Orchestrer l'exÃ©cution SPT (synchrone) et LPT (asynchrone)
    - Maintenir le contexte pendant l'exÃ©cution
    - Communiquer avec l'utilisateur pendant les LPT
    """
    
    def __init__(self, 
                 collection_name: str,
                 firebase_user_id: str,
                 dms_system: str = "google_drive",
                 dms_mode: str = "prod"):
        """
        Initialise l'agent cerveau Pinnokio
        
        Args:
            collection_name: Nom de la collection (sociÃ©tÃ©)
            firebase_user_id: ID utilisateur Firebase
            dms_system: SystÃ¨me DMS (google_drive, etc.)
            dms_mode: Mode DMS (prod, test)
        """
        self.collection_name = collection_name
        self.firebase_user_id = firebase_user_id
        self.dms_system = dms_system
        self.dms_mode = dms_mode
        
        # â­ NOUVELLE ARCHITECTURE: Agent principal crÃ©Ã© via initialize_agents()
        # Chaque brain a son propre agent principal isolÃ©
        self.pinnokio_agent: Optional[BaseAIAgent] = None
        
        # Configuration du provider (modÃ¨le de raisonnement)
        self.default_provider = ModelProvider.OPENAI
        self.default_size = ModelSize.MEDIUM  # Kimi K2 pour raisonnement + streaming + tools
        
        # â­ NOUVELLE ARCHITECTURE: L'historique est gÃ©rÃ© par self.pinnokio_agent
        # Plus de duplication d'historique au niveau du brain
        
        # Ã‰tat de l'orchestration
        self.active_plans: Dict[str, Dict] = {}  # {thread_key: plan_data}
        self.active_lpt_tasks: Dict[str, List[str]] = {}  # {thread_key: [task_ids]}
        
        # â­ NOUVEAU: Contexte utilisateur (mÃ©tadonnÃ©es sociÃ©tÃ©)
        # Contient: mandate_path, dms_system, communication_mode, etc.
        # Accessible par tous les outils (SPT et LPT)
        self.user_context: Optional[Dict[str, Any]] = None
        
        # â­ NOUVEAU: Agent SPT ContextManager (sera initialisÃ© dans initialize_spt_agents)
        # Chaque agent SPT a son propre BaseAIAgent et chat_history isolÃ©
        self.context_manager = None
        
        # â­ NOUVEAU: Jobs data et mÃ©triques (assignÃ©s depuis LLMSession)
        # Ces donnÃ©es sont chargÃ©es Ã  l'initialisation de la session pour allÃ©ger le contexte
        self.jobs_data: Dict[str, Any] = {}  # DonnÃ©es complÃ¨tes des jobs (pour GET_JOBS)
        self.jobs_metrics: Dict[str, Any] = {}  # MÃ©triques pour le system prompt
        
        # â­ NOUVEAU: Thread actif (pour workflows d'approbation avec cartes)
        self.active_thread_key: Optional[str] = None
        
        # â­ NOUVEAU: Proposition de contexte en attente (pour UPDATE_CONTEXT â†’ PUBLISH_CONTEXT)
        self.context_proposal: Optional[Dict[str, Any]] = None

        # â­ NOUVEAU: DonnÃ©es de la tÃ¢che en cours d'exÃ©cution (si mode task_execution)
        self.active_task_data: Optional[Dict[str, Any]] = None

        # â­ Mode de chat courant (utilisÃ© pour la config prompt/outils)
        self.current_chat_mode: str = "general_chat"

        # â­ DonnÃ©es spÃ©cifiques onboarding (chargÃ©es Ã  la demande, uniquement pour onboarding_chat)
        self.onboarding_data: Optional[Dict[str, Any]] = None
        
        # â­ DonnÃ©es spÃ©cifiques job (chargÃ©es Ã  la demande, pour router_chat, banker_chat, etc.)
        self.job_data: Optional[Dict[str, Any]] = None

        logger.info(f"PinnokioBrain initialisÃ© pour user={firebase_user_id}, collection={collection_name}")
    
    async def initialize_agents(self):
        """
        CrÃ©e les agents du brain (principal + outils SPT).
        
        â­ NOUVELLE ARCHITECTURE : Chaque brain a ses propres agents isolÃ©s
        
        CrÃ©ation:
        1. Agent principal (pinnokio_agent) - BaseAIAgent pour interaction utilisateur
        2. Agents SPT (context_manager, etc.) - Pour outils rapides
        
        Cette mÃ©thode doit Ãªtre appelÃ©e immÃ©diatement aprÃ¨s la crÃ©ation du brain,
        avant d'injecter les donnÃ©es de session et d'initialiser le system prompt.
        """
        try:
            logger.info(f"[BRAIN] ğŸ¤– CrÃ©ation agents pour brain (user={self.firebase_user_id}, collection={self.collection_name})")
            
            # â•â•â• 1. CrÃ©er l'agent principal â•â•â•
            self.pinnokio_agent = BaseAIAgent(
                collection_name=self.collection_name,
                dms_system=self.dms_system,
                dms_mode=self.dms_mode,
                firebase_user_id=self.firebase_user_id
            )
            
            # Configurer le provider et la taille par dÃ©faut
            self.pinnokio_agent.default_provider = self.default_provider
            self.pinnokio_agent.default_model_size = self.default_size
            
            # â•â•â• 2. CrÃ©er et enregistrer l'instance du provider â•â•â•
            # CrÃ©er l'instance OpenAI (sans arguments)
            openai_instance = NEW_OpenAiAgent()
            
            # Enregistrer le provider dans BaseAIAgent
            # BaseAIAgent a dÃ©jÃ  collection_name, dms_system, dms_mode, firebase_user_id
            self.pinnokio_agent.register_provider(
                provider=self.default_provider,
                instance=openai_instance,
                default_model_size=self.default_size
            )
            
            logger.info(f"[BRAIN] âœ… Agent principal crÃ©Ã© (provider={self.default_provider.value}, size={self.default_size.value}, model=Kimi K2)")
            
            # â•â•â• 3. CrÃ©er les agents SPT â•â•â•
            
            logger.info(f"[BRAIN] âœ… Agents SPT crÃ©Ã©s")
            
            logger.info(f"[BRAIN] ğŸ‰ Tous les agents crÃ©Ã©s avec succÃ¨s")
            
        except Exception as e:
            logger.error(f"[BRAIN] âŒ Erreur crÃ©ation agents: {e}", exc_info=True)
            raise
    
    def initialize_system_prompt(self, chat_mode: str = "general_chat", jobs_metrics: Dict = None):
        """Initialise le system prompt en fonction du mode dÃ©clarÃ©."""

        config = get_agent_mode_config(chat_mode)

        if not self.pinnokio_agent:
            raise RuntimeError("Pinnokio agent non initialisÃ© avant initialize_system_prompt")

        prompt = config.prompt_builder(self, jobs_metrics, chat_mode)
        self.pinnokio_agent.update_system_prompt(prompt)
        self.current_chat_mode = config.name

        logger.info(
            f"System prompt initialisÃ© pour mode={chat_mode} (config={config.name})"
        )
    
    
    def create_workflow_tools(
        self,
        thread_key: str,
        session=None,
        chat_mode: str = "general_chat",
    ) -> Tuple[List[Dict], Dict]:
        """Retourne l'ensemble d'outils configurÃ© pour le mode de chat."""

        config = get_agent_mode_config(chat_mode)
        tool_set, tool_mapping = config.tool_builder(self, thread_key, session, chat_mode)

        logger.info(
            f"Outils initialisÃ©s pour mode={chat_mode} (config={config.name}) : {len(tool_set)} outils"
        )
        return tool_set, tool_mapping


    def _build_general_chat_tools(self, thread_key: str, session=None) -> Tuple[List[Dict], Dict]:
        """Construit l'ensemble d'outils standard (mode gÃ©nÃ©ral)."""
        from ..tools.spt_tools import SPTTools
        from ..tools.lpt_client import LPTClient
        
        
        # CrÃ©er les outils SPT
        # â­ IMPORTANT : Passer le brain pour accÃ¨s au contexte utilisateur
        spt_tools = SPTTools(
            firebase_user_id=self.firebase_user_id,
            collection_name=self.collection_name,
            brain=self
        )
        spt_tools_list = spt_tools.get_tools_definitions()
        spt_tools_mapping = spt_tools.get_tools_mapping()

        # âš ï¸ SPT_CONTEXT_MANAGER DÃ‰SACTIVÃ‰ TEMPORAIREMENT
        # Les outils de contexte sont maintenant intÃ©grÃ©s directement dans l'agent principal
        # via ContextTools (job_tools.py) pour un accÃ¨s plus rapide et direct.
        # Le code SPT est conservÃ© pour usage futur avec d'autres agents SPT.
        #
        # from ..tools.spt_context_manager import create_spt_context_manager_wrapper
        # tool_def, handler = create_spt_context_manager_wrapper(self)
        # spt_tools_list.append(tool_def)
        # spt_tools_mapping["SPT_CONTEXT_MANAGER"] = handler
        
        # CrÃ©er les outils LPT avec session pour cache
        lpt_client = LPTClient()
        lpt_tools_list, lpt_tools_mapping = lpt_client.get_tools_definitions_and_mapping(
            user_id=self.firebase_user_id,
            company_id=self.collection_name,
            thread_key=thread_key,
            session=session,  # â­ Passer la session pour le cache
            brain=self        # â­ IMPORTANT: Passer le brain pour accÃ¨s au contexte utilisateur
        )
        
        # â•â•â• OUTILS JOBS (3 outils sÃ©parÃ©s par dÃ©partement) â•â•â•
        # CrÃ©er les 3 outils jobs avec leurs handlers
        from ..tools.job_tools import APBookkeeperJobTools, RouterJobTools, BankJobTools, ContextTools
        
        # ğŸ” LOGS DE DIAGNOSTIC - VÃ©rifier jobs_data avant crÃ©ation outils
        logger.info(f"[BRAIN] ğŸ” DIAGNOSTIC self.jobs_data avant crÃ©ation outils - "
                   f"ClÃ©s: {list(self.jobs_data.keys()) if self.jobs_data else 'None'}")
        if self.jobs_data and 'ROUTER' in self.jobs_data:
            router_unprocessed = self.jobs_data['ROUTER'].get('unprocessed', [])
            logger.info(f"[BRAIN] ğŸ” DIAGNOSTIC self.jobs_data['ROUTER']['unprocessed'] - "
                       f"Longueur: {len(router_unprocessed) if isinstance(router_unprocessed, list) else 'N/A'}")
        else:
            logger.warning(f"[BRAIN] âš ï¸ DIAGNOSTIC - Pas de donnÃ©es ROUTER dans self.jobs_data !")
        
        # 1. APBookkeeper Jobs
        apbookeeper_tools = APBookkeeperJobTools(jobs_data=self.jobs_data)
        get_apbookeeper_jobs_def = apbookeeper_tools.get_tool_definition()
        
        async def handle_get_apbookeeper_jobs(**kwargs):
            return await apbookeeper_tools.search(**kwargs)
        
        # 2. Router Jobs
        router_tools = RouterJobTools(jobs_data=self.jobs_data)
        get_router_jobs_def = router_tools.get_tool_definition()
        
        async def handle_get_router_jobs(**kwargs):
            return await router_tools.search(**kwargs)
        
        # 3. Bank Transactions
        bank_tools = BankJobTools(jobs_data=self.jobs_data)
        get_bank_transactions_def = bank_tools.get_tool_definition()
        
        async def handle_get_bank_transactions(**kwargs):
            return await bank_tools.search(**kwargs)
        
        # â•â•â• OUTILS CONTEXT (5 outils d'accÃ¨s et modification des contextes) â•â•â•
        # CrÃ©er les outils de contexte avec leurs handlers
        from ...firebase_providers import FirebaseManagement
        firebase_management = FirebaseManagement()
        
        context_tools = ContextTools(
            firebase_management=firebase_management,
            firebase_user_id=self.firebase_user_id,
            collection_name=self.collection_name,
            brain=self  # âœ… Passer le brain pour accÃ¨s au user_context
        )
        
        # DÃ©finitions des outils de contexte
        router_prompt_def = context_tools.get_router_prompt_definition()
        apbookeeper_context_def = context_tools.get_apbookeeper_context_definition()
        company_context_def = context_tools.get_company_context_definition()
        update_context_def = context_tools.get_update_context_definition()
        
        # Handlers pour les outils de contexte
        async def handle_router_prompt(**kwargs):
            return await context_tools.get_router_prompt(**kwargs)
        
        async def handle_apbookeeper_context(**kwargs):
            return await context_tools.get_apbookeeper_context(**kwargs)
        
        async def handle_company_context(**kwargs):
            return await context_tools.get_company_context(**kwargs)
        
        async def handle_update_context(**kwargs):
            return await context_tools.update_context(**kwargs)

        # â•â•â• OUTIL VISION DOCUMENT DRIVE â•â•â•
        view_drive_document_def = {
            "name": "VIEW_DRIVE_DOCUMENT",
            "description": """ğŸ–¼ï¸ Visionner et analyser un document Google Drive.
            
            Utilisez cet outil pour:
            - Voir le contenu d'un document/image dans Google Drive
            - Analyser des factures, PDF, images
            - RÃ©pondre aux questions sur le contenu visuel d'un document
            
            âš ï¸ **WORKFLOW OBLIGATOIRE** :
            1. **D'ABORD** : RÃ©cupÃ©rer le `drive_file_id` avec :
               - `GET_APBOOKEEPER_JOBS` pour les factures
               - `GET_ROUTER_JOBS` pour les documents Ã  router
               - `GET_BANK_TRANSACTIONS` (pas de file_id ici, ne pas utiliser)
            2. **ENSUITE** : Utiliser ce `drive_file_id` avec VIEW_DRIVE_DOCUMENT
            
            âŒ **NE PAS** inventer ou deviner un file_id !
            âŒ **NE PAS** utiliser un nom de fichier comme file_id !
            
            Exemples corrects:
            1. GET_APBOOKEEPER_JOBS(file_name_contains="38653") â†’ obtenir drive_file_id
            2. VIEW_DRIVE_DOCUMENT(file_id="1A2B3C4D5E...", question="DÃ©tails de la facture")
            """,
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_id": {
                        "type": "string",
                        "description": "ID du fichier Google Drive Ã  visionner (ex: '1A2B3C4D5E')"
                    },
                    "question": {
                        "type": "string",
                        "description": "Question spÃ©cifique sur le document (optionnel). Si non fourni, fait une analyse gÃ©nÃ©rale."
                    }
                },
                "required": ["file_id"]
            }
        }
        
        async def handle_view_drive_document(**kwargs):
            """Handler pour visionner un document Google Drive."""
            try:
                file_id = kwargs.get("file_id")
                question = kwargs.get("question", "DÃ©cris le contenu de ce document en dÃ©tail.")
                
                # âœ… VALIDATION : VÃ©rifier que file_id est fourni et non vide
                if not file_id or not isinstance(file_id, str) or len(file_id.strip()) == 0:
                    error_msg = (
                        "âŒ ParamÃ¨tre 'file_id' manquant ou invalide. "
                        "Pour voir un document, tu DOIS d'abord rÃ©cupÃ©rer son drive_file_id "
                        "en utilisant GET_APBOOKEEPER_JOBS, GET_ROUTER_JOBS ou GET_BANK_TRANSACTIONS."
                    )
                    logger.warning(f"[VIEW_DRIVE_DOCUMENT] {error_msg}")
                    return {
                        "type": "error",
                        "message": error_msg
                    }
                
                # VÃ©rifier que le DMS est disponible
                if not self.pinnokio_agent or not self.pinnokio_agent.dms_system:
                    return {
                        "type": "error",
                        "message": "SystÃ¨me DMS non initialisÃ©. Impossible d'accÃ©der aux documents Drive."
                    }
                
                logger.info(f"[VIEW_DRIVE_DOCUMENT] ğŸ–¼ï¸ Vision du document: file_id={file_id}")
                
                # Utiliser process_vision de BaseAIAgent avec Groq (Llama Scout)
                response = await asyncio.to_thread(
                    self.pinnokio_agent.process_vision,
                    text=question,
                    provider=self.default_provider,  # GROQ
                    size=ModelSize.MEDIUM,  # Llama Scout 17B (vision)
                    file_ids=[file_id],  # ğŸ”¥ CORRECTION: paramÃ¨tre renommÃ© drive_file_ids -> file_ids
                    method='batch',
                    max_tokens=2000,
                    final_resume=True
                )
                
                logger.info(f"[VIEW_DRIVE_DOCUMENT] âœ… Analyse terminÃ©e")
                
                return {
                    "type": "success",
                    "file_id": file_id,
                    "analysis": response if isinstance(response, str) else response.get('text_output', str(response))
                }
                
            except Exception as e:
                logger.error(f"[VIEW_DRIVE_DOCUMENT] âŒ Erreur: {e}", exc_info=True)
                return {
                    "type": "error",
                    "message": f"Erreur lors de la vision du document: {str(e)}"
                }

        # â•â•â• OUTILS TASK (gestion tÃ¢ches planifiÃ©es) â•â•â•
        from ..tools.task_tools import TaskTools

        task_tools = TaskTools(brain=self)
        create_task_def = task_tools.get_tool_definition()

        async def handle_create_task(**kwargs):
            return await task_tools.create_task(**kwargs)

        # â•â•â• OUTILS WORKFLOW CHECKLIST (pour tÃ¢ches planifiÃ©es) â•â•â•
        create_checklist_tool = {
            "name": "CREATE_CHECKLIST",
            "description": """ğŸ“‹ CrÃ©er la workflow checklist pour l'exÃ©cution de la tÃ¢che.

                **Ã€ utiliser uniquement en mode task_execution.**

                CrÃ©ez une liste d'Ã©tapes basÃ©e sur le plan d'action de la mission.
                Chaque Ã©tape doit avoir un ID unique et un nom descriptif.""",
            "input_schema": {
                "type": "object",
                "properties": {
                    "steps": {
                        "type": "array",
                        "description": "Liste des Ã©tapes Ã  rÃ©aliser",
                        "items": {
                            "type": "object",
                            "properties": {
                                "id": {
                                    "type": "string",
                                    "description": "ID unique (ex: 'STEP_1_GET_TRANSACTIONS')"
                                },
                                "name": {
                                    "type": "string",
                                    "description": "Nom descriptif de l'Ã©tape"
                                }
                            },
                            "required": ["id", "name"]
                        }
                    }
                },
                "required": ["steps"]
            }
        }

        update_step_tool = {
            "name": "UPDATE_STEP",
            "description": """ğŸ“Š Mettre Ã  jour l'Ã©tat d'une Ã©tape de la checklist.

                **OBLIGATOIRE lors de l'exÃ©cution de tÃ¢ches planifiÃ©es.**

                Utilisez cet outil pour signaler la progression :
                - Avant de commencer une Ã©tape : status="in_progress"
                - AprÃ¨s avoir terminÃ© une Ã©tape : status="completed"
                - En cas d'erreur : status="error"
                """,
            "input_schema": {
                "type": "object",
                "properties": {
                    "step_id": {
                        "type": "string",
                        "description": "ID de l'Ã©tape"
                    },
                    "status": {
                        "type": "string",
                        "enum": ["in_progress", "completed", "error"],
                        "description": "Nouveau statut"
                    },
                    "message": {
                        "type": "string",
                        "description": "Message descriptif"
                    }
                },
                "required": ["step_id", "status", "message"]
            }
        }

        async def handle_create_checklist(**kwargs):
            """CrÃ©e la workflow checklist."""
            try:
                steps = kwargs["steps"]

                # Valider qu'on est en mode tÃ¢che
                if not self.active_task_data:
                    return {"type": "error", "message": "Non disponible (mode normal)"}

                task_id = self.active_task_data["task_id"]
                execution_id = self.active_task_data["execution_id"]
                mandate_path = self.active_task_data["mandate_path"]
                thread_key = self.active_thread_key

                # PrÃ©parer les Ã©tapes
                formatted_steps = []
                for step in steps:
                    formatted_steps.append({
                        "id": step["id"],
                        "name": step["name"],
                        "status": "pending",
                        "timestamp": "",
                        "message": ""
                    })

                checklist_data = {
                    "total_steps": len(formatted_steps),
                    "current_step": 0,
                    "steps": formatted_steps
                }

                # Sauvegarder dans execution
                from ...firebase_providers import get_firebase_management
                fbm = get_firebase_management()

                fbm.update_task_execution(
                    mandate_path, task_id, execution_id,
                    {"workflow_checklist": checklist_data}
                )

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ENVOI PAR WEBSOCKET + RTDB (comme pour les messages de chat)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                
                from ...ws_hub import hub
                from ...firebase_providers import get_firebase_realtime
                import uuid
                
                checklist_message_id = str(uuid.uuid4())
                timestamp = datetime.now(timezone.utc).isoformat()
                
                # Construire le message de commande
                user_language = self.user_context.get("user_language", "fr") if self.user_context else "fr"
                
                checklist_command = {
                    "action": "SET_WORKFLOW_CHECKLIST",
                    "params": {
                        "checklist": checklist_data,
                        "user_language": user_language
                    }
                }
                
                # 1. Envoi immÃ©diat par WebSocket
                ws_message = {
                    "type": "WORKFLOW_CHECKLIST",
                    "thread_key": thread_key,
                    "timestamp": timestamp,
                    "message_id": checklist_message_id,
                    "content": json.dumps({
                        "message": {
                            "cmmd": checklist_command
                        }
                    })
                }
                
                ws_channel = f"chat:{self.firebase_user_id}:{self.collection_name}:{thread_key}"
                
                await hub.broadcast(self.firebase_user_id, {
                    "type": "WORKFLOW_CHECKLIST",
                    "channel": ws_channel,
                    "payload": ws_message
                })
                
                logger.info(f"[CREATE_CHECKLIST] ğŸ“¡ Checklist envoyÃ©e via WebSocket")
                
                # 2. Sauvegarde dans RTDB pour persistence
                rtdb = get_firebase_realtime()
                rtdb_path = f"{self.collection_name}/chats/{thread_key}/messages/{checklist_message_id}"
                
                message_data = {
                    'content': json.dumps({
                        'message': {
                            'cmmd': checklist_command
                        }
                    }),
                    'sender_id': self.firebase_user_id,
                    'timestamp': timestamp,
                    'message_type': 'CMMD',
                    'read': False,
                    'role': 'assistant'
                }
                
                # Utiliser push() pour gÃ©nÃ©rer une clÃ© unique
                thread_path = f"{self.collection_name}/chats/{thread_key}"
                messages_ref = rtdb.db.child(f'{thread_path}/messages')
                messages_ref.push(message_data)
                
                logger.info(f"[CREATE_CHECKLIST] ğŸ’¾ Checklist sauvegardÃ©e dans RTDB")
                logger.info(f"[CREATE_CHECKLIST] âœ… {len(formatted_steps)} Ã©tapes crÃ©Ã©es")

                return {
                    "type": "success",
                    "message": f"Checklist crÃ©Ã©e : {len(formatted_steps)} Ã©tapes",
                    "total_steps": len(formatted_steps)
                }

            except Exception as e:
                logger.error(f"[CREATE_CHECKLIST] Erreur: {e}", exc_info=True)
                return {"type": "error", "message": str(e)}

        async def handle_update_step(**kwargs):
            """Met Ã  jour une Ã©tape de la checklist."""
            try:
                step_id = kwargs["step_id"]
                status = kwargs["status"]
                message = kwargs["message"]

                # Valider mode tÃ¢che
                if not self.active_task_data:
                    return {"type": "error", "message": "Non disponible (mode normal)"}

                task_id = self.active_task_data["task_id"]
                execution_id = self.active_task_data["execution_id"]
                mandate_path = self.active_task_data["mandate_path"]
                thread_key = self.active_thread_key

                # RÃ©cupÃ©rer l'exÃ©cution
                from ...firebase_providers import get_firebase_management
                fbm = get_firebase_management()

                execution = fbm.get_task_execution(mandate_path, task_id, execution_id)

                if not execution:
                    return {"type": "error", "message": "ExÃ©cution non trouvÃ©e"}

                checklist = execution.get("workflow_checklist", {})
                steps = checklist.get("steps", [])

                # Trouver et mettre Ã  jour l'Ã©tape
                step_found = False
                for step in steps:
                    if step["id"] == step_id:
                        step["status"] = status
                        step["timestamp"] = datetime.now(timezone.utc).isoformat()
                        step["message"] = message
                        step_found = True
                        break

                if not step_found:
                    return {"type": "error", "message": f"Ã‰tape {step_id} non trouvÃ©e"}

                # Sauvegarder
                fbm.update_task_execution(
                    mandate_path, task_id, execution_id,
                    {"workflow_checklist.steps": steps}
                )

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ENVOI PAR WEBSOCKET + RTDB (comme pour les messages de chat)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                
                from ...ws_hub import hub
                from ...firebase_providers import get_firebase_realtime
                import uuid
                
                update_message_id = str(uuid.uuid4())
                timestamp = datetime.now(timezone.utc).isoformat()
                
                # Construire le message de commande
                update_command = {
                    "action": "UPDATE_STEP_STATUS",
                    "params": {
                        "step_id": step_id,
                        "status": status,
                        "timestamp": timestamp,
                        "message": message
                    }
                }
                
                # 1. Envoi immÃ©diat par WebSocket
                ws_message = {
                    "type": "WORKFLOW_STEP_UPDATE",
                    "thread_key": thread_key,
                    "timestamp": timestamp,
                    "message_id": update_message_id,
                    "content": json.dumps({
                        "message": {
                            "cmmd": update_command
                        }
                    })
                }
                
                ws_channel = f"chat:{self.firebase_user_id}:{self.collection_name}:{thread_key}"
                
                await hub.broadcast(self.firebase_user_id, {
                    "type": "WORKFLOW_STEP_UPDATE",
                    "channel": ws_channel,
                    "payload": ws_message
                })
                
                logger.info(f"[UPDATE_STEP] ğŸ“¡ Mise Ã  jour envoyÃ©e via WebSocket")
                
                # 2. Sauvegarde dans RTDB pour persistence
                rtdb = get_firebase_realtime()
                
                message_data = {
                    'content': json.dumps({
                        'message': {
                            'cmmd': update_command
                        }
                    }),
                    'sender_id': self.firebase_user_id,
                    'timestamp': timestamp,
                    'message_type': 'CMMD',
                    'read': False,
                    'role': 'assistant'
                }
                
                # Utiliser push() pour gÃ©nÃ©rer une clÃ© unique
                thread_path = f"{self.collection_name}/chats/{thread_key}"
                messages_ref = rtdb.db.child(f'{thread_path}/messages')
                messages_ref.push(message_data)
                
                logger.info(f"[UPDATE_STEP] ğŸ’¾ Mise Ã  jour sauvegardÃ©e dans RTDB")
                logger.info(f"[UPDATE_STEP] âœ… {step_id} â†’ {status}: {message}")

                return {
                    "type": "success",
                    "message": f"Ã‰tape {step_id} mise Ã  jour : {status}"
                }

            except Exception as e:
                logger.error(f"[UPDATE_STEP] Erreur: {e}", exc_info=True)
                return {"type": "error", "message": str(e)}

        # Outil TERMINATE_TASK
        terminate_tool = {
            "name": "TERMINATE_TASK",
            "description": "ğŸ¯ Terminer la tÃ¢che quand la mission est accomplie. Utilisez cet outil dÃ¨s que vous avez rÃ©solu la requÃªte de l'utilisateur et fourni une rÃ©ponse complÃ¨te.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "reason": {
                        "type": "string",
                        "description": "Raison de la terminaison (ex: 'Mission accomplie', 'Information fournie', 'TÃ¢che longue lancÃ©e')"
                    },
                    "conclusion": {
                        "type": "string",
                        "description": "Votre rÃ©ponse finale COMPLÃˆTE pour l'utilisateur, rÃ©sumant les actions effectuÃ©es et les rÃ©sultats."
                    }
                },
                "required": ["reason", "conclusion"]
            }
        }
        
        # Combiner tous les outils (avec les 3 outils jobs + 4 outils context + VIEW_DRIVE_DOCUMENT + CREATE_TASK + checklist)
        tool_set = [
            get_apbookeeper_jobs_def,
            get_router_jobs_def,
            get_bank_transactions_def,
            router_prompt_def,
            apbookeeper_context_def,
            company_context_def,
            update_context_def,
            view_drive_document_def,  # â­ Outil de vision Drive
            create_task_def,
            create_checklist_tool,
            update_step_tool
        ] + spt_tools_list + lpt_tools_list + [terminate_tool]

        tool_mapping = {
            "GET_APBOOKEEPER_JOBS": handle_get_apbookeeper_jobs,
            "GET_ROUTER_JOBS": handle_get_router_jobs,
            "GET_BANK_TRANSACTIONS": handle_get_bank_transactions,
            "ROUTER_PROMPT": handle_router_prompt,
            "APBOOKEEPER_CONTEXT": handle_apbookeeper_context,
            "COMPANY_CONTEXT": handle_company_context,
            "UPDATE_CONTEXT": handle_update_context,
            "VIEW_DRIVE_DOCUMENT": handle_view_drive_document,  # â­ Handler vision Drive
            "CREATE_TASK": handle_create_task,
            "CREATE_CHECKLIST": handle_create_checklist,
            "UPDATE_STEP": handle_update_step,
            **spt_tools_mapping,
            **lpt_tools_mapping
        }
        
        # Ancienne dÃ©finition en dur - CONSERVÃ‰E CI-DESSOUS POUR COMPATIBILITÃ‰ (Ã  supprimer plus tard)
        old_tool_set = [
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SPT TOOLS - Outils rapides (<30s)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                "name": "READ_FIREBASE_DOCUMENT",
                "description": "ğŸ“„ [SPT] Lire un document Firebase. Temps < 5 secondes.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "collection_path": {
                            "type": "string",
                            "description": "Chemin de la collection (ex: 'invoices', 'clients')"
                        },
                        "document_id": {
                            "type": "string",
                            "description": "ID du document"
                        }
                    },
                    "required": ["collection_path", "document_id"]
                }
            },
            {
                "name": "SEARCH_CHROMADB",
                "description": "ğŸ” [SPT] Recherche vectorielle dans ChromaDB. Temps < 10 secondes.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Question ou requÃªte de recherche"
                        },
                        "n_results": {
                            "type": "integer",
                            "description": "Nombre de rÃ©sultats (dÃ©faut: 5)"
                        }
                    },
                    "required": ["query"]
                }
            },
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # LPT TOOLS - TÃ¢ches longues (>30s)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                "name": "CALL_FILE_MANAGER_AGENT",
                "description": """ğŸ“‚ [LPT] Appeler l'Agent File Manager pour des tÃ¢ches de gestion documentaire complexes.
                
                Utilisez cet outil pour :
                - AccÃ©der Ã  des dossiers dans Drive
                - Rechercher des documents spÃ©cifiques
                - Analyser et extraire des informations de documents
                - Traiter des lots de fichiers
                
                âš ï¸ TÃ¢che asynchrone : Vous serez notifiÃ© quand terminÃ©, restez disponible pour l'utilisateur.""",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "action": {
                            "type": "string",
                            "description": "Action Ã  effectuer (ex: 'search_and_analyze_document')"
                        },
                        "params": {
                            "type": "object",
                            "description": "ParamÃ¨tres de l'action"
                        },
                        "task_title": {
                            "type": "string",
                            "description": "Titre descriptif de la tÃ¢che"
                        }
                    },
                    "required": ["action", "params", "task_title"]
                }
            },
            {
                "name": "CALL_ACCOUNTING_AGENT",
                "description": """ğŸ§¾ [LPT] Appeler l'Agent Comptable pour des tÃ¢ches de saisie et traitement comptable.
                
                Utilisez cet outil pour :
                - Saisir des factures fournisseurs en lot
                - Effectuer des rapprochements bancaires
                - GÃ©nÃ©rer des Ã©critures comptables
                - Traiter des paiements
                
                âš ï¸ TÃ¢che asynchrone : Vous serez notifiÃ© quand terminÃ©, restez disponible pour l'utilisateur.""",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "action": {
                            "type": "string",
                            "description": "Action comptable (ex: 'batch_invoice_entry')"
                        },
                        "params": {
                            "type": "object",
                            "description": "ParamÃ¨tres de l'action"
                        },
                        "task_title": {
                            "type": "string",
                            "description": "Titre descriptif de la tÃ¢che"
                        }
                    },
                    "required": ["action", "params", "task_title"]
                }
            },
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # TOOL DE TERMINAISON
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            {
                "name": "TERMINATE_TASK",
                "description": """ğŸ¯ Terminer la tÃ¢che quand TOUTES les actions sont complÃ©tÃ©es.
                
                Utilisez cet outil UNIQUEMENT quand :
                - Tous les SPT sont exÃ©cutÃ©s
                - Tous les LPT ont reÃ§u leurs callbacks
                - Toutes les informations sont collectÃ©es
                - La mission est accomplie OU impossible Ã  terminer
                
                Ne terminez PAS s'il y a encore des LPT en cours d'exÃ©cution !""",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "reason": {
                            "type": "string",
                            "description": "Raison de la terminaison"
                        },
                        "conclusion": {
                            "type": "string",
                            "description": "Rapport final complet avec rÃ©sumÃ© de toutes les actions"
                        },
                        "tasks_completed": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Liste des IDs de tÃ¢ches complÃ©tÃ©es"
                        }
                    },
                    "required": ["reason", "conclusion"]
                }
            }
        ]
        
        # Tool mapping (les fonctions seront implÃ©mentÃ©es dans TaskExecutor)
        old_tool_map = {
            "READ_FIREBASE_DOCUMENT": lambda **kwargs: self._spt_read_firebase(**kwargs),
            "SEARCH_CHROMADB": lambda **kwargs: self._spt_search_chromadb(**kwargs),
            "CALL_FILE_MANAGER_AGENT": lambda **kwargs: self._lpt_file_manager(thread_key, **kwargs),
            "CALL_ACCOUNTING_AGENT": lambda **kwargs: self._lpt_accounting(thread_key, **kwargs)
        }
        
        # â­ RETOURNER LES NOUVEAUX OUTILS (SPT + LPT simplifiÃ©s)
        logger.info(f"Outils crÃ©Ã©s: {len(tool_set)} outils (SPT: {len(spt_tools_list)}, LPT: {len(lpt_tools_list)})")
        return tool_set, tool_mapping

    async def load_onboarding_data(self) -> Dict[str, Any]:
        """Charge les donnÃ©es d'onboarding spÃ©cifiques Ã  l'utilisateur."""

        if self.onboarding_data is not None:
            return self.onboarding_data

        try:
            from ...firebase_providers import FirebaseManagement

            firebase = FirebaseManagement()
            onboarding_path = f"clients/{self.firebase_user_id}/temp_data/onboarding"
            doc_ref = firebase.db.document(onboarding_path)
            doc = await asyncio.to_thread(doc_ref.get)

            if doc.exists:
                self.onboarding_data = doc.to_dict() or {}
                logger.info(
                    f"[BRAIN_ONBOARDING] DonnÃ©es onboarding chargÃ©es ({list(self.onboarding_data.keys())})"
                )
            else:
                logger.warning(
                    f"[BRAIN_ONBOARDING] Aucun document onboarding trouvÃ© pour path={onboarding_path}"
                )
                self.onboarding_data = {}

        except Exception as e:
            logger.error(f"[BRAIN_ONBOARDING] Erreur chargement donnÃ©es: {e}", exc_info=True)
            self.onboarding_data = {}

        return self.onboarding_data
    
    async def load_job_data(self, job_id: str) -> Dict[str, Any]:
        """Charge les donnÃ©es de job depuis notifications/{job_id}."""
        
        if self.job_data is not None and self.job_data.get("job_id") == job_id:
            return self.job_data
        
        try:
            from ...firebase_providers import FirebaseManagement
            
            firebase = FirebaseManagement()
            job_path = f"clients/{self.firebase_user_id}/notifications/{job_id}"
            doc_ref = firebase.db.document(job_path)
            doc = await asyncio.to_thread(doc_ref.get)
            
            if doc.exists:
                doc_data = doc.to_dict() or {}
                # Extraire les champs requis : instructions, job_id, file_id, status
                self.job_data = {
                    "instructions": doc_data.get("instructions", ""),
                    "job_id": doc_data.get("job_id", job_id),
                    "file_id": doc_data.get("file_id", ""),
                    "status": doc_data.get("status", ""),
                    # Conserver les autres champs au cas oÃ¹
                    **{k: v for k, v in doc_data.items() if k not in ["instructions", "job_id", "file_id", "status"]}
                }
                
                # â•â•â• EXTRACTION DES TRANSACTIONS POUR BANKER_CHAT â•â•â•
                # Extraire et formater les transactions depuis le champ 'transactions'
                transactions_raw = doc_data.get("transactions", {})
                formatted_transactions = []
                
                if transactions_raw:
                    # Cas 1: transactions est un dictionnaire avec des clÃ©s numÃ©riques (0, 1, 2, ...)
                    if isinstance(transactions_raw, dict):
                        # Trier les clÃ©s numÃ©riquement pour maintenir l'ordre
                        for key in sorted(transactions_raw.keys(), key=lambda x: int(x) if str(x).isdigit() else 999):
                            transaction = transactions_raw[key]
                            if isinstance(transaction, dict):
                                # Extraire uniquement les champs importants
                                formatted_transaction = {
                                    "amount": transaction.get("amount"),
                                    "currency_name": transaction.get("currency_name", ""),
                                    "date": transaction.get("date", ""),
                                    "payment_ref": transaction.get("payment_ref", ""),
                                    "status": transaction.get("status", ""),
                                    "transaction_id": transaction.get("transaction_id", "")
                                }
                                formatted_transactions.append(formatted_transaction)
                    # Cas 2: transactions est une liste
                    elif isinstance(transactions_raw, list):
                        for transaction in transactions_raw:
                            if isinstance(transaction, dict):
                                # Extraire uniquement les champs importants
                                formatted_transaction = {
                                    "amount": transaction.get("amount"),
                                    "currency_name": transaction.get("currency_name", ""),
                                    "date": transaction.get("date", ""),
                                    "payment_ref": transaction.get("payment_ref", ""),
                                    "status": transaction.get("status", ""),
                                    "transaction_id": transaction.get("transaction_id", "")
                                }
                                formatted_transactions.append(formatted_transaction)
                    
                    if formatted_transactions:
                        self.job_data["formatted_transactions"] = formatted_transactions
                        logger.info(
                            f"[BRAIN_JOB_DATA] {len(formatted_transactions)} transactions formatÃ©es "
                            f"pour job_id={job_id}"
                        )
                
                logger.info(
                    f"[BRAIN_JOB_DATA] DonnÃ©es job chargÃ©es pour job_id={job_id} "
                    f"(instructions={bool(self.job_data.get('instructions'))}, "
                    f"file_id={self.job_data.get('file_id')}, "
                    f"status={self.job_data.get('status')}, "
                    f"transactions={len(self.job_data.get('formatted_transactions', []))})"
                )
            else:
                # C'est normal si le document n'existe pas encore (job pas encore lancÃ©)
                # On initialise avec des valeurs par dÃ©faut
                self.job_data = {
                    "instructions": "",
                    "job_id": job_id,
                    "file_id": "",
                    "status": "pending"
                }
                logger.debug(
                    f"[BRAIN_JOB_DATA] Document job non trouvÃ© pour path={job_path} "
                    f"(job_id={job_id}) - Initialisation avec valeurs par dÃ©faut. "
                    f"C'est normal si le job n'a pas encore Ã©tÃ© lancÃ©."
                )
        
        except Exception as e:
            logger.error(f"[BRAIN_JOB_DATA] Erreur chargement donnÃ©es: {e}", exc_info=True)
            self.job_data = {
                "instructions": "",
                "job_id": job_id,
                "file_id": "",
                "status": ""
            }
        
        return self.job_data
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰THODES SPT (synchrones)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _spt_read_firebase(self, collection_path: str, document_id: str) -> Dict:
        """SPT: Lecture Firebase"""
        try:
            from ...firebase_client import get_firestore
            db = get_firestore()
            doc = db.collection(f"{self.collection_name}/{collection_path}").document(document_id).get()
            
            if doc.exists:
                return {
                    'type': 'success',
                    'data': doc.to_dict(),
                    'document_id': document_id
                }
            else:
                return {
                    'type': 'not_found',
                    'message': f"Document {document_id} non trouvÃ©"
                }
        except Exception as e:
            logger.error(f"Erreur lecture Firebase: {e}")
            return {'type': 'error', 'message': str(e)}
    
    def _spt_search_chromadb(self, query: str, n_results: int = 5) -> Dict:
        """SPT: Recherche ChromaDB"""
        try:
            from ...chroma_vector_service import get_chroma_vector_service
            chroma = get_chroma_vector_service()
            
            results = chroma.query_collection(
                user_id=self.firebase_user_id,
                collection_name=self.collection_name,
                query_texts=[query],
                n_results=n_results
            )
            
            return {
                'type': 'success',
                'results': results,
                'count': len(results.get('documents', [[]])[0]) if results else 0
            }
        except Exception as e:
            logger.error(f"Erreur recherche ChromaDB: {e}")
            return {'type': 'error', 'message': str(e)}
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰THODES LPT (asynchrones via HTTP)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _lpt_file_manager(self, thread_key: str, action: str, params: Dict, task_title: str) -> Dict:
        """
        LPT: Appel Ã  l'Agent File Manager (HTTP)
        
        Cette mÃ©thode dÃ©clenche une tÃ¢che asynchrone et retourne immÃ©diatement.
        Le rÃ©sultat arrivera plus tard via callback.
        """
        try:
            from .task_tracker import TaskTracker
            tracker = TaskTracker(self.firebase_user_id, self.collection_name)
            
            # CrÃ©er une tÃ¢che LPT
            task_id = tracker.create_lpt_task(
                thread_key=thread_key,
                agent_type="file_manager",
                action=action,
                params=params,
                task_title=task_title
            )
            
            # Enregistrer la tÃ¢che active
            if thread_key not in self.active_lpt_tasks:
                self.active_lpt_tasks[thread_key] = []
            self.active_lpt_tasks[thread_key].append(task_id)
            
            logger.info(f"TÃ¢che LPT crÃ©Ã©e: {task_id} pour File Manager")
            
            return {
                'type': 'lpt_started',
                'task_id': task_id,
                'agent': 'file_manager',
                'estimated_duration': '2-3 minutes',
                'message': f"âœ… TÃ¢che '{task_title}' envoyÃ©e Ã  l'Agent File Manager. Traitement en cours..."
            }
            
        except Exception as e:
            logger.error(f"Erreur dÃ©marrage LPT File Manager: {e}")
            return {'type': 'error', 'message': str(e)}
    
    def _lpt_accounting(self, thread_key: str, action: str, params: Dict, task_title: str) -> Dict:
        """
        LPT: Appel Ã  l'Agent Comptable (HTTP)
        
        Cette mÃ©thode dÃ©clenche une tÃ¢che asynchrone et retourne immÃ©diatement.
        Le rÃ©sultat arrivera plus tard via callback.
        """
        try:
            from .task_tracker import TaskTracker
            tracker = TaskTracker(self.firebase_user_id, self.collection_name)
            
            # CrÃ©er une tÃ¢che LPT
            task_id = tracker.create_lpt_task(
                thread_key=thread_key,
                agent_type="accounting",
                action=action,
                params=params,
                task_title=task_title
            )
            
            # Enregistrer la tÃ¢che active
            if thread_key not in self.active_lpt_tasks:
                self.active_lpt_tasks[thread_key] = []
            self.active_lpt_tasks[thread_key].append(task_id)
            
            logger.info(f"TÃ¢che LPT crÃ©Ã©e: {task_id} pour Accounting")
            
            return {
                'type': 'lpt_started',
                'task_id': task_id,
                'agent': 'accounting',
                'estimated_duration': '5-10 minutes',
                'message': f"âœ… TÃ¢che '{task_title}' envoyÃ©e Ã  l'Agent Comptable. Traitement en cours..."
            }
            
        except Exception as e:
            logger.error(f"Erreur dÃ©marrage LPT Accounting: {e}")
            return {'type': 'error', 'message': str(e)}
    
    def has_active_lpt_tasks(self, thread_key: str) -> bool:
        """VÃ©rifie si des tÃ¢ches LPT sont en cours pour ce thread"""
        return thread_key in self.active_lpt_tasks and len(self.active_lpt_tasks[thread_key]) > 0
    
    def get_active_lpt_count(self, thread_key: str) -> int:
        """Retourne le nombre de tÃ¢ches LPT actives"""
        return len(self.active_lpt_tasks.get(thread_key, []))
    
    def reset_context_with_summary(self, summary: str) -> int:
        """
        RÃ©initialise le contexte avec un rÃ©sumÃ© intÃ©grÃ© au system prompt.
        
        Cette mÃ©thode :
        1. Ajoute le rÃ©sumÃ© au system prompt de base
        2. Vide l'historique du chat
        3. Calcule et retourne le nombre de tokens du nouveau contexte
        
        Args:
            summary: RÃ©sumÃ© de la conversation Ã  intÃ©grer
        
        Returns:
            Nombre de tokens du nouveau contexte (system prompt + rÃ©sumÃ©)
        """
        logger.info("[RESET] RÃ©initialisation du contexte avec rÃ©sumÃ©")
        
        # RÃ©cupÃ©rer l'instance provider
        provider_instance = self.pinnokio_agent.get_provider_instance(self.default_provider)
        
        # Sauvegarder le system prompt de base (si pas dÃ©jÃ  sauvegardÃ©)
        if not hasattr(self, '_base_system_prompt'):
            self._base_system_prompt = provider_instance.system_prompt if hasattr(provider_instance, 'system_prompt') else ""
        
        # CrÃ©er le nouveau system prompt avec rÃ©sumÃ© intÃ©grÃ©
        new_system_prompt = f"""{self._base_system_prompt}

                â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                ğŸ“‹ CONTEXTE DE LA CONVERSATION PRÃ‰CÃ‰DENTE :

                {summary}

                â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                Continue la conversation en tenant compte de ce contexte historique.
                """
        
        # Mettre Ã  jour le system prompt
        if hasattr(provider_instance, 'update_system_prompt'):
            provider_instance.update_system_prompt(new_system_prompt)
        elif hasattr(provider_instance, 'system_prompt'):
            provider_instance.system_prompt = new_system_prompt
        
        # â­ NOUVEAU: Vider l'historique du brain (isolÃ© par thread)
        self.clear_chat_history()
        
        # Calculer les tokens du nouveau contexte
        # â­ get_history_token_count() compte maintenant automatiquement :
        # - L'historique du chat (vide aprÃ¨s clear)
        # - Le system prompt complet (avec rÃ©sumÃ© intÃ©grÃ©)
        tokens_after_reset = self.get_history_token_count()
        
        logger.info(
            f"[RESET] Contexte rÃ©initialisÃ© - "
            f"Nouveau contexte: {tokens_after_reset:,} tokens "
            f"(system prompt avec rÃ©sumÃ© + historique vide)"
        )
        
        return tokens_after_reset
    
    def generate_conversation_summary(self, thread_key: str, total_tokens_used: int) -> str:
        """
        GÃ©nÃ¨re un rÃ©sumÃ© compressÃ© de la conversation actuelle
        pour rÃ©initialiser le contexte tout en gardant l'essentiel.
        
        Cette mÃ©thode est appelÃ©e quand le budget de tokens est atteint (80K)
        pour compresser l'historique et permettre de continuer la conversation.
        
        Args:
            thread_key: ClÃ© du thread de conversation
            total_tokens_used: Nombre total de tokens utilisÃ©s dans la session
        
        Returns:
            RÃ©sumÃ© compressÃ© de la conversation (max 500 tokens)
        """
        logger.info(f"[SUMMARY] GÃ©nÃ©ration rÃ©sumÃ© - thread={thread_key}, tokens={total_tokens_used}")
        
        summary_prompt = f"""RÃ©sume cette conversation en gardant UNIQUEMENT les informations critiques:

                **Instructions de RÃ©sumÃ©** :
                1. **Contexte initial**: Quelle Ã©tait la demande originale de l'utilisateur ?
                2. **Actions effectuÃ©es**: Quels outils ont Ã©tÃ© utilisÃ©s (SPT/LPT) et pourquoi ?
                3. **RÃ©sultats clÃ©s**: Qu'avons-nous dÃ©couvert ou accompli ?
                4. **Ã‰tat actuel**: OÃ¹ en sommes-nous maintenant ? Que reste-t-il Ã  faire ?
                5. **TÃ¢ches LPT en cours**: Y a-t-il des tÃ¢ches longues en cours d'exÃ©cution ?

                **Contraintes** :
                - Maximum 500 tokens
                - Format concis et structurÃ©
                - Garde uniquement l'essentiel pour continuer efficacement

                Tokens utilisÃ©s dans cette session: {total_tokens_used:,}
                """
        
        try:
            # Utiliser l'agent pour gÃ©nÃ©rer le rÃ©sumÃ© (sans outils)
            summary_response = self.pinnokio_agent.process_tool_use(
                content=summary_prompt,
                tools=[],  # Pas d'outils pour le rÃ©sumÃ©
                tool_mapping={},
                provider=self.default_provider,
                size=ModelSize.SMALL,  # ModÃ¨le rapide suffit pour un rÃ©sumÃ©
                max_tokens=600,
                raw_output=True
            )
            
            # Extraire le texte du rÃ©sumÃ©
            summary_text = self._extract_text_from_summary_response(summary_response)
            
            logger.info(f"[SUMMARY] RÃ©sumÃ© gÃ©nÃ©rÃ© - longueur={len(summary_text)} caractÃ¨res")
            
            return summary_text
            
        except Exception as e:
            logger.error(f"[SUMMARY] Erreur gÃ©nÃ©ration rÃ©sumÃ©: {e}", exc_info=True)
            
            # RÃ©sumÃ© de fallback en cas d'erreur
            return f"""RÃ©sumÃ© automatique de la session:
            - Tokens utilisÃ©s: {total_tokens_used:,}
            - Thread: {thread_key}
            - TÃ¢ches LPT actives: {self.get_active_lpt_count(thread_key)}
            - Budget tokens atteint, contexte rÃ©initialisÃ©.
            """
    
    def _extract_text_from_summary_response(self, response: Any) -> str:
        """
        Extrait le texte d'une rÃ©ponse de rÃ©sumÃ©.
        Helper method pour extract le texte peu importe le format de rÃ©ponse.
        """
        if not response:
            return "Aucun rÃ©sumÃ© gÃ©nÃ©rÃ©."
        
        # Si c'est une liste de rÃ©ponses
        if isinstance(response, list):
            for item in response:
                if isinstance(item, dict):
                    # Chercher text_output
                    if "text_output" in item:
                        text_block = item["text_output"]
                        if isinstance(text_block, dict) and "content" in text_block:
                            return str(text_block["content"])
                        elif isinstance(text_block, str):
                            return text_block
        
        # Si c'est directement un dict
        if isinstance(response, dict):
            if "text_output" in response:
                text_block = response["text_output"]
                if isinstance(text_block, dict) and "content" in text_block:
                    return str(text_block["content"])
                elif isinstance(text_block, str):
                    return text_block
        
        # Fallback: convertir en string
        return str(response)[:1000]  # Limiter Ã  1000 chars par sÃ©curitÃ©

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GESTION HISTORIQUE CHAT (ISOLÃ‰ PAR THREAD)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def add_user_message(self, content):
        """
        Ajoute un message utilisateur Ã  l'historique du chat.
        Proxy vers self.pinnokio_agent.add_user_message()
        
        Args:
            content: Contenu du message utilisateur (str ou list pour tool_results)
        """
        if self.pinnokio_agent:
            self.pinnokio_agent.add_user_message(content, provider=self.default_provider)
            content_len = len(content) if isinstance(content, (str, list)) else 0
            logger.debug(f"[CHAT_HISTORY] Message utilisateur ajoutÃ© via agent (type={type(content).__name__}, len={content_len})")
        else:
            logger.warning(f"[CHAT_HISTORY] Agent non initialisÃ©, message non ajoutÃ©")
    
    def add_assistant_message(self, content: Any):
        """
        Ajoute un message assistant Ã  l'historique du chat.
        Proxy vers self.pinnokio_agent.add_ai_message()
        
        Args:
            content: Contenu du message assistant (str, list ou dict)
                     - str: texte simple
                     - list: blocs Anthropic (text, tool_use, etc.)
                     - dict: ancien format (sera prÃ©servÃ©)
        """
        if self.pinnokio_agent:
            self.pinnokio_agent.add_ai_message(content, provider=self.default_provider)
            content_type = type(content).__name__
            content_len = len(content) if isinstance(content, (str, list)) else 1
            logger.debug(f"[CHAT_HISTORY] Message assistant ajoutÃ© via agent (type={content_type}, len={content_len})")
        else:
            logger.warning(f"[CHAT_HISTORY] Agent non initialisÃ©, message non ajoutÃ©")
    
    def get_chat_history(self) -> List[Dict[str, Any]]:
        """
        Retourne l'historique complet du chat.
        Proxy vers self.pinnokio_agent.chat_history
        
        Returns:
            Liste des messages du chat
        """
        if self.pinnokio_agent:
            return self.pinnokio_agent.chat_history.get(self.default_provider.value, []).copy()
        return []
    
    def clear_chat_history(self):
        """
        Vide l'historique du chat pour ce thread.
        Proxy vers self.pinnokio_agent.clear_chat_history()
        """
        if self.pinnokio_agent:
            current_history = self.get_chat_history()
            message_count = len(current_history)
            self.pinnokio_agent.clear_chat_history()
            logger.info(f"[CHAT_HISTORY] Historique vidÃ© via agent ({message_count} messages supprimÃ©s)")
        else:
            logger.warning(f"[CHAT_HISTORY] Agent non initialisÃ©, rien Ã  vider")
    
    async def load_user_context(self, mode: str = "UI") -> Dict[str, Any]:
        """
        Charge le contexte utilisateur (mÃ©tadonnÃ©es sociÃ©tÃ©) dans le brain SESSION.
        
        â­ NOUVEAU : Support dual-mode (UI/BACKEND)
        
        Mode UI (utilisateur connectÃ©) :
        1. Tenter cache Redis (TTL 1h)
        2. Si CACHE MISS â†’ Fallback Firebase
        3. Mettre en cache pour prochains appels
        
        Mode BACKEND (utilisateur dÃ©connectÃ©) :
        1. AccÃ¨s direct Firebase (source de vÃ©ritÃ©)
        2. Pas de cache
        
        Ce contexte contient toutes les mÃ©tadonnÃ©es importantes :
        - mandate_path, client_uuid, company_name
        - dms_system, drive_space_parent_id
        - communication_mode, log_communication_mode
        - bank_erp (odoo_url, odoo_db, etc.)
        
        â­ IMPORTANT : AppelÃ© lors de initialize_agent() (pas besoin de thread_key)
        
        Args:
            mode: "UI" (cache prioritaire) ou "BACKEND" (Firebase direct)
        
        Returns:
            Dict contenant le contexte utilisateur
        """
        try:
            logger.info(f"[BRAIN_CONTEXT] Chargement contexte utilisateur - Mode: {mode}")
            
            import json
            from ...redis_client import get_redis
            
            context = None
            
            # â•â•â• MODE UI : Cache Redis â†’ Fallback Firebase â•â•â•
            if mode == "UI":
                try:
                    redis_client = get_redis()
                    cache_key = f"context:{self.firebase_user_id}:{self.collection_name}"
                    
                    logger.info(f"[BRAIN_CONTEXT] ğŸ” DEBUG - Tentative lecture cache: {cache_key}")
                    cached_data = redis_client.get(cache_key)
                    logger.info(f"[BRAIN_CONTEXT] ğŸ” DEBUG - cached_data type: {type(cached_data)}, value: {cached_data[:100] if cached_data else None}")
                    
                    if cached_data:
                        context = json.loads(cached_data)
                        logger.info(f"[BRAIN_CONTEXT] âœ… CACHE HIT: {cache_key}")
                    else:
                        logger.info(f"[BRAIN_CONTEXT] âŒ CACHE MISS: {cache_key} - Fallback Firebase")
                
                except Exception as e:
                    logger.warning(f"[BRAIN_CONTEXT] Erreur accÃ¨s cache: {e} - Fallback Firebase")
            
            # â•â•â• Si pas de cache OU mode BACKEND : Firebase direct â•â•â•
            if context is None:
                logger.info(f"[BRAIN_CONTEXT] RÃ©cupÃ©ration depuis Firebase...")
                
                from ..tools.lpt_client import LPTClient
                
                lpt_client = LPTClient()
                
                # RÃ©cupÃ©rer depuis Firebase (sans cache)
                context = await lpt_client._reconstruct_full_company_profile(
                    self.firebase_user_id,
                    self.collection_name
                )
                
                # Si mode UI, mettre en cache
                if mode == "UI" and context:
                    try:
                        redis_client = get_redis()
                        cache_key = f"context:{self.firebase_user_id}:{self.collection_name}"
                        
                        # Ajouter timestamp
                        context["cached_at"] = datetime.now().isoformat()
                        
                        redis_client.setex(
                            cache_key,
                            3600,  # TTL 1 heure
                            json.dumps(context)
                        )
                        
                        logger.info(f"[BRAIN_CONTEXT] âœ… Contexte mis en cache: {cache_key}")
                    
                    except Exception as e:
                        logger.warning(f"[BRAIN_CONTEXT] Erreur mise en cache: {e}")
            
            # â•â•â• Stocker dans le brain â•â•â•
            if context:
                self.user_context = context
                
                logger.info(
                    f"[BRAIN_CONTEXT] âœ… Contexte chargÃ©: mandate_path={context.get('mandate_path')}, "
                    f"dms_system={context.get('dms_system')}, "
                    f"client_uuid={context.get('client_uuid')}, "
                    f"mode={mode}"
                )
                
                # ğŸ” DEBUG : Afficher les champs critiques pour Router et Bank
                logger.info(
                    f"[BRAIN_CONTEXT] ğŸ” DEBUG - Champs Drive: "
                    f"drive_space_parent_id={context.get('drive_space_parent_id')}, "
                    f"input_drive_doc_id={context.get('input_drive_doc_id')}"
                )
                logger.info(
                    f"[BRAIN_CONTEXT] ğŸ” DEBUG - Champs ERP Bank: "
                    f"mandate_bank_erp={context.get('mandate_bank_erp')}, "
                    f"erp_odoo_url={context.get('erp_odoo_url')}, "
                    f"erp_erp_type={context.get('erp_erp_type')}"
                )
                logger.info(
                    f"[BRAIN_CONTEXT] ğŸ” DEBUG - Toutes les clÃ©s: {list(context.keys())}"
                )
                
                return context
            
            else:
                raise ValueError("Contexte vide depuis Firebase")
        
        except Exception as e:
            logger.error(f"[BRAIN_CONTEXT] âŒ Erreur chargement contexte: {e}", exc_info=True)
            
            # Retourner un contexte minimal pour ne pas bloquer
            self.user_context = {
                "mandate_path": self.collection_name,
                "dms_system": "google_drive",
                "communication_mode": "webhook",
                "log_communication_mode": "firebase",
                "user_language": "fr",
                "mode": mode
            }
            
            return self.user_context
    
    def get_user_context(self) -> Dict[str, Any]:
        """
        RÃ©cupÃ¨re le contexte utilisateur stockÃ© dans le brain.
        
        Returns:
            Dict contenant le contexte utilisateur, ou dict vide si non chargÃ©
        """
        if self.user_context is None:
            logger.warning(
                f"[BRAIN_CONTEXT] âš ï¸ Contexte non chargÃ©. "
                f"Appelez load_user_context() aprÃ¨s crÃ©ation du brain."
            )
            return {}
        
        return self.user_context
    
    def set_active_thread(self, thread_key: str):
        """
        DÃ©finit le thread actif pour les workflows d'approbation.
        
        Cette mÃ©thode doit Ãªtre appelÃ©e au dÃ©but du traitement d'un message
        pour que les outils sachent sur quel thread envoyer les cartes d'approbation.
        
        Args:
            thread_key: ClÃ© du thread de conversation actif
        """
        self.active_thread_key = thread_key
        logger.info(f"[BRAIN] Thread actif dÃ©fini: {thread_key}")
    
    def get_history_token_count(self) -> int:
        """
        Estime le nombre de tokens dans le contexte actuel complet.
        
        â­ PROXY vers BaseAIAgent.get_total_context_tokens()
        
        Calcule automatiquement :
        - Chat history (messages utilisateur + assistant + tool_results)
        - System prompt (avec rÃ©sumÃ©s Ã©ventuels)
        
        Returns:
            Nombre approximatif de tokens dans le contexte complet
        """
        if not self.pinnokio_agent:
            return 0
        
        # DÃ©lÃ©guer le calcul Ã  BaseAIAgent (Ã©vite duplication de code)
        return self.pinnokio_agent.get_total_context_tokens(self.default_provider)

