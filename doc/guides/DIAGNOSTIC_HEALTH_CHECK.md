# üîç Diagnostic des √©checs de Health Check ECS

## üìã R√©sum√© du probl√®me

Vos t√¢ches ECS sont tu√©es apr√®s **2-6 minutes** avec le message d'erreur :
```
Task failed ELB health checks (code 137 - SIGKILL)
```

## ‚úÖ Analyse des logs

### Ce qui fonctionne correctement :
- ‚úÖ L'application d√©marre sans erreur
- ‚úÖ Le endpoint `/healthz` r√©pond avec `200 OK`
- ‚úÖ Les health checks internes r√©ussissent
- ‚úÖ Aucune erreur critique dans l'application

### Le probl√®me identifi√© :
Les logs montrent que les health checks **r√©ussissent**, mais ECS tue quand m√™me les t√¢ches !

## üéØ Cause racine

La configuration du **Target Group** et du **Service ECS** est trop stricte :

| Param√®tre | Valeur Actuelle | Probl√®me |
|-----------|----------------|----------|
| **Grace Period ECS** | 60 secondes | ‚ö†Ô∏è Trop court pour initialiser l'app |
| **Unhealthy Threshold** | 2 √©checs | ‚ö†Ô∏è Pas assez tol√©rant |
| **Timeout** | 5 secondes | ‚ö†Ô∏è Peut √™tre trop court sous charge |
| **Healthy Threshold** | 5 succ√®s | ‚è∞ Trop long pour d√©marrer |

### Timeline du probl√®me :
```
0s      : Conteneur d√©marre
0-6s    : Pull de l'image Docker
6-30s   : Application s'initialise (Redis, Firebase, ChromaDB, etc.)
60s     : FIN du grace period ‚ö†Ô∏è
60-90s  : Premier health check apr√®s grace period
90-120s : Deuxi√®me health check - SI 2 √âCHECS ‚Üí T√ÇCHE TU√âE üíÄ
```

**R√©sultat** : La t√¢che est tu√©e apr√®s ~120 secondes (2 minutes) si 2 health checks √©chouent !

## üõ†Ô∏è Solution recommand√©e

### Nouvelle configuration optimale :

```powershell
# 1. Augmenter le grace period √† 5 minutes
aws ecs update-service \
    --cluster pinnokio_cluster \
    --service pinnokio_microservice \
    --health-check-grace-period-seconds 300 \
    --region us-east-1

# 2. Rendre le Target Group plus tol√©rant
aws elbv2 modify-target-group \
    --target-group-arn arn:aws:elasticloadbalancing:us-east-1:654654322636:targetgroup/new-pinnokio-firebase-backend/6c7046f6f3969fee \
    --health-check-interval-seconds 30 \
    --health-check-timeout-seconds 10 \
    --healthy-threshold-count 2 \
    --unhealthy-threshold-count 5 \
    --region us-east-1
```

### Param√®tres optimis√©s :

| Param√®tre | Avant | Apr√®s | B√©n√©fice |
|-----------|-------|-------|----------|
| **Grace Period** | 60s | **300s** | 5 minutes pour initialiser |
| **Timeout** | 5s | **10s** | Plus de temps pour r√©pondre |
| **Healthy Threshold** | 5 | **2** | D√©marre plus vite (60s au lieu de 150s) |
| **Unhealthy Threshold** | 2 | **5** | Tol√®re les pics temporaires |

### Nouvelle timeline :
```
0s      : Conteneur d√©marre
0-30s   : Application s'initialise
300s    : FIN du grace period (5 minutes) ‚úÖ
300s+   : Health checks commencent
‚Üí Besoin de 5 √©checs cons√©cutifs (2.5 minutes) pour tuer la t√¢che
‚Üí Total: Minimum 7.5 minutes avant qu'une t√¢che soit tu√©e
```

## üöÄ Appliquer la solution

### Option 1 : Utiliser le script PowerShell
```powershell
.\fix_health_check.ps1
```

### Option 2 : Commandes manuelles
Ex√©cutez les deux commandes ci-dessus dans votre terminal.

## üìä V√©rification

Apr√®s avoir appliqu√© les corrections, v√©rifiez :

```bash
# 1. V√©rifier le grace period
aws ecs describe-services \
    --cluster pinnokio_cluster \
    --services pinnokio_microservice \
    --region us-east-1 \
    --query "services[0].healthCheckGracePeriodSeconds"

# 2. V√©rifier le Target Group
aws elbv2 describe-target-groups \
    --target-group-arns arn:aws:elasticloadbalancing:us-east-1:654654322636:targetgroup/new-pinnokio-firebase-backend/6c7046f6f3969fee \
    --region us-east-1 \
    --query "TargetGroups[0].[HealthCheckTimeoutSeconds,HealthyThresholdCount,UnhealthyThresholdCount]"

# 3. Surveiller les t√¢ches
aws ecs list-tasks \
    --cluster pinnokio_cluster \
    --service-name pinnokio_microservice \
    --desired-status RUNNING \
    --region us-east-1
```

## üìà R√©sultats attendus

Apr√®s avoir appliqu√© ces corrections :
- ‚úÖ Les t√¢ches ne seront plus tu√©es pr√©matur√©ment
- ‚úÖ Plus de tol√©rance aux pics de charge temporaires
- ‚úÖ D√©ploiements plus stables
- ‚úÖ Moins de red√©marrages inutiles

## üìù Notes additionnelles

### Pourquoi ces valeurs ?

1. **Grace Period (300s)** : Permet √† l'application de :
   - Initialiser les connexions Firebase
   - Se connecter √† Redis
   - Charger ChromaDB
   - D√©marrer tous les listeners

2. **Unhealthy Threshold (5)** : 
   - Tol√®re les pics temporaires de charge
   - √âvite les faux positifs
   - 5 √©checs √ó 30s = 2.5 minutes avant de tuer une t√¢che saine

3. **Healthy Threshold (2)** :
   - Nouvelle t√¢che devient "healthy" apr√®s 2 succ√®s (60s)
   - Acc√©l√®re les d√©ploiements

4. **Timeout (10s)** :
   - Laisse le temps √† l'application de r√©pondre m√™me sous charge
   - Compatible avec des temps de r√©ponse variables

## üîÑ Prochaines √©tapes

1. Appliquer les corrections
2. Surveiller les t√¢ches pendant 15-20 minutes
3. V√©rifier qu'aucune t√¢che n'est tu√©e pr√©matur√©ment
4. Si tout fonctionne, documenter la configuration

## üìû Commandes utiles pour le monitoring

```bash
# Surveiller les √©v√©nements du service
aws ecs describe-services \
    --cluster pinnokio_cluster \
    --services pinnokio_microservice \
    --region us-east-1 \
    --query 'services[0].events[:10]'

# Voir les t√¢ches arr√™t√©es r√©cemment
aws ecs list-tasks \
    --cluster pinnokio_cluster \
    --service-name pinnokio_microservice \
    --desired-status STOPPED \
    --region us-east-1

# T√©l√©charger les logs d'une t√¢che
python download_logs.py
```

---

**Date du diagnostic** : 27 novembre 2025  
**Fichiers de logs analys√©s** :
- `logs_task_6ac9ae34675d448b9a904c4d8f538524.txt`
- `logs_task_46e2329eaa0c4352affa79f697746163.txt`
- `logs_task_35effd67684940bfaf39cf48dd2830af.txt`

