# ğŸš€ Configuration du Streaming LLM via WebSocket pour Reflex

## ğŸ“¡ Vue d'ensemble

Le microservice Python a Ã©tÃ© migrÃ© du streaming RTDB vers le streaming WebSocket pour amÃ©liorer la fluiditÃ© et rÃ©duire la latence des rÃ©ponses IA.

**Changements clÃ©s :**
- âœ… **Streaming temps rÃ©el via WebSocket** : Latence rÃ©duite de ~50-200ms Ã  ~1-5ms
- âœ… **1 seule Ã©criture RTDB finale** : Ã‰conomie sur les coÃ»ts Firebase
- âœ… **Format de canal identique** : Facilite la transition depuis RTDB
- âœ… **CompatibilitÃ© maintenue** : L'Ã©criture finale dans RTDB reste pour l'historique

---

## ğŸ”Œ Connexion WebSocket

### Endpoint WebSocket
```
wss://your-microservice.com/ws?uid={user_id}&space_code={collection_name}&thread_key={thread_key}
```

### ParamÃ¨tres de connexion
- `uid` **(requis)** : ID Firebase de l'utilisateur
- `space_code` **(optionnel)** : Code de la sociÃ©tÃ©/espace (collection_name)
- `thread_key` **(optionnel)** : ClÃ© du thread de conversation
- `mode` **(optionnel)** : Mode de chat (dÃ©faut: "auto")

---

## ğŸ“¨ Format du Canal WebSocket

Le canal WebSocket utilise **exactement le mÃªme format que RTDB** :

```
chat:{user_id}:{collection_name}:{thread_key}
```

### Exemple
```
chat:user123:company456:thread789
```

Ce format est retournÃ© dans la rÃ©ponse RPC `LLM.send_message` sous la clÃ© `ws_channel`.

---

## ğŸ“¥ Types d'Ã‰vÃ©nements WebSocket

Tous les Ã©vÃ©nements WebSocket contiennent :
- `type` : Type d'Ã©vÃ©nement (voir ci-dessous)
- `channel` : Canal au format `chat:{user_id}:{collection_name}:{thread_key}`
- `payload` : DonnÃ©es de l'Ã©vÃ©nement

### 1ï¸âƒ£ **llm_stream_start** - DÃ©but du streaming

ReÃ§u au dÃ©but de la gÃ©nÃ©ration de la rÃ©ponse IA.

```json
{
  "type": "llm_stream_start",
  "channel": "chat:user123:company456:thread789",
  "payload": {
    "message_id": "msg-uuid-1234",
    "thread_key": "thread789",
    "space_code": "company456",
    "timestamp": "2025-10-12T10:30:00.123456Z"
  }
}
```

**Action Reflex recommandÃ©e :**
- CrÃ©er un message temporaire avec `is_streaming=True`
- Afficher un indicateur "IA en train d'Ã©crire..."

---

### 2ï¸âƒ£ **llm_stream_chunk** - Chunk de contenu

ReÃ§u pour chaque morceau de texte gÃ©nÃ©rÃ© par l'IA (trÃ¨s haute frÃ©quence).

```json
{
  "type": "llm_stream_chunk",
  "channel": "chat:user123:company456:thread789",
  "payload": {
    "message_id": "msg-uuid-1234",
    "thread_key": "thread789",
    "space_code": "company456",
    "chunk": " puis-je",
    "accumulated": "Bonjour, comment puis-je",
    "is_final": false
  }
}
```

**Champs importants :**
- `chunk` : Nouveau fragment de texte (Ã  ajouter)
- `accumulated` : Contenu complet jusqu'Ã  prÃ©sent (Ã  afficher)
- `is_final` : `true` si c'est le dernier chunk

**Action Reflex recommandÃ©e :**
- Mettre Ã  jour le message temporaire avec `accumulated`
- DÃ©clencher un re-render pour effet de "typing"

---

### 3ï¸âƒ£ **llm_stream_complete** - Fin du streaming

ReÃ§u une fois la gÃ©nÃ©ration terminÃ©e avec succÃ¨s.

```json
{
  "type": "llm_stream_complete",
  "channel": "chat:user123:company456:thread789",
  "payload": {
    "message_id": "msg-uuid-1234",
    "thread_key": "thread789",
    "space_code": "company456",
    "full_content": "Bonjour, comment puis-je vous aider aujourd'hui ?",
    "metadata": {
      "tokens_used": {
        "prompt": 150,
        "completion": 25,
        "total": 175
      },
      "duration_ms": 2340,
      "model": "claude-3-7-sonnet-20250219",
      "status": "complete",
      "completed_at": "2025-10-12T10:30:02.463456Z"
    }
  }
}
```

**Action Reflex recommandÃ©e :**
- Convertir le message temporaire en message permanent
- Retirer l'indicateur de streaming
- Sauvegarder les mÃ©tadonnÃ©es (tokens, durÃ©e, etc.)

---

### 4ï¸âƒ£ **llm_stream_interrupted** - Streaming interrompu

ReÃ§u si l'utilisateur interrompt le streaming (stop_streaming).

```json
{
  "type": "llm_stream_interrupted",
  "channel": "chat:user123:company456:thread789",
  "payload": {
    "message_id": "msg-uuid-1234",
    "thread_key": "thread789",
    "space_code": "company456",
    "accumulated": "Bonjour, comment"
  }
}
```

**Action Reflex recommandÃ©e :**
- Afficher le contenu partiel `accumulated`
- Ajouter une note "âš ï¸ RÃ©ponse interrompue"
- Marquer le message comme interrompu

---

### 5ï¸âƒ£ **llm_stream_error** - Erreur pendant le streaming

ReÃ§u en cas d'erreur pendant la gÃ©nÃ©ration.

```json
{
  "type": "llm_stream_error",
  "channel": "chat:user123:company456:thread789",
  "payload": {
    "message_id": "msg-uuid-1234",
    "thread_key": "thread789",
    "space_code": "company456",
    "error": "Anthropic API rate limit exceeded"
  }
}
```

**Action Reflex recommandÃ©e :**
- Afficher un message d'erreur Ã  l'utilisateur
- Supprimer le message temporaire ou le marquer comme erreur
- Logger l'erreur pour debugging

---

## ğŸ”„ Flux Complet : Envoi d'un Message

### 1. Envoi du message via RPC (inchangÃ©)

```python
# Code Reflex cÃ´tÃ© client
response = await rpc_client.call(
    method="LLM.send_message",
    kwargs={
        "user_id": "user123",
        "collection_name": "company456",
        "thread_key": "thread789",
        "message": "Bonjour, peux-tu m'aider ?",
        "chat_mode": "general_chat",
        "system_prompt": "Tu es un assistant utile...",
        "selected_tool": None
    }
)

# RÃ©ponse RPC
{
  "success": True,
  "user_message_id": "msg-user-uuid",
  "assistant_message_id": "msg-assistant-uuid",
  "ws_channel": "chat:user123:company456:thread789",  # â† NOUVEAU
  "message": "Message envoyÃ©, rÃ©ponse en cours de streaming via WebSocket"
}
```

### 2. Ã‰coute des Ã©vÃ©nements WebSocket

```python
# Code Reflex - Gestion des Ã©vÃ©nements WebSocket
class ChatState(rx.State):
    messages: List[Message] = []
    streaming_message: Optional[StreamingMessage] = None
    
    async def handle_websocket_event(self, event: dict):
        """Gestionnaire unifiÃ© des Ã©vÃ©nements WebSocket LLM"""
        event_type = event.get("type")
        channel = event.get("channel")
        payload = event.get("payload", {})
        
        # VÃ©rifier que c'est bien notre canal
        expected_channel = f"chat:{self.user_id}:{self.space_code}:{self.thread_key}"
        if channel != expected_channel:
            return  # Ignorer si ce n'est pas notre canal
        
        if event_type == "llm_stream_start":
            self._handle_stream_start(payload)
        
        elif event_type == "llm_stream_chunk":
            self._handle_stream_chunk(payload)
        
        elif event_type == "llm_stream_complete":
            self._handle_stream_complete(payload)
        
        elif event_type == "llm_stream_interrupted":
            self._handle_stream_interrupted(payload)
        
        elif event_type == "llm_stream_error":
            self._handle_stream_error(payload)
    
    def _handle_stream_start(self, payload: dict):
        """DÃ©but du streaming : crÃ©er un message temporaire"""
        self.streaming_message = StreamingMessage(
            id=payload["message_id"],
            thread_key=payload["thread_key"],
            content="",
            is_streaming=True,
            timestamp=payload["timestamp"]
        )
    
    def _handle_stream_chunk(self, payload: dict):
        """Chunk reÃ§u : mettre Ã  jour le contenu"""
        if self.streaming_message and self.streaming_message.id == payload["message_id"]:
            self.streaming_message.content = payload["accumulated"]
            # âœ¨ Forcer le re-render pour l'effet de typing
            self.streaming_message = self.streaming_message
    
    def _handle_stream_complete(self, payload: dict):
        """Streaming terminÃ© : convertir en message permanent"""
        if self.streaming_message:
            final_message = Message(
                id=self.streaming_message.id,
                content=payload["full_content"],
                role="assistant",
                timestamp=self.streaming_message.timestamp,
                metadata=payload.get("metadata", {})
            )
            self.messages.append(final_message)
            self.streaming_message = None
    
    def _handle_stream_interrupted(self, payload: dict):
        """Streaming interrompu : afficher contenu partiel"""
        if self.streaming_message:
            partial_message = Message(
                id=self.streaming_message.id,
                content=payload["accumulated"] + "\n\nâš ï¸ *RÃ©ponse interrompue*",
                role="assistant",
                timestamp=self.streaming_message.timestamp,
                is_interrupted=True
            )
            self.messages.append(partial_message)
            self.streaming_message = None
    
    def _handle_stream_error(self, payload: dict):
        """Erreur de streaming : afficher l'erreur"""
        if self.streaming_message:
            error_message = Message(
                id=self.streaming_message.id,
                content=f"âŒ Erreur : {payload['error']}",
                role="assistant",
                timestamp=datetime.now(),
                is_error=True
            )
            self.messages.append(error_message)
            self.streaming_message = None
```

---

## ğŸ”§ IntÃ©gration avec le SystÃ¨me Existant

### RTDB : Uniquement pour l'historique

L'Ã©coute RTDB **n'est plus nÃ©cessaire pour le streaming LLM**, mais doit Ãªtre conservÃ©e pour :

1. **Chargement de l'historique au dÃ©marrage**
   ```python
   # Charger les messages existants depuis RTDB
   async def load_chat_history(self, thread_key: str):
       history = await firebase_rtdb.get_messages(thread_key)
       self.messages = history
   ```

2. **Messages non-LLM** (notifications, messages systÃ¨me, autres utilisateurs)
   ```python
   # Conserver le listener RTDB pour les Ã©vÃ©nements non-streaming
   async def listen_rtdb_for_system_messages(self, thread_key: str):
       def callback(message_data):
           # Ignorer les messages assistant (gÃ©rÃ©s par WebSocket)
           if message_data.get("role") != "assistant":
               self.messages.append(Message.from_dict(message_data))
       
       firebase_rtdb.listen_channel(thread_key, callback)
   ```

### Architecture hybride recommandÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Reflex                             â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   WebSocket      â”‚         â”‚   RTDB Listener     â”‚        â”‚
â”‚  â”‚   (Streaming)    â”‚         â”‚   (Historique)      â”‚        â”‚
â”‚  â”‚                  â”‚         â”‚                     â”‚        â”‚
â”‚  â”‚  - llm_stream_*  â”‚         â”‚  - load_history     â”‚        â”‚
â”‚  â”‚  - Temps rÃ©el    â”‚         â”‚  - system_messages  â”‚        â”‚
â”‚  â”‚  - Ultra rapide  â”‚         â”‚  - notifications    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â†‘                            â†‘                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                            â”‚
            â”‚ (WebSocket)                â”‚ (RTDB Listener)
            â”‚                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           â”‚    Python Microservice     â”‚                       â”‚
â”‚           â”‚                            â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  WebSocket Hub      â”‚    â”‚   RTDB Writer      â”‚           â”‚
â”‚  â”‚  (ws_hub.py)        â”‚    â”‚   (1 write/msg)    â”‚           â”‚
â”‚  â”‚                     â”‚    â”‚                    â”‚           â”‚
â”‚  â”‚  - broadcast()      â”‚    â”‚  - Final message   â”‚           â”‚
â”‚  â”‚  - Streaming chunks â”‚    â”‚  - Persistence     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚        LLMManager                             â”‚            â”‚
â”‚  â”‚  (_process_message_with_ws_streaming)        â”‚            â”‚
â”‚  â”‚                                               â”‚            â”‚
â”‚  â”‚  1. hub.broadcast(llm_stream_start)          â”‚            â”‚
â”‚  â”‚  2. For each chunk:                          â”‚            â”‚
â”‚  â”‚     hub.broadcast(llm_stream_chunk)          â”‚            â”‚
â”‚  â”‚  3. rtdb.set(final_message) â† 1 write only   â”‚            â”‚
â”‚  â”‚  4. hub.broadcast(llm_stream_complete)       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ APIs RPC Disponibles

### LLM.send_message

Envoie un message et dÃ©marre le streaming via WebSocket.

```python
response = rpc_client.call(
    method="LLM.send_message",
    kwargs={
        "user_id": "user123",
        "collection_name": "company456",
        "thread_key": "thread789",
        "message": "Bonjour",
        "chat_mode": "general_chat",
        "system_prompt": "Tu es un assistant...",
        "selected_tool": None
    }
)
```

**RÃ©ponse :**
```json
{
  "success": true,
  "user_message_id": "msg-user-uuid",
  "assistant_message_id": "msg-assistant-uuid",
  "ws_channel": "chat:user123:company456:thread789",
  "message": "Message envoyÃ©, rÃ©ponse en cours de streaming via WebSocket"
}
```

### LLM.stop_streaming

Interrompt un streaming en cours.

```python
response = rpc_client.call(
    method="LLM.stop_streaming",
    kwargs={
        "user_id": "user123",
        "collection_name": "company456",
        "thread_key": "thread789"  # Optionnel
    }
)
```

**RÃ©ponse :**
```json
{
  "success": true,
  "message": "Stream arrÃªtÃ© pour thread thread789",
  "thread_key": "thread789"
}
```

---

## âš¡ Avantages de cette Architecture

| CritÃ¨re | RTDB (Ancien) | WebSocket (Nouveau) |
|---------|---------------|---------------------|
| **Latence chunk** | ~50-200ms | ~1-5ms âš¡ |
| **Ã‰critures Firebase** | ~50-100 par message | 1 seule âœ… |
| **CoÃ»t Firebase** | Ã‰levÃ© ğŸ’¸ | Minimal ğŸ’° |
| **FluiditÃ© UX** | SaccadÃ© ğŸ˜• | Fluide comme ChatGPT ğŸ¯ |
| **ComplexitÃ©** | Buffer + Debounce | Direct ğŸš€ |
| **Historique** | âœ… PersistÃ© | âœ… PersistÃ© (final) |
| **Scaling** | LimitÃ© | Excellent |

---

## ğŸ”’ Gestion des DÃ©connexions

### Auto-reconnexion recommandÃ©e

```python
class WebSocketManager:
    async def connect(self):
        """Connexion avec auto-reconnexion"""
        while True:
            try:
                await self._connect_websocket()
                break  # SuccÃ¨s
            except Exception as e:
                logger.error(f"Erreur connexion WebSocket: {e}")
                await asyncio.sleep(2)  # Backoff
    
    async def on_disconnect(self):
        """Gestion de dÃ©connexion"""
        logger.info("WebSocket dÃ©connectÃ©, tentative de reconnexion...")
        await self.connect()
```

### RÃ©cupÃ©ration aprÃ¨s dÃ©connexion

Si le WebSocket se dÃ©connecte pendant un streaming :

1. **Tentative de reconnexion automatique**
2. **Rechargement depuis RTDB** : Le message final sera dans RTDB une fois le streaming terminÃ©
3. **VÃ©rification des messages manquÃ©s** : Comparer les `message_id` locaux avec RTDB

---

## ğŸ“Š Monitoring & Debugging

### Logs cÃ´tÃ© Backend

```python
# Les logs suivants sont automatiquement gÃ©nÃ©rÃ©s :
logger.info(f"Traitement message avec streaming WebSocket pour thread: {thread_key}")
logger.info(f"Canal WebSocket: {ws_channel}")
logger.info(f"Chunk #{chunk_count} reÃ§u: '{chunk_content[:50]}...'")
logger.info(f"Streaming terminÃ©. Total chunks: {chunk_count}")
```

### Logs cÃ´tÃ© Reflex (recommandÃ©s)

```python
def _handle_stream_chunk(self, payload: dict):
    logger.debug(f"WS chunk reÃ§u : {payload['message_id'][:8]}... | {len(payload['accumulated'])} chars")
    self.streaming_message.content = payload["accumulated"]
```

---

## ğŸ¯ Checklist de Migration

- [ ] Connecter WebSocket avec `uid`, `space_code`, `thread_key`
- [ ] ImplÃ©menter `handle_websocket_event()` pour gÃ©rer les 5 types d'Ã©vÃ©nements
- [ ] GÃ©rer `streaming_message` temporaire pendant le streaming
- [ ] Convertir en message permanent Ã  la fin (`llm_stream_complete`)
- [ ] GÃ©rer les interruptions (`llm_stream_interrupted`)
- [ ] GÃ©rer les erreurs (`llm_stream_error`)
- [ ] Conserver l'Ã©coute RTDB pour l'historique et les messages non-LLM
- [ ] ImplÃ©menter auto-reconnexion WebSocket
- [ ] Tester la rÃ©cupÃ©ration aprÃ¨s dÃ©connexion
- [ ] Monitorer les performances (latence, chunks/sec)

---

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- Consulter les logs backend avec `LISTENERS_DEBUG=true`
- VÃ©rifier la connexion WebSocket dans les dev tools du navigateur
- VÃ©rifier que le `ws_channel` correspond au format attendu

---

**Version** : 1.0.0  
**Date** : Octobre 2025  
**Auteur** : Ã‰quipe Backend Python

