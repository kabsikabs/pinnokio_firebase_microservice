# IntÃ©gration LLM avec Reflex

Ce document dÃ©crit comment modifier l'application Reflex pour utiliser le service LLM du microservice via RPC.

## ğŸ¯ Objectif

Rediriger toutes les interactions LLM de `ChatState` vers le microservice, sans changer la logique mÃ©tier de Reflex, uniquement la couche de communication.

## ğŸ“‹ Modifications nÃ©cessaires

### 1. Modifier `ChatState.initialize_llm_agent()`

**Fichier:** `C:\Users\Cedri\Coding\pinnokio_app\pinnokio_app\state\base_state.py`

**Avant (code actuel):**
```python
async def initialize_llm_agent(self):
    """Initialise l'instance LLM avec le contexte utilisateur."""
    try:
        if not self._llm_instance:
            # CrÃ©ation directe de BaseAIAgent
            instance = BaseAIAgent(
                collection_name=self.user_info.collection_name,
                dms_system=self.user_info.dms_system or "google_drive",
                dms_mode=self.user_info.dms_mode or "prod",
                firebase_user_id=self.user_info.firebase_user_id,
                chat_instance=None,
                job_id=None
            )
            
            LLMSingleton.initialize(instance)
            self._llm_instance = LLMSingleton.get_instance()
        
        # Mise Ã  jour du prompt systÃ¨me
        if self._llm_instance and hasattr(self, 'dms_system_prompt'):
            self._llm_instance.update_system_prompt(self.dms_system_prompt)
        
        return True
    except Exception as e:
        logger.error(f"Erreur initialisation LLM: {e}")
        return False
```

**AprÃ¨s (avec RPC):**
```python
async def initialize_llm_agent(self):
    """Initialise l'instance LLM via le microservice."""
    try:
        from .manager import get_manager
        
        # Appel RPC au microservice pour initialiser la session LLM
        result = await get_manager().rpc_call(
            method="LLM.initialize_session",
            args={
                "user_id": self.user_info.firebase_user_id,
                "collection_name": self.user_info.collection_name,
                "dms_system": self.user_info.dms_system or "google_drive",
                "dms_mode": self.user_info.dms_mode or "prod",
                "chat_mode": self.chat_mode or "general_chat"
            }
        )
        
        if result.get("success"):
            self._llm_session_id = result.get("session_id")
            logger.info(f"Session LLM initialisÃ©e: {self._llm_session_id}")
            
            # Marquer comme initialisÃ© (pour compatibilitÃ© avec le code existant)
            # Note: On ne stocke plus l'instance localement, tout est cÃ´tÃ© microservice
            return True
        else:
            logger.error(f"Ã‰chec initialisation LLM: {result.get('error')}")
            return False
            
    except Exception as e:
        logger.error(f"Erreur initialisation LLM: {e}")
        return False
```

### 2. Modifier `ChatState.send_message()`

**Avant (code actuel):**
```python
async def send_message(self, form_data: dict):
    """Envoie un message au LLM."""
    message = form_data.get("message", "").strip()
    if not message:
        return
    
    # Ajouter message utilisateur (UI optimiste)
    user_msg = {
        "role": "user",
        "content": message,
        "timestamp": datetime.now().isoformat()
    }
    self.messages.append(user_msg)
    
    # Appeler le LLM local
    llm_instance = self._llm_instance
    if llm_instance:
        response = llm_instance.process_text(
            content=message,
            provider=ModelProvider.ANTHROPIC,
            size=ModelSize.MEDIUM
        )
        
        # Ajouter rÃ©ponse
        assistant_msg = {
            "role": "assistant",
            "content": response.get("text_output", {}).get("content", {}).get("answer_text", ""),
            "timestamp": datetime.now().isoformat()
        }
        self.messages.append(assistant_msg)
```

**AprÃ¨s (avec RPC + Firebase RTDB):**
```python
async def send_message(self, form_data: dict):
    """Envoie un message au LLM via le microservice."""
    message = form_data.get("message", "").strip()
    if not message:
        return
    
    try:
        from .manager import get_manager
        
        # Appel RPC au microservice
        # Le microservice Ã©crira directement dans Firebase RTDB
        # Le listener RTDB de Reflex mettra Ã  jour l'UI automatiquement
        result = await get_manager().rpc_call(
            method="LLM.send_message",
            args={
                "user_id": self.user_info.firebase_user_id,
                "collection_name": self.user_info.collection_name,
                "space_code": self.user_info.space_code,
                "chat_thread": self.current_thread_key,
                "message": message,
                "chat_mode": self.chat_mode or "general_chat",
                "system_prompt": self.dms_system_prompt if hasattr(self, 'dms_system_prompt') else None
            }
        )
        
        if result.get("success"):
            logger.info(f"Message envoyÃ© au microservice: {result.get('assistant_message_id')}")
            # Note: Pas besoin d'ajouter aux messages ici
            # Le listener RTDB _handle_chat_message() s'en chargera automatiquement
        else:
            logger.error(f"Ã‰chec envoi message: {result.get('error')}")
            # Afficher une erreur Ã  l'utilisateur
            yield rx.toast.error(f"Erreur: {result.get('error')}")
            
    except Exception as e:
        logger.error(f"Erreur envoi message LLM: {e}")
        yield rx.toast.error(f"Erreur de communication: {str(e)}")
```

### 3. Le listener RTDB `_handle_chat_message()` reste INCHANGÃ‰

Le listener Firebase RTDB existant dans `ChatState` continue de fonctionner exactement comme avant:

```python
def _handle_chat_message(self, event):
    """GÃ©rÃ© automatiquement par le listener RTDB.
    ReÃ§oit les messages (user + assistant streaming) depuis Firebase RTDB.
    """
    # Ce code reste identique - il Ã©coute dÃ©jÃ  Firebase RTDB
    # et met Ã  jour self.messages automatiquement
    pass
```

**Pourquoi Ã§a fonctionne:**
- Le microservice Ã©crit dans `{space_code}/chats/{thread_key}/messages/`
- Le `ChatListener` de Reflex Ã©coute dÃ©jÃ  ce chemin
- Quand un nouveau message arrive (user ou assistant), le listener le dÃ©tecte
- Il appelle `_handle_chat_message()` qui met Ã  jour l'UI

## ğŸ”„ Flux de communication complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reflex (UI)    â”‚
â”‚   ChatState     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ 1. send_message()
         â”‚    via RPC: LLM.send_message
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Microservice   â”‚
â”‚   LLMManager    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ 2. Ã‰crit message user dans Firebase RTDB
         â”‚    {space_code}/chats/{thread}/messages/{user_msg_id}
         â”‚
         â”‚ 3. Traite avec BaseAIAgent
         â”‚
         â”‚ 4. Stream rÃ©ponse assistant dans Firebase RTDB
         â”‚    {space_code}/chats/{thread}/messages/{assistant_msg_id}
         â”‚    (mise Ã  jour toutes les 100ms)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Firebase RTDB   â”‚
â”‚   Messages      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ 5. Listener RTDB dÃ©tecte les nouveaux messages
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reflex (UI)    â”‚
â”‚ _handle_chat_   â”‚
â”‚    message()    â”‚
â”‚                 â”‚
â”‚ Met Ã  jour UI   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Tests

### Test 1: VÃ©rifier la connexion RPC

```python
# Dans le microservice
python test_llm_connection.py
```

### Test 2: Tester depuis Reflex

```python
# Dans l'application Reflex, ajouter un test dans ChatState
async def test_llm_connection(self):
    """Test de connexion LLM avec le microservice."""
    from .manager import get_manager
    
    result = await get_manager().rpc_call(
        method="LLM.initialize_session",
        args={
            "user_id": "test_user",
            "collection_name": "test_company",
            "dms_system": "google_drive"
        }
    )
    
    print(f"RÃ©sultat: {result}")
    return result.get("success", False)
```

## ğŸ“ Variables d'Ã©tat Ã  ajouter dans `ChatState`

```python
class ChatState(rx.State):
    # ... code existant ...
    
    # ğŸ†• NOUVEAU: ID de session LLM cÃ´tÃ© microservice
    _llm_session_id: Optional[str] = None
    
    # ... reste du code ...
```

## âš™ï¸ Configuration requise

### Dans le microservice

Aucune configuration supplÃ©mentaire requise. Le service LLM utilise:
- Firebase RTDB (dÃ©jÃ  configurÃ©)
- BaseAIAgent (dÃ©jÃ  dans `app/llm/klk_agents.py`)
- RPC existant

### Dans Reflex

S'assurer que le `ListenerManager` de Reflex est correctement configurÃ© pour:
1. Ã‰couter `{space_code}/chats/{thread_key}/messages/`
2. Appeler `_handle_chat_message()` sur nouveaux messages

## ğŸš€ DÃ©ploiement

### Ã‰tape 1: DÃ©ployer le microservice

```bash
# Le microservice inclut maintenant le service LLM
# Pas de changement dans le Dockerfile
docker build -t firebase-microservice .
docker push ...
```

### Ã‰tape 2: Mettre Ã  jour Reflex

```bash
# Modifier les 2 mÃ©thodes dans base_state.py
# Tester localement
reflex run
```

### Ã‰tape 3: VÃ©rifier

1. Ouvrir l'application Reflex
2. Initialiser un chat
3. Envoyer un message
4. VÃ©rifier que la rÃ©ponse apparaÃ®t (streaming)
5. VÃ©rifier les logs du microservice

## ğŸ” Debug

### Logs microservice

```bash
# VÃ©rifier l'initialisation
docker logs <container> | grep "Session LLM initialisÃ©e"

# VÃ©rifier les messages
docker logs <container> | grep "Message assistant complÃ©tÃ©"
```

### Logs Reflex

```python
# Dans ChatState
import logging
logger = logging.getLogger("reflex.chat")

# Dans send_message()
logger.info(f"Envoi message au microservice: user={self.user_info.firebase_user_id}")
```

## âœ… Checklist

- [ ] Modifier `initialize_llm_agent()` dans `ChatState`
- [ ] Modifier `send_message()` dans `ChatState`
- [ ] Ajouter `_llm_session_id` Ã  `ChatState`
- [ ] Tester connexion RPC (test_llm_connection.py)
- [ ] Tester depuis Reflex
- [ ] VÃ©rifier listeners RTDB
- [ ] VÃ©rifier streaming des rÃ©ponses
- [ ] DÃ©ployer en production

## ğŸ“ Notes importantes

1. **Pas d'UI optimiste nÃ©cessaire**: Le microservice Ã©crit directement dans RTDB, le listener met Ã  jour l'UI immÃ©diatement

2. **Streaming automatique**: Le buffer intelligent du microservice optimise les Ã©critures RTDB (100ms)

3. **Session rÃ©utilisable**: Une session LLM par `user_id:collection_name`, partagÃ©e entre tous les threads

4. **Backward compatible**: Si le microservice est indisponible, on peut ajouter un fallback vers l'ancien systÃ¨me

## ğŸ”® Prochaines Ã©tapes

Une fois la connexion de base Ã©tablie:
1. Ajouter le framework agentic (SPT/LPT)
2. GÃ©rer les Ã©vÃ©nements systÃ¨me (thinking, tool execution)
3. Ajouter les quotas/limites de tÃ¢ches
4. Monitoring et mÃ©triques


