# ü§ñ Architecture LLM Microservice - Documentation Technique (FINALE)

## üìã Vue d'ensemble

Cette architecture permet de **d√©placer toute la logique LLM** de l'application Reflex vers le microservice Firebase, tout en **maintenant une compatibilit√© totale** avec le code existant c√¥t√© Reflex.

**‚úÖ D√âCISIONS VALID√âES :**
1. **Communication** : Firebase Realtime Database (comme les chats existants)
2. **Path RTDB** : `{space_code}/chats/{thread_key}/messages/` (nouveau chemin d√©di√©)
3. **Streaming** : Update toutes les 100ms avec debounce intelligent
4. **√âv√©nements syst√®me** : Firebase RTDB avec champ `metadata` et `role: system`
5. **Agent** : `BaseAIAgent` d√©j√† pr√©sent dans `app/llm/klk_agents.py`

---

## üéØ Objectifs

1. ‚úÖ **D√©placer le LLM** : Toute la logique `BaseAIAgent` vers le microservice
2. ‚úÖ **Isolation parfaite** : Par `user_id` + `collection_name` + `chat_thread`
3. ‚úÖ **Communication Firebase RTDB** : R√©utilise l'infrastructure existante (ChatListener)
4. ‚úÖ **Z√©ro changement Reflex** : Seule la communication change, pas l'API
5. ‚úÖ **Framework agentic** : Support SPT (Short Process Tooling) et LPT (Long Process Tooling)
6. ‚úÖ **Scalabilit√©** : Gestion de milliers de conversations simultan√©es

---

## üèóÔ∏è Architecture Technique

### **1. Vue d'ensemble du flux de communication**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         APPLICATION REFLEX (INCHANG√âE)                      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ChatState                                                                  ‚îÇ
‚îÇ  ‚îú‚îÄ question: str                                                           ‚îÇ
‚îÇ  ‚îú‚îÄ processing: bool                                                        ‚îÇ
‚îÇ  ‚îú‚îÄ chats: Dict[str, List[QA]]                                             ‚îÇ
‚îÇ  ‚îî‚îÄ Methods:                                                                ‚îÇ
‚îÇ     ‚îú‚îÄ send_message(question: str)  ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ     ‚îú‚îÄ _handle_chat_message(...)      ‚îÇ (d√©j√† existant)                   ‚îÇ
‚îÇ     ‚îî‚îÄ update_chat_display(...)       ‚îÇ                                    ‚îÇ
‚îÇ                                        ‚îÇ                                    ‚îÇ
‚îÇ  ChatListener (Firebase RTDB)         ‚îÇ                                    ‚îÇ
‚îÇ  ‚îú‚îÄ √âcoute: {space_code}/chats/{thread}/messages/                         ‚îÇ
‚îÇ  ‚îî‚îÄ Callback: _handle_chat_message()  ‚îÇ                                    ‚îÇ
‚îÇ                                        ‚îÇ                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                         ‚îÇ
                                         ‚îÇ RPC Call
                                         ‚îÇ rpc_call("LLM.send_message", args=[...])
                                         ‚îÇ
                                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          MICROSERVICE FIREBASE                              ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  main.py - RPC Handler                                           ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ @app.post("/rpc")                                            ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ _resolve_method("LLM.*") ‚Üí LLM Manager                       ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  llm_service/llm_manager.py - LLM Service Manager                ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ send_message() ‚Üí √âcrit dans Firebase RTDB                    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ get_or_create_session()                                      ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ _process_message_with_rtdb_streaming()                       ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  llm_service/llm_session.py - Session LLM Isol√©e                 ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Namespace: {user_id}:{collection_name}                          ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ agent: BaseAIAgent (klk_agents.py)                          ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ conversations: Dict[thread_key, List[Message]]               ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ active_tasks: Dict[thread_key, List[TaskID]]                ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ process_message_streaming() ‚Üí async generator               ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Firebase Realtime Database                                      ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  {space_code}/chats/{thread_key}/messages/                       ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ {msg_id_1}: {role: "user", content: "..."}                  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ {msg_id_2}: {role: "assistant", content: "...",             ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                status: "streaming", streaming_progress: 0.45} ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ {msg_id_3}: {role: "system", type: "tool_execution", ...}   ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                                     ‚îÇ Firebase RTDB Listener (d√©j√† actif)
                                     ‚îÇ ChatListener.on_event()
                                     ‚îÇ
                                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    REFLEX - ChatState._handle_chat_message()                ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  D√©tecte automatiquement :                                                  ‚îÇ
‚îÇ  ‚îú‚îÄ Nouveaux messages (role: user/assistant)                               ‚îÇ
‚îÇ  ‚îú‚îÄ Updates streaming (content mis √† jour progressivement)                 ‚îÇ
‚îÇ  ‚îú‚îÄ Statuts (streaming ‚Üí complete ‚Üí error)                                 ‚îÇ
‚îÇ  ‚îî‚îÄ Messages syst√®me (tool_execution, long_task, etc.)                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚Üí UI se met √† jour automatiquement en temps r√©el ‚úÖ                        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä **Structure Firebase Realtime Database**

### **1. Path pour les conversations LLM**

```
{space_code}/                        # Collection name (soci√©t√©)
  ‚îî‚îÄ chats/                          # ‚úÖ Nouveau chemin d√©di√© aux conversations LLM
      ‚îî‚îÄ {thread_key}/               # Thread de conversation
          ‚îî‚îÄ messages/
              ‚îú‚îÄ {message_id_1}/
              ‚îÇ   ‚îú‚îÄ role: "user"
              ‚îÇ   ‚îú‚îÄ content: "Comment analyser cette facture ?"
              ‚îÇ   ‚îú‚îÄ timestamp: "2025-10-10T12:34:56Z"
              ‚îÇ   ‚îú‚îÄ user_id: "user_abc123"
              ‚îÇ   ‚îî‚îÄ read: false
              ‚îÇ
              ‚îú‚îÄ {message_id_2}/
              ‚îÇ   ‚îú‚îÄ role: "assistant"
              ‚îÇ   ‚îú‚îÄ content: "Je vais analyser..."  # ‚úÖ Mis √† jour progressivement (streaming)
              ‚îÇ   ‚îú‚îÄ timestamp: "2025-10-10T12:35:02Z"
              ‚îÇ   ‚îú‚îÄ status: "streaming" | "complete" | "error"
              ‚îÇ   ‚îú‚îÄ streaming_progress: 0.75  # Pour barre de progression
              ‚îÇ   ‚îú‚îÄ last_update: "2025-10-10T12:35:03.245Z"
              ‚îÇ   ‚îî‚îÄ metadata:
              ‚îÇ       ‚îú‚îÄ tokens_used: {prompt: 150, completion: 320, total: 470}
              ‚îÇ       ‚îú‚îÄ tools_called: ["read_document", "analyze_invoice"]
              ‚îÇ       ‚îú‚îÄ duration_ms: 3420
              ‚îÇ       ‚îî‚îÄ model: "claude-3-7-sonnet-20250219"
              ‚îÇ
              ‚îú‚îÄ {message_id_3}/  # ‚úÖ Message syst√®me pour tool execution
              ‚îÇ   ‚îú‚îÄ role: "system"
              ‚îÇ   ‚îú‚îÄ type: "tool_execution"
              ‚îÇ   ‚îú‚îÄ content: "üîß Lecture du document invoice_2025.pdf..."
              ‚îÇ   ‚îú‚îÄ timestamp: "2025-10-10T12:35:15Z"
              ‚îÇ   ‚îú‚îÄ ephemeral: true  # Supprim√© apr√®s traitement
              ‚îÇ   ‚îî‚îÄ metadata:
              ‚îÇ       ‚îú‚îÄ tool_name: "read_document"
              ‚îÇ       ‚îú‚îÄ tool_args: {document_id: "doc_456"}
              ‚îÇ       ‚îú‚îÄ status: "running" | "complete" | "error"
              ‚îÇ       ‚îî‚îÄ duration_ms: 2100
              ‚îÇ
              ‚îî‚îÄ {message_id_4}/  # ‚úÖ Message syst√®me pour LPT
                  ‚îú‚îÄ role: "system"
                  ‚îú‚îÄ type: "long_task"
                  ‚îú‚îÄ content: "üìä Rapprochement comptable lanc√© (environ 1h)..."
                  ‚îú‚îÄ timestamp: "2025-10-10T12:35:25Z"
                  ‚îú‚îÄ persistent: true  # Gard√© dans l'historique
                  ‚îî‚îÄ metadata:
                      ‚îú‚îÄ task_id: "lpt_accounting_12345"
                      ‚îú‚îÄ task_type: "accounting_reconciliation"
                      ‚îú‚îÄ status: "queued" | "processing" | "complete" | "error"
                      ‚îú‚îÄ progress_percent: 35
                      ‚îú‚îÄ current_step: "Analyse des transactions (2/5)"
                      ‚îî‚îÄ estimated_completion: "2025-10-10T13:35:00Z"
```

---

## üîß **Impl√©mentation Microservice**

### **1. Service LLM Manager - Version Firebase RTDB**

```python
# app/llm_service/llm_manager.py

import asyncio
import json
import uuid
import time
from typing import Dict, Optional, Any
from datetime import datetime, timezone
from ..llm.klk_agents import BaseAIAgent, ModelProvider, ModelSize
from .llm_context import LLMContext

class RTDBStreamingBuffer:
    """Buffer intelligent pour optimiser les √©critures Firebase RTDB (100ms debounce)."""
    
    def __init__(self, min_interval_ms: int = 100, max_buffer_size: int = 50):
        self.min_interval_ms = min_interval_ms
        self.max_buffer_size = max_buffer_size
        self.buffer = ""
        self.last_write_time = 0
        self.pending_task = None
        self.accumulated_content = ""
    
    async def add_chunk(self, chunk: str, rtdb_ref, force_flush: bool = False):
        """Ajoute un chunk et flush intelligemment."""
        self.buffer += chunk
        self.accumulated_content += chunk
        current_time = time.time() * 1000  # ms
        
        # Conditions de flush :
        # 1. Intervalle minimum atteint
        # 2. Buffer plein (pour ne pas accumuler trop)
        # 3. Force flush (dernier chunk)
        should_flush = (
            (current_time - self.last_write_time) >= self.min_interval_ms or
            len(self.buffer) >= self.max_buffer_size or
            force_flush
        )
        
        if should_flush:
            await self._flush(rtdb_ref)
        else:
            # Planifier un flush automatique si rien ne vient
            if self.pending_task:
                self.pending_task.cancel()
            self.pending_task = asyncio.create_task(
                self._auto_flush(rtdb_ref, self.min_interval_ms / 1000)
            )
    
    async def _flush(self, rtdb_ref):
        """Flush le buffer vers Firebase RTDB."""
        if not self.buffer:
            return
        
        try:
            rtdb_ref.update({
                "content": self.accumulated_content,
                "last_update": datetime.now(timezone.utc).isoformat()
            })
            
            self.buffer = ""
            self.last_write_time = time.time() * 1000
        except Exception as e:
            print(f"‚ùå Erreur flush RTDB: {e}")
    
    async def _auto_flush(self, rtdb_ref, delay: float):
        """Flush automatique apr√®s un d√©lai."""
        try:
            await asyncio.sleep(delay)
            await self._flush(rtdb_ref)
        except asyncio.CancelledError:
            pass


class LLMSession:
    """Session LLM isol√©e pour un utilisateur/soci√©t√©.
    
    G√®re l'agent BaseAIAgent et l'historique des conversations pour tous les threads
    de cet utilisateur dans cette soci√©t√©.
    """
    
    def __init__(self, session_key: str, context: LLMContext):
        self.session_key = session_key  # user_id:collection_name
        self.context = context
        self.agent: Optional[BaseAIAgent] = None
        
        # Historique par thread de conversation
        self.conversations: Dict[str, List[dict]] = {}
        
        # T√¢ches actives par thread
        self.active_tasks: Dict[str, List[str]] = {}
        
        # √âtat par thread
        self.thread_states: Dict[str, str] = {}
        
        # M√©triques
        self.created_at = datetime.now(timezone.utc)
        self.last_activity: Dict[str, datetime] = {}
        self.response_times: Dict[str, List[float]] = {}
    
    async def initialize_agent(self):
        """Initialise l'agent BaseAIAgent avec le contexte."""
        try:
            print(f"üöÄ Initialisation BaseAIAgent pour session {self.session_key}")
            
            # Initialiser BaseAIAgent avec les param√®tres du contexte
            self.agent = BaseAIAgent(
                collection_name=self.context.collection_name,
                dms_system=self.context.dms_system,
                dms_mode=self.context.dms_mode,
                firebase_user_id=self.context.user_id
            )
            
            # Enregistrer les providers par d√©faut (√† adapter selon vos besoins)
            # Exemple : Anthropic
            from ..llm.klk_agents import Anthropic_Agent
            anthropic_instance = Anthropic_Agent()
            self.agent.register_provider(ModelProvider.ANTHROPIC, anthropic_instance)
            
            # Vous pouvez ajouter d'autres providers ici
            # from ..llm.klk_agents import OpenAI_Agent
            # openai_instance = OpenAI_Agent()
            # self.agent.register_provider(ModelProvider.OPENAI, openai_instance)
            
            print(f"‚úÖ Agent LLM initialis√© pour session {self.session_key}")
            
        except Exception as e:
            print(f"‚ùå Erreur initialisation agent: {e}")
            raise
    
    def update_context(self, **kwargs):
        """Met √† jour le contexte dynamiquement."""
        for key, value in kwargs.items():
            if hasattr(self.context, key):
                setattr(self.context, key, value)
        
        # Si DMS change, r√©initialiser l'agent
        if 'dms_system' in kwargs or 'dms_mode' in kwargs:
            if self.agent:
                self.agent._initialize_dms(
                    self.context.dms_mode,
                    self.context.dms_system,
                    self.context.user_id
                )
    
    def add_user_message(self, thread_key: str, message: str):
        """Ajoute un message utilisateur √† l'historique d'un thread."""
        if thread_key not in self.conversations:
            self.conversations[thread_key] = []
        
        self.conversations[thread_key].append({
            "role": "user",
            "content": message,
            "timestamp": datetime.now(timezone.utc).isoformat()
        })
        
        self.last_activity[thread_key] = datetime.now(timezone.utc)
    
    async def process_message_streaming(
        self,
        thread_key: str,
        message: str,
        system_prompt: str = None
    ):
        """Traite un message et yield les chunks de r√©ponse.
        
        Yields:
            dict: {"content": str, "index": int, "is_final": bool, "tool_calls": list}
        """
        try:
            self.thread_states[thread_key] = "processing"
            start_time = datetime.now(timezone.utc)
            
            # Mettre √† jour le prompt syst√®me si fourni
            if system_prompt and self.agent:
                self.agent.update_system_prompt(system_prompt)
            
            # ‚úÖ Appeler BaseAIAgent pour traiter le message
            # Note: BaseAIAgent n'a pas de m√©thode streaming native, donc on va
            # simuler un streaming en envoyant la r√©ponse par chunks
            
            if not self.agent:
                raise Exception("Agent non initialis√©")
            
            # Utiliser process_text avec le provider et size par d√©faut
            response = self.agent.process_text(
                content=message,
                provider=self.agent.default_provider or ModelProvider.ANTHROPIC,
                size=self.agent.default_model_size or ModelSize.MEDIUM
            )
            
            # Extraire le texte de la r√©ponse
            response_text = ""
            if isinstance(response, dict):
                if 'text_output' in response:
                    text_output = response.get('text_output', {})
                    if isinstance(text_output, dict):
                        content = text_output.get('content', {})
                        if isinstance(content, dict):
                            response_text = content.get('answer_text', '')
                        else:
                            response_text = str(content)
                    else:
                        response_text = str(text_output)
                else:
                    response_text = str(response)
            else:
                response_text = str(response)
            
            # Simuler un streaming en envoyant la r√©ponse par chunks
            chunk_size = 5  # Nombre de caract√®res par chunk
            total_chars = len(response_text)
            
            for i in range(0, total_chars, chunk_size):
                chunk = response_text[i:i+chunk_size]
                yield {
                    "content": chunk,
                    "index": i // chunk_size,
                    "is_final": (i + chunk_size >= total_chars),
                    "tool_calls": None
                }
                await asyncio.sleep(0.01)  # Petit d√©lai pour simuler le streaming
            
            # Ajouter r√©ponse √† l'historique
            self.conversations[thread_key].append({
                "role": "assistant",
                "content": response_text,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })
            
            # M√©triques
            duration_ms = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
            if thread_key not in self.response_times:
                self.response_times[thread_key] = []
            self.response_times[thread_key].append(duration_ms)
            
            self.thread_states[thread_key] = "idle"
            
        except Exception as e:
            self.thread_states[thread_key] = "error"
            print(f"‚ùå Erreur process_message_streaming: {e}")
            raise
    
    def get_token_stats(self, thread_key: str) -> dict:
        """Retourne les stats de tokens depuis BaseAIAgent."""
        if not self.agent:
            return {"prompt": 0, "completion": 0, "total": 0}
        
        try:
            # BaseAIAgent a une m√©thode get_token_usage_by_provider()
            usage = self.agent.get_token_usage_by_provider()
            
            # Agr√©ger les stats de tous les providers
            total_input = sum(p.get('total_input_tokens', 0) for p in usage.values())
            total_output = sum(p.get('total_output_tokens', 0) for p in usage.values())
            
            return {
                "prompt": total_input,
                "completion": total_output,
                "total": total_input + total_output
            }
        except Exception:
            return {"prompt": 0, "completion": 0, "total": 0}
    
    def get_last_response_duration_ms(self, thread_key: str) -> int:
        """Retourne la dur√©e de la derni√®re r√©ponse en ms."""
        if thread_key in self.response_times and self.response_times[thread_key]:
            return int(self.response_times[thread_key][-1])
        return 0


class LLMManager:
    """Gestionnaire LLM utilisant Firebase Realtime Database."""
    
    def __init__(self):
        self.sessions: Dict[str, LLMSession] = {}
        self._lock = asyncio.Lock()
    
    def _get_rtdb_ref(self, path: str):
        """Obtient une r√©f√©rence Firebase RTDB."""
        from ..listeners_manager import _get_rtdb_ref
        return _get_rtdb_ref(path)
    
    async def initialize_session(
        self,
        user_id: str,
        collection_name: str,
        dms_system: str = "google_drive",
        dms_mode: str = "prod",
        chat_mode: str = "general_chat"
    ) -> dict:
        """Initialise une session LLM pour un utilisateur/soci√©t√©."""
        try:
            async with self._lock:
                base_session_key = f"{user_id}:{collection_name}"
                
                # V√©rifier si session existe d√©j√†
                if base_session_key in self.sessions:
                    session = self.sessions[base_session_key]
                    # Mettre √† jour le contexte si n√©cessaire
                    if (session.context.dms_system != dms_system or 
                        session.context.chat_mode != chat_mode):
                        session.update_context(
                            dms_system=dms_system,
                            dms_mode=dms_mode,
                            chat_mode=chat_mode
                        )
                    
                    return {
                        "success": True,
                        "session_id": base_session_key,
                        "status": "existing",
                        "message": "Session LLM r√©utilis√©e"
                    }
                
                # Cr√©er nouvelle session
                context = LLMContext(
                    user_id=user_id,
                    collection_name=collection_name,
                    dms_system=dms_system,
                    dms_mode=dms_mode,
                    chat_mode=chat_mode
                )
                
                session = LLMSession(
                    session_key=base_session_key,
                    context=context
                )
                
                # Initialiser l'agent
                await session.initialize_agent()
                
                # Stocker en cache
                self.sessions[base_session_key] = session
                
                return {
                    "success": True,
                    "session_id": base_session_key,
                    "status": "created",
                    "message": "Session LLM initialis√©e avec succ√®s"
                }
                
        except Exception as e:
            print(f"‚ùå Erreur initialisation session LLM: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "√âchec de l'initialisation LLM"
            }
    
    async def send_message(
        self,
        user_id: str,
        collection_name: str,
        space_code: str,      # ‚úÖ Pour path RTDB
        chat_thread: str,
        message: str,
        chat_mode: str = "general_chat",
        system_prompt: str = None
    ) -> dict:
        """Envoie un message √† l'agent LLM et √©crit la r√©ponse dans Firebase RTDB."""
        try:
            base_session_key = f"{user_id}:{collection_name}"
            
            # R√©cup√©rer ou cr√©er la session
            async with self._lock:
                if base_session_key not in self.sessions:
                    init_result = await self.initialize_session(
                        user_id, collection_name, chat_mode=chat_mode
                    )
                    if not init_result.get("success"):
                        return init_result
                
                session = self.sessions[base_session_key]
            
            # G√©n√©rer IDs pour les messages
            user_message_id = str(uuid.uuid4())
            assistant_message_id = str(uuid.uuid4())
            
            # ‚úÖ 1. √âcrire le message utilisateur dans Firebase RTDB
            user_msg_path = f"{space_code}/chats/{chat_thread}/messages/{user_message_id}"
            user_msg_ref = self._get_rtdb_ref(user_msg_path)
            user_msg_ref.set({
                "role": "user",
                "content": message,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "user_id": user_id,
                "read": False
            })
            
            # ‚úÖ 2. Cr√©er un message assistant "vide" (pour le streaming)
            assistant_msg_path = f"{space_code}/chats/{chat_thread}/messages/{assistant_message_id}"
            assistant_msg_ref = self._get_rtdb_ref(assistant_msg_path)
            assistant_msg_ref.set({
                "role": "assistant",
                "content": "",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "status": "streaming",
                "streaming_progress": 0.0,
                "read": False
            })
            
            # ‚úÖ 3. Lancer le traitement en arri√®re-plan
            asyncio.create_task(
                self._process_message_with_rtdb_streaming(
                    session=session,
                    user_id=user_id,
                    space_code=space_code,
                    chat_thread=chat_thread,
                    assistant_message_id=assistant_message_id,
                    message=message,
                    system_prompt=system_prompt
                )
            )
            
            return {
                "success": True,
                "user_message_id": user_message_id,
                "assistant_message_id": assistant_message_id,
                "message": "Message envoy√©, r√©ponse en cours de streaming dans Firebase RTDB"
            }
            
        except Exception as e:
            print(f"‚ùå Erreur envoi message LLM: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _process_message_with_rtdb_streaming(
        self,
        session: LLMSession,
        user_id: str,
        space_code: str,
        chat_thread: str,
        assistant_message_id: str,
        message: str,
        system_prompt: str = None
    ):
        """Traite le message et stream la r√©ponse directement dans Firebase RTDB."""
        
        assistant_msg_path = f"{space_code}/chats/{chat_thread}/messages/{assistant_message_id}"
        assistant_msg_ref = self._get_rtdb_ref(assistant_msg_path)
        
        try:
            # Cr√©er le buffer intelligent pour le streaming
            buffer = RTDBStreamingBuffer(min_interval_ms=100, max_buffer_size=50)
            
            # ‚úÖ Stream depuis l'agent LLM
            async for chunk in session.process_message_streaming(
                chat_thread, 
                message,
                system_prompt=system_prompt
            ):
                chunk_content = chunk.get("content", "")
                is_final = chunk.get("is_final", False)
                
                # Ajouter au buffer (flush automatique toutes les 100ms ou si buffer plein)
                await buffer.add_chunk(
                    chunk_content,
                    assistant_msg_ref,
                    force_flush=is_final
                )
            
            # ‚úÖ Finaliser le message
            assistant_msg_ref.update({
                "status": "complete",
                "streaming_progress": 1.0,
                "metadata": {
                    "tokens_used": session.get_token_stats(chat_thread),
                    "duration_ms": session.get_last_response_duration_ms(chat_thread),
                    "model": "claude-3-7-sonnet-20250219"  # √Ä r√©cup√©rer depuis l'agent
                },
                "completed_at": datetime.now(timezone.utc).isoformat()
            })
            
        except Exception as e:
            print(f"‚ùå Erreur streaming RTDB: {e}")
            # Marquer comme erreur dans Firebase RTDB
            assistant_msg_ref.update({
                "status": "error",
                "error": str(e),
                "error_at": datetime.now(timezone.utc).isoformat()
            })


# Singleton pour le gestionnaire LLM
_llm_manager: Optional[LLMManager] = None

def get_llm_manager() -> LLMManager:
    """R√©cup√®re l'instance singleton du LLM Manager."""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager
```

---

### **2. Contexte LLM dynamique**

```python
# app/llm_service/llm_context.py

from dataclasses import dataclass
from typing import Optional

@dataclass
class LLMContext:
    """Contexte dynamique pour une session LLM."""
    
    user_id: str
    collection_name: str
    dms_system: str = "google_drive"
    dms_mode: str = "prod"
    chat_mode: str = "general_chat"
    
    # Contexte m√©tier (optionnel, r√©cup√©r√© depuis Firestore)
    company_name: Optional[str] = None
    company_context: Optional[str] = None
    gl_accounting_erp: Optional[str] = None
    mandate_path: Optional[str] = None
    
    def to_dict(self) -> dict:
        """Convertit le contexte en dictionnaire."""
        return {
            "user_id": self.user_id,
            "collection_name": self.collection_name,
            "dms_system": self.dms_system,
            "dms_mode": self.dms_mode,
            "chat_mode": self.chat_mode,
            "company_name": self.company_name,
            "company_context": self.company_context,
            "gl_accounting_erp": self.gl_accounting_erp,
            "mandate_path": self.mandate_path
        }
```

---

### **3. Cr√©er les fichiers `__init__.py`**

```python
# app/llm_service/__init__.py

from .llm_manager import get_llm_manager, LLMManager
from .llm_context import LLMContext

__all__ = ['get_llm_manager', 'LLMManager', 'LLMContext']
```

---

## üîå **Int√©gration dans main.py**

```python
# app/main.py (modifications √† ajouter)

from .llm_service import get_llm_manager

# Dans _resolve_method() - Ajouter r√©solution des m√©thodes LLM
def _resolve_method(method: str) -> Tuple[Callable, str]:
    # ... code existant ...
    
    # üÜï Ajouter r√©solution des m√©thodes LLM
    if method.startswith("LLM."):
        name = method.split(".", 1)[1]
        llm_manager = get_llm_manager()
        target = getattr(llm_manager, name, None)
        if callable(target):
            return target, "LLM"
    
    # ... reste du code ...
```

---

## üì± **Modifications c√¥t√© Reflex (minimales)**

### **1. Mise √† jour de ChatState**

```python
# pinnokio_app/state/base_state.py

class ChatState(rx.State):
    # ... variables existantes INCHANG√âES ...
    
    @rx.event(background=True)
    async def initialize_llm_agent(self):
        """‚úÖ MODIFI√â : Initialise l'agent LLM via le microservice."""
        async with self:
            try:
                if self.llm_connected:
                    print("‚ö†Ô∏è LLM d√©j√† connect√©, initialisation ignor√©e")
                    return
                
                self.llm_init_inflight = True
                yield
                
                # ‚úÖ Appel RPC au microservice
                result = rpc_call(
                    "LLM.initialize_session",
                    args=[
                        self.firebase_user_id,
                        self.base_collection_id,
                        self.dms_type_extracted or "google_drive",
                        "prod",
                        self.chat_mode
                    ],
                    user_id=self.firebase_user_id,
                    timeout_ms=30000
                )
                
                if result and result.get("success"):
                    self.llm_connected = True
                    self.llm_params_fingerprint = result.get("session_id", "")
                    print(f"‚úÖ LLM initialis√© via microservice: {self.llm_params_fingerprint}")
                else:
                    error_msg = result.get("error", "Unknown error") if result else "No response"
                    print(f"‚ùå Erreur initialisation LLM: {error_msg}")
                    self.llm_connected = False
                
                self.llm_init_inflight = False
                yield
                
            except Exception as e:
                print(f"‚ùå Exception initialisation LLM: {e}")
                self.llm_connected = False
                self.llm_init_inflight = False
                yield
    
    @rx.event(background=True)
    async def send_message(self):
        """‚úÖ MODIFI√â : Envoie un message via le microservice (qui √©crit dans Firebase RTDB)."""
        async with self:
            if not self.question.strip():
                return
            
            try:
                # V√©rifier que l'agent est connect√©
                if not self.llm_connected:
                    print("‚ö†Ô∏è LLM non connect√©, initialisation...")
                    yield ChatState.initialize_llm_agent
                    
                    # Attendre fin d'initialisation
                    max_wait = 30
                    waited = 0
                    while self.llm_init_inflight and waited < max_wait:
                        await asyncio.sleep(0.5)
                        waited += 0.5
                    
                    if not self.llm_connected:
                        yield rx.toast.error("Impossible de se connecter √† l'assistant")
                        return
                
                question = self.question
                self.question = ""
                self.processing = True
                current_chat_key = self.current_chat
                
                # ‚úÖ Pas besoin d'ajouter optimistic UI
                # Le listener Firebase RTDB le fera automatiquement
                yield
                
                # ‚úÖ R√©cup√©rer le system prompt selon le mode
                system_prompt = self._get_system_prompt_by_mode()
                
                # ‚úÖ Envoi RPC au microservice
                result = rpc_call(
                    "LLM.send_message",
                    args=[
                        self.firebase_user_id,
                        self.base_collection_id,
                        self.base_collection_id,  # space_code = collection_name
                        current_chat_key,
                        question,
                        self.chat_mode,
                        system_prompt  # ‚úÖ Passer le system prompt
                    ],
                    user_id=self.firebase_user_id,
                    timeout_ms=5000  # Timeout court car c'est juste pour envoyer
                )
                
                if not result or not result.get("success"):
                    error_msg = result.get("error", "Unknown error") if result else "No response"
                    print(f"‚ùå Erreur envoi message: {error_msg}")
                    self.processing = False
                    yield rx.toast.error("Erreur lors de l'envoi du message")
                    return
                
                # ‚úÖ C'EST TOUT ! Le listener ChatListener va :
                # 1. D√©tecter le nouveau message utilisateur dans Firebase RTDB
                # 2. D√©tecter les updates du message assistant (streaming)
                # 3. Mettre √† jour l'UI automatiquement via _handle_chat_message()
                
            except Exception as e:
                print(f"‚ùå Exception send_message: {e}")
                self.processing = False
                yield rx.toast.error(f"Erreur: {str(e)}")
    
    def _get_system_prompt_by_mode(self) -> str:
        """Retourne le prompt syst√®me selon le chat_mode."""
        # √Ä adapter selon vos prompts existants
        if self.chat_mode == "router_chat":
            return """Tu es un assistant comptable sp√©cialis√© dans le routage de documents..."""
        elif self.chat_mode == "apbookeeper_chat":
            return """Tu es Pinnokio, assistant comptable sp√©cialis√© dans les fournisseurs..."""
        elif self.chat_mode == "onboarding_chat":
            return """Tu es un assistant d'onboarding qui aide les nouveaux utilisateurs..."""
        else:  # general_chat
            return """Tu es Pinnokio, assistant comptable intelligent..."""
    
    async def _handle_chat_message(self, message_data: dict):
        """‚úÖ D√âJ√Ä EXISTANT : Appel√© automatiquement par ChatListener.
        
        Cette m√©thode g√®re automatiquement :
        - Les nouveaux messages (role: user/assistant/system)
        - Les updates de streaming (content mis √† jour progressivement)
        - Les statuts (streaming, complete, error)
        """
        try:
            role = message_data.get("role", "")
            content = message_data.get("content", "")
            status = message_data.get("status", "complete")
            message_type = message_data.get("type", "")
            metadata = message_data.get("metadata", {})
            ephemeral = message_data.get("ephemeral", False)
            
            # ‚úÖ Messages syst√®me
            if role == "system":
                if message_type == "tool_execution":
                    tool_name = metadata.get("tool_name", "")
                    tool_status = metadata.get("status", "")
                    
                    if tool_status == "running":
                        yield rx.toast.info(f"‚öôÔ∏è {tool_name}...")
                    elif tool_status == "complete":
                        yield rx.toast.success(f"‚úÖ {tool_name} termin√©")
                    
                    # Si ephemeral, ne pas ajouter au chat
                    if not ephemeral:
                        self._add_system_message(content, metadata)
                
                elif message_type == "long_task":
                    # Toujours ajouter les t√¢ches longues au chat (persistent=True)
                    self._add_system_message(content, metadata)
                    
                    # Afficher barre de progression si disponible
                    if "progress_percent" in metadata:
                        self._update_task_progress(
                            metadata.get("task_id"),
                            metadata.get("progress_percent")
                        )
            
            # ‚úÖ Messages assistant
            elif role == "assistant":
                if status == "streaming":
                    self.processing = True
                    # Mettre √† jour progressivement l'UI
                    if self.current_chat in self.chats and self.chats[self.current_chat]:
                        for qa in reversed(self.chats[self.current_chat]):
                            if qa.answer and not qa.question:
                                qa.answer = content
                                break
                        else:
                            # Cr√©er un nouveau QA si pas trouv√©
                            self.chats[self.current_chat].append(QA(
                                question="",
                                answer=content,
                                show_metadata=False,
                                timestamp=message_data.get("timestamp", "")
                            ))
                
                elif status == "complete":
                    self.processing = False
                    # Le contenu est d√©j√† √† jour gr√¢ce au streaming
                    
                    # Sauvegarder dans Firebase (si vous voulez une sauvegarde suppl√©mentaire)
                    # Mais normalement c'est d√©j√† dans RTDB !
                
                elif status == "error":
                    self.processing = False
                    yield rx.toast.error(f"Erreur LLM: {message_data.get('error', 'Unknown')}")
            
            # ‚úÖ Messages utilisateur
            elif role == "user":
                if self.current_chat not in self.chats:
                    self.chats[self.current_chat] = []
                
                self.chats[self.current_chat].append(QA(
                    question=content,
                    answer="",
                    show_metadata=False,
                    timestamp=message_data.get("timestamp", "")
                ))
            
        except Exception as e:
            print(f"‚ùå Erreur _handle_chat_message: {e}")
    
    def _add_system_message(self, content: str, metadata: dict):
        """Ajoute un message syst√®me au chat."""
        if self.current_chat not in self.chats:
            self.chats[self.current_chat] = []
        
        self.chats[self.current_chat].append(QA(
            question="",
            answer=content,
            show_metadata=True,
            metadata=metadata,
            timestamp=datetime.now(timezone.utc).isoformat()
        ))
    
    def _update_task_progress(self, task_id: str, progress: int):
        """Met √† jour la barre de progression d'une t√¢che."""
        # TODO: Impl√©menter UI de progression
        pass
```

### **2. D√©marrage du listener (comme vous le faites d√©j√†)**

```python
@rx.event(background=True)
async def start_llm_chat_listener(self):
    """D√©marre le listener Firebase RTDB pour les conversations LLM."""
    async with self:
        try:
            from pinnokio_app.listeners.manager import listener_manager
            
            # ‚úÖ Utiliser le m√™me listener que les chats existants
            await listener_manager.start_chat_listener(
                space_code=self.base_collection_id,
                thread_key=self.current_chat,
                user_id=self.firebase_user_id,
                main_loop=asyncio.get_event_loop(),
                handler=self._handle_chat_message,
                mode="chats"  # ‚úÖ Nouveau mode pour les conversations LLM
            )
            
            self.Chat_realtime_listener_active = True
            print(f"‚úÖ Listener LLM chat d√©marr√© pour thread {self.current_chat}")
            
        except Exception as e:
            print(f"‚ùå Erreur d√©marrage listener LLM: {e}")
```

---

## üéØ **Framework Agentic - SPT et LPT**

### **Diff√©rence SPT vs LPT**

| Crit√®re | SPT (Short Process Tooling) | LPT (Long Process Tooling) |
|---------|----------------------------|----------------------------|
| **Dur√©e** | < 30 secondes | > 30 secondes (jusqu'√† plusieurs heures) |
| **Ex√©cution** | Synchrone dans le m√™me conteneur | Asynchrone via Celery/workflows externes |
| **Exemples** | Lire un fichier, analyser une facture simple, recherche ChromaDB | Rapprochement comptable complet, g√©n√©ration de rapport mensuel, workflow APBookeeper |
| **R√©ponse** | L'agent attend la r√©ponse avant de continuer | L'agent informe l'utilisateur et continue √† √™tre disponible |
| **Statut** | Bloquant pour le thread de conversation | Non-bloquant, l'utilisateur peut interagir pendant le traitement |

*(√Ä d√©velopper dans une phase ult√©rieure)*

---

## ‚úÖ **Plan d'impl√©mentation - √âtapes**

### **Phase 1 : Infrastructure de base (EN COURS)** ‚è≥

1. ‚úÖ Cr√©er la structure de dossiers `llm_service/`
2. ‚úÖ Impl√©menter `LLMManager` avec Firebase RTDB
3. ‚úÖ Impl√©menter `LLMSession` avec BaseAIAgent
4. ‚úÖ Impl√©menter `LLMContext`
5. ‚è≥ Int√©grer dans `main.py` pour r√©solution RPC
6. ‚è≥ Tester initialisation session via RPC depuis Reflex

### **Phase 2 : Communication Firebase RTDB (√Ä VENIR)**

1. ‚è≥ Tester streaming dans Firebase RTDB avec debounce
2. ‚è≥ Modifier `ChatState.send_message()` pour utiliser RPC
3. ‚è≥ Adapter `_handle_chat_message()` pour g√©rer les messages syst√®me
4. ‚è≥ Tester conversation compl√®te end-to-end

### **Phase 3 : Optimisations (√Ä VENIR)**

1. ‚è≥ Tuning du buffer streaming (100ms optimal ?)
2. ‚è≥ Gestion des erreurs et timeouts
3. ‚è≥ Cache Redis pour historiques (optionnel)
4. ‚è≥ Monitoring et m√©triques

### **Phase 4 : Framework agentic SPT/LPT (FUTURE)**

1. ‚è≥ Impl√©menter d√©tection d'appels d'outils
2. ‚è≥ Cr√©er `TaskOrchestrator`
3. ‚è≥ Impl√©menter t√¢ches Celery LPT

---

## üéØ **Avantages de cette architecture**

1. ‚úÖ **Coh√©rence totale** : M√™me pattern que vos chats onboarding/job existants
2. ‚úÖ **Z√©ro changement c√¥t√© Reflex** : Seuls `initialize_llm_agent()` et `send_message()` modifi√©s
3. ‚úÖ **Une seule source de v√©rit√©** : Firebase RTDB pour tout
4. ‚úÖ **R√©utilisation** : `ChatListener`, `_handle_chat_message()`, `BaseAIAgent` existants
5. ‚úÖ **Streaming optimis√©** : Debounce 100ms = 90-96% d'√©conomie d'√©critures Firebase
6. ‚úÖ **Scalabilit√©** : Gestion de milliers de conversations simultan√©es
7. ‚úÖ **Historique automatique** : Tout est sauvegard√© dans Firebase RTDB

---

## üìù **Checklist de validation**

### **Avant de commencer l'impl√©mentation**
- [x] Architecture valid√©e
- [x] D√©cisions techniques prises (Firebase RTDB, path, streaming, etc.)
- [x] `BaseAIAgent` d√©j√† pr√©sent et fonctionnel
- [ ] Cr√©er les dossiers `app/llm_service/`

### **Phase 1 - Infrastructure**
- [ ] Cr√©er `app/llm_service/__init__.py`
- [ ] Cr√©er `app/llm_service/llm_context.py`
- [ ] Cr√©er `app/llm_service/llm_manager.py`
- [ ] Int√©grer dans `app/main.py`
- [ ] Tester RPC `LLM.initialize_session` depuis Reflex

### **Phase 2 - Communication**
- [ ] Tester `LLM.send_message` RPC
- [ ] V√©rifier streaming Firebase RTDB
- [ ] V√©rifier listener Reflex d√©tecte les messages
- [ ] Test conversation compl√®te

### **Production**
- [ ] Tests de charge (10+ utilisateurs simultan√©s)
- [ ] Monitoring des co√ªts Firebase
- [ ] Documentation √©quipe

---

## üéâ **Conclusion**

Cette architecture **r√©utilise au maximum l'existant** :
- ‚úÖ Firebase Realtime Database (d√©j√† utilis√©)
- ‚úÖ `ChatListener` (d√©j√† impl√©ment√©)
- ‚úÖ `BaseAIAgent` (d√©j√† pr√©sent dans `klk_agents.py`)
- ‚úÖ Pattern de communication (identique aux chats job/onboarding)

**Avantages imm√©diats :**
- ‚úÖ Coh√©rence architecturale totale
- ‚úÖ Minimise les changements c√¥t√© Reflex
- ‚úÖ √âconomise 90-96% des co√ªts d'√©criture Firebase
- ‚úÖ Simplicit√© : une seule source de v√©rit√©

**Le syst√®me peut √™tre impl√©ment√© progressivement, phase par phase !** üöÄ
